[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks & Presentations",
    "section": "",
    "text": "Here you‚Äôll find recordings of my conference talks, workshops, and other presentations.\n\n\nVirtual Presentation\nDiscover how at Bolt I have been applying MMM to optimize our marketing budgets.\n\n\n\n\n\n\n\n\nVirtual Community Connect - 22 November 2023\nPresentation on open source marketing science techniques used by Meta.\n\n\n\n\n\n\n\n\nVirtual Webinar - 21 December 2023\nAnnual review of PyMC Labs‚Äô open source contributions and achievements.\n\n\n\n\n\n\n\n\nVirtual Webinar - 30 January 2024\nPractical guide to implementing and operationalizing Marketing Mix Modeling in technology companies.\n\n\n\n\n\n\n\n\nVirtual Webinar - 28 February 2024\nPanel discussion on Marketing Mix Modeling with industry experts Jim Gianoglio and Michael Kaminsky.\n\n\n\n\n\n\n\nHave a conference or workshop where you‚Äôd like me to speak? Let‚Äôs connect!\n\n\nContact Me\n\n\nBook 30 Minutes"
  },
  {
    "objectID": "talks.html#interested-in-collaboration",
    "href": "talks.html#interested-in-collaboration",
    "title": "Talks & Presentations",
    "section": "",
    "text": "Have a conference or workshop where you‚Äôd like me to speak? Let‚Äôs connect!\n\n\nContact Me\n\n\nBook 30 Minutes"
  },
  {
    "objectID": "articles/nomore_experiments_without_causality/nomore_experiments_without_causality.html",
    "href": "articles/nomore_experiments_without_causality/nomore_experiments_without_causality.html",
    "title": "Media Mix Model calibration is useless without causal knowledge",
    "section": "",
    "text": "Introduction\nImagine you just shipped a shiny new Bayesian Media-Mix Model (MMM) that perfectly back-fits years of marketing data. A/B-lift experiments then tell you channel-A is worth ‚Ç¨2.7 M, but your model insists it is worth ‚Ç¨7 M. ‚ÄúEasy fix,‚Äù you think: calibrate the MMM with the lift tests‚Äîadd an extra likelihood term, rerun, publish.\nYet the calibrated model still over/under-values the channel based on the experimental evidence. Looks like it can‚Äôt reconcile the experimental evidence with the data, and adding new calibration for other channels actually makes it worse.\nThat is the calibration trap: without causal structure the posterior can‚Äôt happily reconcile observations and clean experiments at the same time.\nIn this article we will build a PyMC MMM, add lift-test calibration, and then show‚Äîstep-by-step‚Äîwhy calibration alone cannot save a misspecified causal story.\n\n\n\nWhy marketers love calibration\n\nGround-truth anchor. Lift tests are randomised, so their incremental effects are (almost) unbiased.\n\nSample-size boost. MMMs see every day and every channel; experiments see only a slice. Combining them promises lower variance.\n\nStorytelling power. ‚ÄúOur model matches the experiments‚Äù is an executive-friendly sound-bite.\n\nCalibration therefore feels like catching two Bayesian birds with one conjugate stone.\n\n\n\nWhat is calibration‚Äîmathematically?\nFor each experiment \\(i\\) the model predicts a lift\n\\[\n\\widehat{\\Delta y_i}(\\theta)\\;=\\;\ns\\bigl(x_i+\\Delta x_i;\\,\\theta_{c(i)}\\bigr)\n\\;-\\;\ns\\bigl(x_i;\\,\\theta_{c(i)}\\bigr),\n\\]\nwhere\n\n\\(x_i\\) ‚Äì baseline spend before the experiment,\n\n\\(\\Delta x_i\\) ‚Äì change in spend during the experiment,\n\n\\(s(\\cdot;\\theta_{c(i)})\\) ‚Äì saturation curve for the channel that experiment \\(i\\) targets,\n\n\\(\\theta\\) ‚Äì all saturation-curve parameters,\n\n\\(\\widehat{\\Delta y_i}(\\theta)\\) ‚Äì model-predicted incremental outcome.\n\nWe then attach the observed lift \\(\\Delta y_i\\) and its error \\(\\sigma_i\\) through an additional likelihood\n\\[\np\\!\\bigl(\\Delta y_i \\mid \\theta\\bigr)\\;=\\;\n\\operatorname{Gamma}\\!\\bigl(\n\\mu=\\lvert\\widehat{\\Delta y_i}(\\theta)\\rvert,\\;\n\\sigma=\\sigma_i\n\\bigr),\n\\]\nwhere\n\n\\(\\Delta y_i\\) ‚Äì experimentally measured incremental outcome,\n\n\\(\\sigma_i\\) ‚Äì reported standard error of \\(\\Delta y_i\\),\n\n\\(\\mu\\) ‚Äì mean parameter set to the absolute predicted lift so the Gamma remains non-negative.\n\nStacking all \\(n_{\\text{lift}}\\) experiments gives the calibrated posterior\n\\[\np\\!\\bigl(\\theta \\mid \\mathbf y,\\mathcal L\\bigr)\n\\;\\propto\\;\np\\!\\bigl(\\mathbf y \\mid \\theta\\bigr)\\;\n\\prod_{i=1}^{n_{\\text{lift}}}\np\\!\\bigl(\\Delta y_i \\mid \\theta\\bigr)\\;\np(\\theta),\n\\]\nwhere\n\n\\(\\mathbf y\\) ‚Äì full time-series of observed outcomes (sales, sign-ups ‚Ä¶),\n\n\\(\\mathcal L\\) ‚Äì the collection of lift-test observations \\((\\Delta y_i,\\sigma_i)\\),\n\n\\(p(\\theta)\\) ‚Äì priors for all parameters.\n\nPyMC turns this into a three-liner:\nadd_lift_measurements_to_likelihood_from_saturation(\n    model=mmm,\n    df_lift=df_lifts,     # experiment data-frame\n    dist=pm.Gamma,\n)\nIn simple terms, calibration appends one extra likelihood per experiment: for lift i we run the channel‚Äôs saturation curve at the pre-spend and post-spend levels, subtract the two, and call that result the model-expected incremental response for experiment i (a deterministic function of the saturation parameter vector \\(\\theta\\)). We then treat the observed lift \\(\\Delta y_i\\) as a Gamma-distributed draw whose mean is the absolute value of that model-expected increment and whose dispersion is the experiment‚Äôs reported standard error \\(\\sigma_i\\).\nThese independent \\(\\Gamma(\\mu = |\\text{model-expected increment}|, \\sigma = \\sigma_i)\\) factors multiply into the original time-series likelihood, yielding a posterior where \\(\\theta\\) is pulled toward values that keep every model-expected increment within the experimental noise band. In effect, each lift test imposes a Bayesian anchor that penalises any parameter setting whose predicted causal effect disagrees with ground-truth, while still allowing the full sales history to inform the remaining uncertainty.\nLet‚Äôs see how this works in practice, by creating a synthetic dataset and fitting a simple MMM.\n\n\nGetting started\nWe‚Äôll use Pytensor to run our data-generation-process (DGP). Let‚Äôs set the seed for reproducibility, and define the number of observations, and finally add some default configurations for the notebook.\n\n\nCode\nimport warnings\nimport pymc as pm\nimport arviz as az\nimport pytensor.tensor as pt\nfrom pytensor.graph import rewrite_graph\nimport preliz as pz\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\n\nfrom pymc_marketing.mmm import GeometricAdstock, MichaelisMentenSaturation, MMM\nfrom pymc_marketing.prior import Prior\n\nSEED = 42\nn_observations = 1050\n\nwarnings.filterwarnings(\"ignore\")\n\n# Set the style\naz.style.use(\"arviz-darkgrid\")\nplt.rcParams[\"figure.figsize\"] = [8, 4]\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"axes.labelsize\"] = 6\nplt.rcParams[\"xtick.labelsize\"] = 6\nplt.rcParams[\"ytick.labelsize\"] = 6\n\n%config InlineBackend.figure_format = \"retina\"\n\n\nNow, we can define the date range.\n\n\nCode\nmin_date = pd.to_datetime(\"2022-01-01\")\nmax_date = min_date + pd.Timedelta(days=n_observations)\n\ndate_range = pd.date_range(start=min_date, end=max_date, freq=\"D\")\n\ndf = pd.DataFrame(data={\"date_week\": date_range}).assign(\n    year=lambda x: x[\"date_week\"].dt.year,\n    month=lambda x: x[\"date_week\"].dt.month,\n    dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n)\n\n\nWe can start by creating the spend vectors for each channel. These are the will define later the amount of impressions or exposition we get from each channel, which by the end will transform into sales.\n\n\nCode\nspend_x1 = pt.vector(\"spend_x1\")\nspend_x2 = pt.vector(\"spend_x2\")\nspend_x3 = pt.vector(\"spend_x3\")\nspend_x4 = pt.vector(\"spend_x4\")\n\n# Create sample inputs for demonstration using preliz distributions:\npz_spend_x1 = np.convolve(\n    pz.Gamma(mu=.8, sigma=.3).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x1[:14] = pz_spend_x1.mean()\npz_spend_x1[-14:] = pz_spend_x1.mean()\n\npz_spend_x2 = np.convolve(\n    pz.Gamma(mu=.6, sigma=.4).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x2[:14] = pz_spend_x2.mean()\npz_spend_x2[-14:] = pz_spend_x2.mean()\n\npz_spend_x3 = np.convolve(\n    pz.Gamma(mu=.2, sigma=.2).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x3[:14] = pz_spend_x3.mean()\npz_spend_x3[-14:] = pz_spend_x3.mean()\n\npz_spend_x4 = np.convolve(\n    pz.Gamma(mu=.1, sigma=.03).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x4[:14] = pz_spend_x4.mean()\npz_spend_x4[-14:] = pz_spend_x4.mean()\n\nfig, ax = plt.subplots()\nax.plot(date_range[1:], pz_spend_x1, label='Channel 1')\nax.plot(date_range[1:], pz_spend_x2, label='Channel 2')\nax.plot(date_range[1:], pz_spend_x3, label='Channel 3')\nax.plot(date_range[1:], pz_spend_x4, label='Channel 4')\nax.set_xlabel('Time')\nax.set_ylabel('Spend')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUsing the same logic we can create other components such as trend, noise, seasonality, and certain events.\n\n\nCode\n## Trend\ntrend = pt.vector(\"trend\")\n# Create a sample input for the trend\nnp_trend = (np.linspace(start=0.0, stop=.50, num=n_observations) + .10) ** (.1 / .4)\n\n## NOISE \nglobal_noise = pt.vector(\"global_noise\")\n# Create a sample input for the noise\npz_global_noise = pz.Normal(mu=0, sigma=.005).rvs(size=n_observations, random_state=SEED)\n\n# EVENTS EFFECT\npt_event_signal = pt.vector(\"event_signal\")\npt_event_contributions = pt.vector(\"event_contributions\")\n\nevent_dates = [\"24-12\", \"09-07\"]  # List of events as month-day strings\nstd_devs = [25, 15]  # List of standard deviations for each event\nevents_coefficients = [.094, .018]\n\nsignals_independent = []\n\n# Initialize the event effect array\nevent_signal = np.zeros(len(date_range))\nevent_contributions = np.zeros(len(date_range))\n\n# Generate event signals\nfor event, std_dev, event_coef in zip(\n    event_dates, std_devs, events_coefficients, strict=False\n):\n    # Find all occurrences of the event in the date range\n    event_occurrences = date_range[date_range.strftime(\"%d-%m\") == event]\n\n    for occurrence in event_occurrences:\n        # Calculate the time difference in days\n        time_diff = (date_range - occurrence).days\n\n        # Generate the Gaussian basis for the event\n        _event_signal = np.exp(-0.5 * (time_diff / std_dev) ** 2)\n\n        # Add the event signal to the event effect\n        signals_independent.append(_event_signal)\n        event_signal += _event_signal\n\n        event_contributions += _event_signal * event_coef\n\nnp_event_signal = event_signal\nnp_event_contributions = event_contributions\n\nplt.plot(pz_global_noise, label='Global Noise')\nplt.plot(np_trend, label='Trend')\nplt.plot(np_event_signal, label='Event Contributions')\nplt.title('Components of the Time Series Model')\nplt.xlabel('Time (days)')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn order to make it more interesting, lets add a price variable. Usually, price creates more impact as it‚Äôs slower. The product price contribution function we‚Äôll use is a diminishing returns function:\n\\[f(X, \\alpha, \\lambda) = \\frac{\\alpha}{1 + (X / \\lambda)}\\]\nwhere \\(\\alpha\\) represents the maximum contribution and \\(\\lambda\\) is a scaling parameter that controls how quickly the contribution diminishes as price increases.\n\n\nCode\ndef product_price_contribution(X, alpha, lam):\n    return alpha / (1 + (X / lam))\n    \n# Create a product price vector.\nproduct_price = pt.vector(\"product_price\")\nproduct_price_alpha = pt.scalar(\"product_price_alpha\")\nproduct_price_lam = pt.scalar(\"product_price_lam\")\n\n# Create a sample input for the product price\npz_product_price = np.convolve(\n    pz.Gamma(mu=.05, sigma=.02).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_product_price[:14] = pz_product_price.mean()\npz_product_price[-14:] = pz_product_price.mean()\n\nproduct_price_alpha_value = .08\nproduct_price_lam_value = .03\n\n# Direct contribution to the target.\npt_product_price_contribution = product_price_contribution(\n    product_price, \n    product_price_alpha, \n    product_price_lam\n)\n\n# plot the product price contribution\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\n# Plot the raw price data\nax1.plot(pz_product_price, color=\"green\")\nax1.set_title('Product Price')\nax1.set_xlabel('Time (days)')\nax1.set_ylabel('Price')\nax1.grid(True, alpha=0.3)\n\n# Plot the price contribution\nprice_contribution = pt_product_price_contribution.eval({\n    \"product_price\": pz_product_price,\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value\n})\nax2.plot(price_contribution, color=\"black\")\nax2.set_title('Price Contribution')\nax2.set_xlabel('Time (days)')\nax2.set_ylabel('Contribution')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWith all the principal components in place, all parent nodes we can start to write down our causal DAG to define the relationships we want to explain.\n\n\nCode\n# Plot causal graph of the vars x1, x2, x3, x4 using graphviz\ncdag_impressions = graphviz.Digraph(comment='Causal DAG for Impressions')\n\ncdag_impressions.node('spend_x1', 'Spend X1')\ncdag_impressions.node('spend_x2', 'Spend X2')\ncdag_impressions.node('spend_x3', 'Spend X3')\ncdag_impressions.node('spend_x4', 'Spend X4')\ncdag_impressions.node('events', 'Events')\n\ncdag_impressions.edge('spend_x1', 'impressions_x1')\ncdag_impressions.edge('spend_x2', 'impressions_x2')\ncdag_impressions.edge('spend_x3', 'impressions_x3')\ncdag_impressions.edge('spend_x4', 'impressions_x4')\n\ncdag_impressions.edge('impressions_x1', 'impressions_x3')\ncdag_impressions.edge('impressions_x2', 'impressions_x3')\ncdag_impressions.edge('impressions_x2', 'impressions_x4')\n\ncdag_impressions.edge('events', 'impressions_x2')\ncdag_impressions.edge('events', 'impressions_x3')\n\ncdag_impressions\n\n\n\n\n\n\n\n\n\nOnce our causal graph is defined, we can start to write down in pytensor the structure and relationships.\n\n\nCode\n# Create a impressions vector, result of x1, x2, x3, x4. by some beta with daily values.\n# Define all parameters as PyTensor variables\nbeta_x1 = pt.vector(\"beta_x1\")\nimpressions_x1 = spend_x1 * beta_x1\n\nbeta_x2 = pt.vector(\"beta_x2\")\nalpha_event_x2 = pt.scalar(\"alpha_event_x2\")\nimpressions_x2 = spend_x2 * beta_x2 + pt_event_signal * alpha_event_x2\n\nbeta_x3 = pt.vector(\"beta_x3\")\nalpha_event_x3 = pt.scalar(\"alpha_event_x3\")\nalpha_x1_x3 = pt.scalar(\"alpha_x1_x3\")\nalpha_x2_x3 = pt.scalar(\"alpha_x2_x3\")\nimpressions_x3 = spend_x3 * beta_x3 + pt_event_signal * alpha_event_x3 + (\n    impressions_x2 * alpha_x2_x3\n    + impressions_x1 * alpha_x1_x3\n)\n\nbeta_x4 = pt.vector(\"beta_x4\")\nalpha_x2_x4 = pt.scalar(\"alpha_x2_x4\")\nimpressions_x4 = spend_x4 * beta_x4 + impressions_x2 * alpha_x2_x4\n\n# Create sample values for the parameters (to be used in eval)\npz_beta_x1 = pz.Beta(alpha=0.05, beta=.1).rvs(size=n_observations, random_state=SEED)\npz_beta_x2 = pz.Beta(alpha=.015, beta=.05).rvs(size=n_observations, random_state=SEED)\npz_alpha_event_x2 = 0.015\npz_beta_x3 = pz.Beta(alpha=.1, beta=.1).rvs(size=n_observations, random_state=SEED)\npz_alpha_event_x3 = 0.001\npz_alpha_x1_x3 = 0.005\npz_alpha_x2_x3 = 0.12\npz_beta_x4 = pz.Beta(alpha=.125, beta=.05).rvs(size=n_observations, random_state=SEED)\npz_alpha_x2_x4 = 0.01\n\n# plot all impressions\n# Define dependencies for each variable\nx1_deps = {\n    \"beta_x1\": pz_beta_x1,\n    \"spend_x1\": pz_spend_x1,\n}\n\nx2_deps = {\n    \"beta_x2\": pz_beta_x2,\n    \"spend_x2\": pz_spend_x2,\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"event_signal\": event_signal[:-1],  # Slice to match 1050 length\n}\n\n# For x3, we need all dependencies from x1 and x2 plus its own\nx3_deps = {\n    \"beta_x3\": pz_beta_x3,\n    \"spend_x3\": pz_spend_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    **x1_deps,\n    **x2_deps,\n}\n\n# For x4, we need dependencies from x2 plus its own\nx4_deps = {\n    \"beta_x4\": pz_beta_x4,\n    \"spend_x4\": pz_spend_x4,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    **x2_deps,\n}\n\n# Plot each impression series\nfig, axs = plt.subplots(2, 2, sharex='row', sharey='row')\n\n# Channel 1\naxs[0, 0].plot(impressions_x1.eval(x1_deps), color='blue')\naxs[0, 0].set_title('Channel 1')\naxs[0, 0].set_ylabel('Impressions')\n\n# Channel 2\naxs[0, 1].plot(impressions_x2.eval(x2_deps), color='orange')\naxs[0, 1].set_title('Channel 2')\n\n# Channel 3\naxs[1, 0].plot(impressions_x3.eval(x3_deps), color='green')\naxs[1, 0].set_title('Channel 3')\naxs[1, 0].set_xlabel('Time')\naxs[1, 0].set_ylabel('Impressions')\n\n# Channel 4\naxs[1, 1].plot(impressions_x4.eval(x4_deps), color='red')\naxs[1, 1].set_title('Channel 4')\naxs[1, 1].set_xlabel('Time')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the computational graph\n\n\n\nIn order to check we write down the process properly, we can ask PyTensor to print our structural causal model. This is not necessary for the analysis, but can be helpful for debugging and understanding the model structure.\n\n\nCode\nimport pytensor.printing as printing\n# Plot the graph of our model using pytensor\nprinting.pydotprint(rewrite_graph(impressions_x4), outfile=\"images/impressions.png\", var_with_name_simple=True)\n# Display the generated graph\nfrom IPython.display import Image\nImage(filename=\"images/impressions.png\")\n\n\nThe output file is available at images/impressions.png\n\n\n\n\n\n\n\n\n\nIf, you don‚Äôt like to see the graphical version, you can ask for the string representation.\n\n\nCode\n# dprint the target_var\nrewrite_graph(impressions_x4).dprint(depth=5);\n\n\nAdd [id A]\n ‚îú‚îÄ Mul [id B]\n ‚îÇ  ‚îú‚îÄ spend_x4 [id C]\n ‚îÇ  ‚îî‚îÄ beta_x4 [id D]\n ‚îî‚îÄ Mul [id E]\n    ‚îú‚îÄ Add [id F]\n    ‚îÇ  ‚îú‚îÄ Mul [id G]\n    ‚îÇ  ‚îÇ  ‚îú‚îÄ spend_x2 [id H]\n    ‚îÇ  ‚îÇ  ‚îî‚îÄ beta_x2 [id I]\n    ‚îÇ  ‚îî‚îÄ Mul [id J]\n    ‚îÇ     ‚îú‚îÄ event_signal [id K]\n    ‚îÇ     ‚îî‚îÄ ExpandDims{axis=0} [id L]\n    ‚îî‚îÄ ExpandDims{axis=0} [id M]\n       ‚îî‚îÄ alpha_x2_x4 [id N]\n\n\n\n\nNow, let‚Äôs define our forward pass - how media exposure actually impacts our target variable. In marketing, we typically see two key effects: saturation (diminishing returns) and lagging (delayed impact). We‚Äôll model these using the Michaelis-Menten function for saturation and Geometric Adstock for the lagging effects.\n\n\nCode\n# Creating forward pass for impressions\ndef forward_pass(x, adstock_alpha, saturation_lam, saturation_alpha):\n    # return type pytensor.tensor.variable.TensorVariable\n    return MichaelisMentenSaturation.function(\n        MichaelisMentenSaturation, \n        x=GeometricAdstock(\n            l_max=24, normalize=False\n        ).function(\n            x=x, alpha=adstock_alpha,\n        ), lam=saturation_lam, alpha=saturation_alpha,\n    )\n\n# Applying forward pass to impressions\n# Create scalars variables for the parameters x2, x3, x4\npt_saturation_lam_x2 = pt.scalar(\"saturation_lam_x2\")\npt_saturation_alpha_x2 = pt.scalar(\"saturation_alpha_x2\")\n\npt_saturation_lam_x3 = pt.scalar(\"saturation_lam_x3\")\npt_saturation_alpha_x3 = pt.scalar(\"saturation_alpha_x3\")\n\npt_saturation_lam_x4 = pt.scalar(\"saturation_lam_x4\")\npt_saturation_alpha_x4 = pt.scalar(\"saturation_alpha_x4\")\n\npt_global_adstock_effect = pt.scalar(\"global_adstock_alpha\")\n\n# Apply forward pass to impressions\nimpressions_x2_forward = forward_pass(\n    impressions_x2, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x2, \n    pt_saturation_alpha_x2\n)\n\nimpressions_x3_forward = forward_pass(\n    impressions_x3, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x3, \n    pt_saturation_alpha_x3\n)\n\nimpressions_x4_forward = forward_pass(\n    impressions_x4, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x4, \n    pt_saturation_alpha_x4\n)\n\n\nWith all of the following in place, we can define the causal DAG for the target variable and the structural equation as the sum of all previous variables.\n\n\nCode\n# Plot graphviz causal dag for the target_var\n# Create a Graphviz object\ndot = graphviz.Digraph(comment='Causal DAG for Target Variable')\n\n# Add nodes for each variable\ndot.node('spend_x1', 'Spend X1')\ndot.node('spend_x2', 'Spend X2')\ndot.node('spend_x3', 'Spend X3')\ndot.node('spend_x4', 'Spend X4')\ndot.node('trend', 'Trend')\ndot.node('global_noise', 'Global Noise')\ndot.node('event_contributions', 'Events')\ndot.node('product_price_contribution', 'Product Price Contribution')\n\ndot.edge('spend_x1', 'impressions_x1')\ndot.edge('spend_x2', 'impressions_x2')\ndot.edge('spend_x3', 'impressions_x3')\ndot.edge('spend_x4', 'impressions_x4')\n\ndot.edge('impressions_x1', 'impressions_x3')\ndot.edge('impressions_x2', 'impressions_x3')\ndot.edge('impressions_x2', 'impressions_x4')\ndot.edge('event_contributions', 'impressions_x2')\ndot.edge('event_contributions', 'impressions_x3')\n\ndot.edge('trend', 'target_var')\ndot.edge('global_noise', 'target_var')\ndot.edge('event_contributions', 'target_var')\ndot.edge('product_price_contribution', 'target_var')\n\ndot.edge('impressions_x2', 'target_var')\ndot.edge('impressions_x3', 'target_var')\ndot.edge('impressions_x4', 'target_var')\n\n# Render the graph\ndot\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\n\\text{Target} &\\sim \\sum_{i \\in \\{2,3,4\\}} f_i(\\text{impressions}_i) + \\\\\n&\\text{event\\_contributions} + \\\\\n&\\text{product\\_price\\_contribution} + \\\\\n&\\text{trend} + \\\\\n&\\text{noise}\n\\end{align}\n\\]\nWhere \\(f_i\\) represents the forward pass function (adstock and saturation) applied to each channel‚Äôs impressions.\n\n\nCode\ntarget_var = rewrite_graph(\n    impressions_x4_forward + \n    impressions_x3_forward +\n    impressions_x2_forward +\n    pt_event_contributions +\n    pt_product_price_contribution + \n    trend + \n    global_noise\n)\n\n# Eval target_var and plot\nnp_target_var = target_var.eval({\n    \"spend_x4\": pz_spend_x4,\n    \"spend_x3\": pz_spend_x3,\n    \"spend_x2\": pz_spend_x2,\n    \"spend_x1\": pz_spend_x1,\n    \"event_signal\": event_signal[:-1],\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    \"beta_x2\": pz_beta_x2,\n    \"beta_x3\": pz_beta_x3,\n    \"beta_x4\": pz_beta_x4,\n    \"beta_x1\": pz_beta_x1,\n    \"saturation_lam_x2\": .5,\n    \"saturation_alpha_x2\": .2,\n    \"saturation_lam_x3\": .7,\n    \"saturation_alpha_x3\": .7,\n    \"saturation_lam_x4\": .2,\n    \"saturation_alpha_x4\": .1,\n    \"global_adstock_alpha\": .2,\n    \"product_price\": pz_product_price,\n    \"event_contributions\": np_event_contributions[:-1],\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value,\n    \"trend\": np_trend,\n    \"global_noise\": pz_global_noise,\n})\n\nplt.plot(np_target_var, linewidth=2)\nplt.title('Target Variable Over Time', fontsize=14)\nplt.xlabel('Time Period', fontsize=12)\nplt.ylabel('Target Value', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNow, we can imagine our dataframe in this case will be something like the following:\n\n\nCode\n# make dataset with impressions x1, x2, x3, x4 and target_var\nscaler_factor_for_all = 150\ndates = pd.date_range(start='2020-01-01', periods=n_observations, freq='D')\ndata = pd.DataFrame({\n    \"date\": dates,\n    \"target_var\": np.round(np_target_var * scaler_factor_for_all, 4),\n    \"impressions_x1\": np.round(impressions_x1.eval(x1_deps) * scaler_factor_for_all, 4),\n    \"impressions_x2\": np.round(impressions_x2.eval(x2_deps) * scaler_factor_for_all, 4),\n    \"impressions_x3\": np.round(impressions_x3.eval(x3_deps) * scaler_factor_for_all, 4),\n    \"impressions_x4\": np.round(impressions_x4.eval(x4_deps) * scaler_factor_for_all, 4),\n    \"event_2020_09\": np.round(signals_independent[0][:-1], 4),\n    \"event_2020_12\": np.round(signals_independent[1][:-1], 4),\n    \"event_2021_09\": np.round(signals_independent[2][:-1], 4),\n    \"event_2021_12\": np.round(signals_independent[3][:-1], 4),\n    \"event_2022_09\": np.round(signals_independent[4][:-1], 4),\n})\ndata[\"trend\"] = data.index\ndata.head()\n\n\n\n\n\n\n\n\n\ndate\ntarget_var\nimpressions_x1\nimpressions_x2\nimpressions_x3\nimpressions_x4\nevent_2020_09\nevent_2020_12\nevent_2021_09\nevent_2021_12\nevent_2022_09\ntrend\n\n\n\n\n0\n2020-01-01\n128.7894\n112.9178\n30.9076\n34.3534\n15.0851\n0.0\n0.0\n0.0\n0.0\n0.0\n0\n\n\n1\n2020-01-02\n123.5265\n74.9429\n4.3523\n27.7279\n14.7826\n0.0\n0.0\n0.0\n0.0\n0.0\n1\n\n\n2\n2020-01-03\n98.5682\n0.0000\n0.0000\n0.0000\n0.0000\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n3\n2020-01-04\n107.3861\n5.3253\n0.0001\n12.7077\n13.7833\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n\n\n4\n2020-01-05\n93.9367\n0.0000\n0.0000\n0.0001\n5.6283\n0.0\n0.0\n0.0\n0.0\n0.0\n4\n\n\n\n\n\n\n\nIf we don‚Äôt think in a causal way, we will probably just say, ‚Äúlets add all to the blender‚Äù.\n\n\nCode\n# Building priors for adstock and saturation\nadstock_priors = {\n    \"alpha\": Prior(\"Beta\", alpha=1, beta=1, dims=\"channel\"),\n}\n\nadstock = GeometricAdstock(l_max=28, priors=adstock_priors)\n\nsaturation_priors = {\n    \"lam\": Prior(\n        \"Gamma\",\n        mu=2,\n        sigma=1,\n        dims=\"channel\",\n    ),\n    \"alpha\": Prior(\n        \"Gamma\",\n        mu=.5,\n        sigma=.5,\n        dims=\"channel\",\n    ),\n}\n\nsaturation = MichaelisMentenSaturation(priors=saturation_priors)\n\n# Split data into train and test sets\ntrain_idx = 879\n\nX_train = data.iloc[:train_idx].drop(columns=[\"target_var\"])\nX_test = data.iloc[train_idx:].drop(columns=[\"target_var\"])\ny_train = data.iloc[:train_idx][\"target_var\"]\ny_test = data.iloc[train_idx:][\"target_var\"]\n\ncontrol_columns = [\n    \"event_2020_09\", \"event_2020_12\", \n    \"event_2021_09\", \"event_2021_12\", \n    \"event_2022_09\",\n    \"trend\"\n]\nchannel_columns = [\n    col for col in X_train.columns if col not in control_columns and col != \"date\"\n]\n\n# Model config\nmodel_config = {\n    \"likelihood\": Prior(\n        \"TruncatedNormal\",\n        lower=0,\n        sigma=Prior(\"HalfNormal\", sigma=1),\n        dims=\"date\",\n    ),\n}\n\n# sampling options for PyMC\nsample_kwargs = {\n    \"tune\": 1000,\n    \"draws\": 500,\n    \"chains\": 4,\n    \"random_seed\": 42,\n    \"target_accept\": 0.94,\n}\n\nnon_causal_mmm = MMM(\n    date_column=\"date\",\n    channel_columns=channel_columns,\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\nnon_causal_mmm.build_model(X_train, y_train)\n\n\n\n\n\n\n\n\nBuilding the model\n\n\n\nAll PyMC models are structural causal models, which means they represent the causal generative process of the data. We can visualize this process through a Directed Acyclic Graph (DAG) that shows how variables influence each other in the model.\n\n\nCode\nnon_causal_mmm.model.to_graphviz()\n\n\n\n\n\n\n\n\n\n\n\nOnce the model is build, we can train it.\n\n\nCode\nnon_causal_mmm.fit(X_train, y_train,)\nnon_causal_mmm.sample_posterior_predictive(X_train, extend_idata=True, combined=True)\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [intercept, adstock_alpha, saturation_alpha, saturation_lam, gamma_control, y_sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 82 seconds.\nThere were 13 divergences after tuning. Increase `target_accept` or reparameterize.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n\n\n\n\n\n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 14MB\nDimensions:  (date: 879, sample: 2000)\nCoordinates:\n  * date     (date) datetime64[ns] 7kB 2020-01-01 2020-01-02 ... 2022-05-28\n  * sample   (sample) object 16kB MultiIndex\n  * chain    (sample) int64 16kB 0 0 0 0 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3 3 3 3\n  * draw     (sample) int64 16kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    y        (date, sample) float64 14MB 133.8 131.1 132.8 ... 163.4 155.6 161.9\nAttributes:\n    created_at:                 2025-09-03T12:30:51.450430+00:00\n    arviz_version:              0.21.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1xarray.DatasetDimensions:date: 879sample: 2000Coordinates: (4)date(date)datetime64[ns]2020-01-01 ... 2022-05-28array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2022-05-26T00:00:00.000000000',\n       '2022-05-27T00:00:00.000000000', '2022-05-28T00:00:00.000000000'],\n      dtype='datetime64[ns]')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(date, sample)float64133.8 131.1 132.8 ... 155.6 161.9array([[133.81918965, 131.09113239, 132.79876117, ..., 131.33519924,\n        134.2956799 , 130.06267199],\n       [125.61793647, 127.35617634, 124.50094563, ..., 129.11076385,\n        130.99329171, 127.57028533],\n       [ 98.98509098, 102.83491893, 101.6459546 , ..., 101.45071599,\n        105.78579891, 103.13829591],\n       ...,\n       [149.45308463, 151.78767903, 147.00168105, ..., 149.30100123,\n        151.08311501, 152.10722311],\n       [139.08206233, 139.24038693, 140.44786746, ..., 137.26647095,\n        141.20170821, 138.20248311],\n       [159.58492216, 160.88775306, 159.22967696, ..., 163.41435726,\n        155.61599977, 161.87213003]])Indexes: (2)datePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2022-05-19', '2022-05-20', '2022-05-21', '2022-05-22',\n               '2022-05-23', '2022-05-24', '2022-05-25', '2022-05-26',\n               '2022-05-27', '2022-05-28'],\n              dtype='datetime64[ns]', name='date', length=879, freq=None))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (4)created_at :2025-09-03T12:30:51.450430+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.25.1\n\n\nWe are happy with our model, we don‚Äôt get any divergencies, and the sampling looks good.\n\n\nCode\n# Number of diverging samples\nprint(\n    f\"Total divergencies: {non_causal_mmm.idata['sample_stats']['diverging'].sum().item()}\"\n)\n\naz.summary(\n    data=non_causal_mmm.fit_result,\n    var_names=[\n        \"intercept\",\n        \"y_sigma\",\n        \"saturation_alpha\",\n        \"saturation_lam\",\n        \"adstock_alpha\",\n    ],\n)\n\n\nTotal divergencies: 13\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n0.456\n0.010\n0.430\n0.464\n0.002\n0.003\n46.0\n34.0\n1.06\n\n\ny_sigma\n0.009\n0.000\n0.008\n0.009\n0.000\n0.000\n1886.0\n1483.0\n1.00\n\n\nsaturation_alpha[impressions_x1]\n0.066\n0.029\n0.020\n0.118\n0.001\n0.001\n491.0\n852.0\n1.01\n\n\nsaturation_alpha[impressions_x2]\n0.141\n0.007\n0.129\n0.154\n0.000\n0.000\n964.0\n1113.0\n1.00\n\n\nsaturation_alpha[impressions_x3]\n0.501\n0.019\n0.469\n0.541\n0.001\n0.000\n964.0\n1180.0\n1.00\n\n\nsaturation_alpha[impressions_x4]\n0.097\n0.024\n0.059\n0.140\n0.001\n0.001\n1431.0\n1264.0\n1.00\n\n\nsaturation_lam[impressions_x1]\n2.029\n1.236\n0.021\n4.016\n0.138\n0.075\n57.0\n37.0\n1.06\n\n\nsaturation_lam[impressions_x2]\n0.445\n0.046\n0.369\n0.539\n0.001\n0.001\n979.0\n1288.0\n1.00\n\n\nsaturation_lam[impressions_x3]\n1.292\n0.073\n1.169\n1.441\n0.002\n0.002\n954.0\n1107.0\n1.00\n\n\nsaturation_lam[impressions_x4]\n1.935\n0.726\n0.815\n3.304\n0.020\n0.022\n1380.0\n1258.0\n1.00\n\n\nadstock_alpha[impressions_x1]\n0.993\n0.007\n0.981\n1.000\n0.000\n0.000\n873.0\n673.0\n1.01\n\n\nadstock_alpha[impressions_x2]\n0.190\n0.011\n0.168\n0.210\n0.000\n0.000\n1114.0\n1120.0\n1.00\n\n\nadstock_alpha[impressions_x3]\n0.193\n0.005\n0.183\n0.203\n0.000\n0.000\n2079.0\n1405.0\n1.00\n\n\nadstock_alpha[impressions_x4]\n0.216\n0.031\n0.153\n0.272\n0.001\n0.001\n2016.0\n1253.0\n1.01\n\n\n\n\n\n\n\nIf our model has a correct understanding of causality, we can use it to perform a do-calculus to estimate the effect of our channel, using out of sample (sampling from the posterior). Mathematically, we want to compute the causal effect as the difference between two interventions: \\[P(Y|do(X=x)) - P(Y|do(X=0))\\]\nThis should allows us to isolate the causal impact of our marketing channels on the outcome variable.\n\n\nCode\nX_test_x2_zero = X_test.copy()\nX_test_x2_zero[\"impressions_x2\"].iloc[:100] = 0\n\ny_do_x2_zero = non_causal_mmm.sample_posterior_predictive(\n    X_test_x2_zero, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\ny_do_x2 = non_causal_mmm.sample_posterior_predictive(\n    X_test, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\nNow that we have both posteriors, we can compute the difference between the period with the index 880-890 and plot the causal effect and the cumulative causal effect.\n\n\nCode\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect = (y_do_x2_zero - y_do_x2).y\n# Get dates from the coordinates for x-axis\ndates = x2_causal_effect.coords['date'].values[:100]  # Take only first 100 days\n\n# Plot the causal effect\nplt.subplot(1, 2, 1)\n# Calculate mean and quantiles\nmean_effect = x2_causal_effect.mean(dim=\"sample\")[:100]\nplt.plot(dates, mean_effect)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\n# For cumulative effect, compute quantiles directly from cumulative sums\ncum_effect = x2_causal_effect.cumsum(dim=\"date\")\ncum_mean = cum_effect.mean(dim=\"sample\")[:100]\nplt.plot(dates, cum_mean)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nIn reality, in order to validate the following estimated effect, we‚Äôll need to run an actual experiment. Because we did the data generation process we can run this actual experiment to compare.\n\n\nCode\n# Create an intervened spend_x2 with zeros between index 880 and 980\nintervened_spend_x2 = pz_spend_x2.copy()\nintervened_spend_x2[880:980] = 0\n\n# Evaluate target variable with the intervention\nnp_target_var_x2_zero = target_var.eval({\n    \"spend_x4\": pz_spend_x4,\n    \"spend_x3\": pz_spend_x3,\n    \"spend_x2\": intervened_spend_x2,\n    \"spend_x1\": pz_spend_x1,\n    \"event_signal\": event_signal[:-1],\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    \"beta_x2\": pz_beta_x2,\n    \"beta_x3\": pz_beta_x3,\n    \"beta_x4\": pz_beta_x4,\n    \"beta_x1\": pz_beta_x1,\n    \"saturation_lam_x2\": .5,\n    \"saturation_alpha_x2\": .2,\n    \"saturation_lam_x3\": .7,\n    \"saturation_alpha_x3\": .7,\n    \"saturation_lam_x4\": .2,\n    \"saturation_alpha_x4\": .1,\n    \"global_adstock_alpha\": .2,\n    \"product_price\": pz_product_price,\n    \"event_contributions\": np_event_contributions[:-1],\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value,\n    \"trend\": np_trend,\n    \"global_noise\": pz_global_noise,\n})\n\n# x2 total effect y | do(x2=&gt;1) - y | do(x2=0)\nx2_intervention_real_effect = np_target_var_x2_zero - np_target_var\nx2_intervention_real_cumulative_effect = np.cumsum(x2_intervention_real_effect)\n\n# Plot both the intervention effect and cumulative effect\nplt.subplot(1, 2, 1)\n# Plot the daily effect\ndaily_effect = x2_intervention_real_effect[880:980] * scaler_factor_for_all\nplt.plot(dates, daily_effect)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\ncumulative_effect = x2_intervention_real_cumulative_effect[880:980] * scaler_factor_for_all\nplt.plot(dates, cumulative_effect)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHow does compare to the recovered effect? Let‚Äôs observe! üëÄ\n\n\nCode\n# Create a figure to compare real effects with estimated effects\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe initial model have been under estimating the effect of \\(X2\\). We can see the model was thinking we‚Äôll loosing almost none users when in reality wi‚Äôll loose around 600 in total. Maybe we did something wrong? Are we maybe the wrong causal question?\nThat doesn‚Äôt matter, we have calibration! ü§™\nLets compute the observable delta in Y and observable delta in X and use it for calibration.\n\n\nCode\nintervened_channel = \"impressions_x2\"\ntotal_observed_effect = cumulative_effect[-1] # delta Y\ntotal_previous_imp_before_intervention = X_train[intervened_channel].iloc[-100:].sum()\ntotal_change_imp_during_intervention = -X_train[intervened_channel].iloc[-100:].sum()\nsigma = 0.3 # confidence in the experiment.\n\ndf_lift_test = pd.DataFrame(\n    [{\n        \"channel\": intervened_channel,\n        \"x\": total_previous_imp_before_intervention,\n        \"delta_x\": total_change_imp_during_intervention,\n        \"delta_y\": total_observed_effect,\n        \"sigma\": sigma,\n    }]\n)\n\nintervened_data = data.copy()\nintervened_data.loc[880:980, \"impressions_x2\"] = 0\n\nnon_causal_mmm2 = MMM(\n    date_column=\"date\",\n    channel_columns=channel_columns,\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\nnon_causal_mmm2.build_model(\n    intervened_data.drop(columns=[\"target_var\"]), \n    intervened_data[\"target_var\"]\n)\n\nnon_causal_mmm2.add_lift_test_measurements(df_lift_test)\nnon_causal_mmm2.model.to_graphviz()\n\n\n\n\n\n\n\n\n\nAs we can see a new observational point have been added to our data. This new point must be satisfied as the rest of our data, pooling parameter into a new direction.\n\n\n\n\n\n\nNote\n\n\n\nIn a Bayesian model, each observation‚Äîwhether it is a daily data point \\(y_t\\) or a lift measurement \\(\\Delta y\\)‚Äîcontributes a term to the likelihood. The posterior arises from the product of all these likelihood terms and the prior(s). In other words, theres no actual difference between priors and data, they both carry the same weight and multiply in the numerator of Bayes theorem. There‚Äôs no discrete ‚Äúdecision‚Äù about which part of the data (or which prior) to weight more; it all goes into the same log‚Äêposterior function. The sampling or optimization algorithm (MCMC, variational inference, etc.) explores the parameter space in proportion to the posterior probability (which is prior √ó likelihood). Whichever parameters jointly give higher posterior density get visited more often by the sampler.\n\n\n\n\nCode\nnon_causal_mmm2.fit(\n    intervened_data.drop(columns=[\"target_var\"]), \n    intervened_data[\"target_var\"],\n)\nnon_causal_mmm2.sample_posterior_predictive(\n    intervened_data.drop(columns=[\"target_var\"]), \n    extend_idata=True, \n    combined=True\n)\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [intercept, adstock_alpha, saturation_alpha, saturation_lam, gamma_control, y_sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 159 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n\n\n\n\n\n\n\n\nSampling: [lift_measurements, y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 17MB\nDimensions:                  (lift_measurements_dim_0: 1, sample: 2000,\n                              date: 1050)\nCoordinates:\n  * lift_measurements_dim_0  (lift_measurements_dim_0) int64 8B 0\n  * date                     (date) datetime64[ns] 8kB 2020-01-01 ... 2022-11-15\n  * sample                   (sample) object 16kB MultiIndex\n  * chain                    (sample) int64 16kB 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3\n  * draw                     (sample) int64 16kB 0 1 2 3 4 ... 496 497 498 499\nData variables:\n    lift_measurements        (lift_measurements_dim_0, sample) float64 16kB 2...\n    y                        (date, sample) float64 17MB 133.7 129.3 ... 176.1\nAttributes:\n    created_at:                 2025-09-03T12:33:35.648268+00:00\n    arviz_version:              0.21.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1xarray.DatasetDimensions:lift_measurements_dim_0: 1sample: 2000date: 1050Coordinates: (5)lift_measurements_dim_0(lift_measurements_dim_0)int640array([0])date(date)datetime64[ns]2020-01-01 ... 2022-11-15array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2022-11-13T00:00:00.000000000',\n       '2022-11-14T00:00:00.000000000', '2022-11-15T00:00:00.000000000'],\n      dtype='datetime64[ns]')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (2)lift_measurements(lift_measurements_dim_0, sample)float642.998 2.993 2.994 ... 0.0 0.0 0.0array([[2.99763533, 2.99341442, 2.99384234, ..., 0.        , 0.        ,\n        0.        ]])y(date, sample)float64133.7 129.3 133.5 ... 172.1 176.1array([[133.65032085, 129.27488757, 133.47161631, ..., 123.2957264 ,\n        125.73944586, 119.78032871],\n       [129.52741324, 126.74082   , 127.91233297, ..., 136.45056287,\n        132.00908345, 135.32215986],\n       [102.85763459, 103.79846597, 101.12188774, ...,  96.92158097,\n         95.03130614,  98.70912397],\n       ...,\n       [169.4261703 , 161.15199916, 162.15463881, ..., 159.04708873,\n        172.00519732, 151.648744  ],\n       [149.75961238, 160.04423081, 153.47681467, ..., 144.01826631,\n        152.09073029, 154.39457119],\n       [184.36147919, 179.79514649, 180.6854263 , ..., 170.94224661,\n        172.08260567, 176.10031727]])Indexes: (3)lift_measurements_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='lift_measurements_dim_0'))datePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2022-11-06', '2022-11-07', '2022-11-08', '2022-11-09',\n               '2022-11-10', '2022-11-11', '2022-11-12', '2022-11-13',\n               '2022-11-14', '2022-11-15'],\n              dtype='datetime64[ns]', name='date', length=1050, freq=None))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (4)created_at :2025-09-03T12:33:35.648268+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.25.1\n\n\nNow that our model is ready, we can check the new estimated effect.\n\n\nCode\ny_do_x2_zero_second_model = non_causal_mmm2.idata.posterior_predictive.copy()\ny_do_x2_second_model = non_causal_mmm2.sample_posterior_predictive(\n    data.drop(columns=[\"target_var\"]), \n    extend_idata=False, \n    include_last_observations=False, \n    combined=False,\n    random_seed=42\n)\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect_second_model = (y_do_x2_zero_second_model.y - y_do_x2_second_model.y).isel(date=slice(880, 980))\n\n# Plot the causal effect\nplt.subplot(1, 2, 1)\n# Calculate mean and quantiles\nmean_effect_second_model = x2_causal_effect_second_model.mean(dim=[\"chain\",\"draw\"])\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, mean_effect_second_model)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\n# For cumulative effect, compute quantiles directly from cumulative sums\ncum_effect_second_model = x2_causal_effect_second_model.cumsum(dim=\"date\")\ncum_mean_second_model = cum_effect_second_model.mean(dim=[\"chain\",\"draw\"])\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, cum_mean_second_model)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\n\n\nSampling: [lift_measurements, y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see the effect looks fully different. The size is 1000X higher than before. Let‚Äôs compare!\n\n\nCode\n# Create a figure to compare real effects with estimated effects\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, mean_effect_second_model, label='Estimated Effect (2)', color='orange', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, cum_mean_second_model, \n         label='Estimated Cumulative Effect (2)', color='orange', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs expected the new observation makes the model add more credit to X2 but this came with the price of an overestimation of the true impact. Meanwhile, it was true that X2 impact was bigger than the original one, the second model absorbe all the variability possibly explain by other variables such as X1, X3 and bring a 1000X more extra impact, with a very tight posterior.\n\n\nCode\n# plot the recovered mean daily contribution as distribution.\nchannels_contribution_original_scale_model1 = non_causal_mmm.compute_channel_contribution_original_scale()\n\nchannels_contribution_original_scale_model2 = non_causal_mmm2.compute_channel_contribution_original_scale()\n\n_dist1 = channels_contribution_original_scale_model1.isel(date=slice(0, 800)).mean(\n    dim=[\"date\"]\n).sel(channel=\"impressions_x2\").values.flatten()\n\n_dist2 = channels_contribution_original_scale_model2.isel(date=slice(0, 800)).mean(\n    dim=[\"date\"]\n).sel(channel=\"impressions_x2\").values.flatten()\n\n\n# First subplot for Model 1\nplt.subplot(1, 2, 1)\nsns.kdeplot(_dist1, shade=True, label=\"Model 1\", bw_adjust=4.5)\nplt.title(\"Distribution of Channel X2 Contribution - Model 1\", fontsize=12)\nplt.xlabel(\"Contribution Value\", fontsize=10)\nplt.ylabel(\"Density\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=9)\n\n# Second subplot for Model 2\nplt.subplot(1, 2, 2)\nsns.kdeplot(_dist2, shade=True, label=\"Model 2\", bw_adjust=4.5, color=\"orange\")\nplt.title(\"Distribution of Channel X2 Contribution - Model 2\", fontsize=12)\nplt.xlabel(\"Contribution Value\", fontsize=10)\nplt.ylabel(\"Density\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Danger of Tight Posteriors\n\n\n\nIt‚Äôs important to note that a tight posterior distribution (like we see in Model 2) should never be understood as the model being more correct or certain about the true causal effect. This is a common misconception in Bayesian analysis.\nA tight posterior simply means the model is very confident in its estimates given the data and prior assumptions it has, but says nothing about whether those assumptions are correct. In this case, the addition of the lift test measurement has created a model that is very confident in an incorrect answer.\nThis illustrates an important principle in causal inference and Bayesian modeling: precision is not the same as accuracy. A model can be precisely wrong - having a narrow posterior around an incorrect value. This often happens when:\n\nThe model structure doesn‚Äôt match the true causal process\nImportant confounders are omitted\nThe priors or likelihood are misspecified\n\n\n\nWhy all the following happened? lets take a look to the graph.\n\n\nCode\ndot\n\n\n\n\n\n\n\n\n\nThis DAG shows:\n\nDirect Spend-to-Impression Relationships: Each spend variable (X1-X4) directly influences its corresponding impression variable.\nCross-Channel Effects:\n\nImpressions from X1 influence impressions from X3\nImpressions from X2 influence both X3 and X4 impressions\nEvents influence impressions for X2 and X3\n\n\nIf we were to build a naive regression model including all variables (X1, X2, X3, X4), we would encounter significant estimation problems, particularly for X2. According to Pearl‚Äôs causal theory.\n\n1. Collider Bias\nIn our graph, X2 influences X3 and X4, which both influence the target variable. This creates a collider structure where conditioning on x1 variable because induces a spurious correlation between X2, X3. This violates the independence assumptions of standard regression.\n\n\n2. Mediator Effects\nX2 has both direct effects on the target variable and indirect effects through X3 and X4. A naive regression would conflate these paths, leading to inconsistent estimates of X2‚Äôs true total causal effect.\n\n\n3. Confounding from Events\nEvents influence both X2 impressions and the target variable directly. Without properly accounting for this common cause, the estimate for X2 will capture some of the effect that actually comes from events.\nAll the above means, in order to estimate the effect of X2 we need to address the primal causal questions.\n\n\n4. Minimal Adjustment Set for X2\nTo estimate the total causal effect of X2 on the target variable, we need to identify the minimal adjustment set that blocks all non-causal paths while preserving the causal paths. According to Pearl‚Äôs backdoor criterion, we must control for any confounders (common causes) while avoiding adjusting for colliders or mediators. In our DAG, the minimal adjustment set for estimating X2‚Äôs total effect would include Events (as it‚Äôs a confounder affecting both X2 and the target) and Spend X1 (as it influences the target through X3, creating a backdoor path). We should not adjust for impressions_x3 or impressions_x4, as these are mediators through which X2 partially exerts its effect on the target variable. Nevertheless, events are a cofounder of X2, meaning, we need to control for them if we want to get the estimates right on spot.\nThe proper identification of this minimal adjustment set is crucial for unbiased estimation. If we control for too few variables, confounding bias remains. If we control for mediators, we block part of the causal effect we‚Äôre trying to measure. This highlights why structural causal models are superior to naive regression approaches - they allow us to explicitly model the causal pathways and make appropriate adjustments based on causal reasoning rather than statistical correlation. By conditioning only on the minimal adjustment set, we can obtain a consistent estimate of X2‚Äôs total causal effect, including both its direct impact and indirect effects through other channels.\nSo, let‚Äôs see what happen if we apply causal theory üòÉ\n\n\nCode\n# Lets rebuild our media mix model\ncausal_mmm = MMM(\n    date_column=\"date\",\n    channel_columns=[\"impressions_x2\"],\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\ncausal_mmm.fit(X_train, y_train,)\ncausal_mmm.sample_posterior_predictive(X_train, extend_idata=True, combined=True)\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [intercept, adstock_alpha, saturation_alpha, saturation_lam, gamma_control, y_sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 22 seconds.\n\n\n\n\n\n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 14MB\nDimensions:  (date: 879, sample: 2000)\nCoordinates:\n  * date     (date) datetime64[ns] 7kB 2020-01-01 2020-01-02 ... 2022-05-28\n  * sample   (sample) object 16kB MultiIndex\n  * chain    (sample) int64 16kB 0 0 0 0 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3 3 3 3\n  * draw     (sample) int64 16kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    y        (date, sample) float64 14MB 125.4 119.5 131.4 ... 151.4 169.8 144.8\nAttributes:\n    created_at:                 2025-09-03T12:34:02.437548+00:00\n    arviz_version:              0.21.0\n    inference_library:          pymc\n    inference_library_version:  5.25.1xarray.DatasetDimensions:date: 879sample: 2000Coordinates: (4)date(date)datetime64[ns]2020-01-01 ... 2022-05-28array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2022-05-26T00:00:00.000000000',\n       '2022-05-27T00:00:00.000000000', '2022-05-28T00:00:00.000000000'],\n      dtype='datetime64[ns]')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(date, sample)float64125.4 119.5 131.4 ... 169.8 144.8array([[125.37174538, 119.48868131, 131.41018253, ..., 114.72608677,\n        106.81133416, 136.82964694],\n       [111.38400378, 119.47217343, 111.68295159, ..., 110.88782277,\n        117.93889351, 121.95710826],\n       [124.81854599, 101.91048623, 119.40986367, ..., 100.55611047,\n        121.9504995 , 129.04248805],\n       ...,\n       [171.18166549, 165.3113808 , 151.86873425, ..., 161.87058754,\n        164.74254816, 154.29190207],\n       [153.77744067, 149.95703362, 164.99545556, ..., 133.31986158,\n        154.74990007, 153.78741855],\n       [122.79449872, 149.42964751, 149.33161317, ..., 151.43926854,\n        169.7588566 , 144.75492479]])Indexes: (2)datePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2022-05-19', '2022-05-20', '2022-05-21', '2022-05-22',\n               '2022-05-23', '2022-05-24', '2022-05-25', '2022-05-26',\n               '2022-05-27', '2022-05-28'],\n              dtype='datetime64[ns]', name='date', length=879, freq=None))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (4)created_at :2025-09-03T12:34:02.437548+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.25.1\n\n\nNow, lets repeat again the estimation of the effect when X2 is zero.\n\n\nCode\nX_test_x2_zero = X_test.copy()\nX_test_x2_zero[\"impressions_x2\"].iloc[:100] = 0\n\ny_do_x2_zero_causal = causal_mmm.sample_posterior_predictive(\n    X_test_x2_zero, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\ny_do_x2_causal = causal_mmm.sample_posterior_predictive(\n    X_test, extend_idata=False, include_last_observations=True, random_seed=42\n)\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect_causal = (y_do_x2_zero_causal - y_do_x2_causal).y\n# Get dates from the coordinates for x-axis\ndates = x2_causal_effect_causal.coords['date'].values[:100]  # Take only first 100 days\n\n# Calculate mean and quantiles\nmean_effect = x2_causal_effect_causal.mean(dim=\"sample\")[:100]\ncum_effect = x2_causal_effect_causal.cumsum(dim=\"date\")\ncum_mean = cum_effect.mean(dim=\"sample\")[:100]\n\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreat, as expected the true causal effect for X2 was recovered, and its possible to prove with an experiment. This just prove that maths are not magic, and that if we want to create models that explain the dynamics of the world, we need to use causal reasoning to it üî•üôåüèª\n\n\n\nConclusion\nThe evidence is clear: calibration cannot rescue a misspecified causal model. We‚Äôve seen that:\n\nCausal misspecification persists despite calibration. Our Model 2 became confidently wrong after calibration‚Äîtight posteriors around incorrect values.\nColliders and mediators matter. Standard MMMs ignore that marketing channels influence each other, creating spurious correlations that no amount of experimental data can fix.\nAdjustment sets are crucial. Simply including every variable yields biased estimates; we must control only for confounders while preserving causal pathways.\n\nWhen we finally built a causally-aware MMM‚Äîcontrolling for events as confounders but avoiding adjustment for mediators‚Äîour estimates matched the ground truth. The same experimental evidence that couldn‚Äôt rescue our misspecified model perfectly aligned with our correctly specified one.\nThe message: invest in causal discovery before calibration. Draw your DAGs. Identify your minimal adjustment sets. No amount of experimental evidence will save a model asking the wrong causal question.\nAs Pearl might say: statistics tells us what the data says; causality tells us what to do with it.\nCalibration without causation is just computation without comprehension!\n\n\nCode\n%load_ext watermark\n%watermark -n -u -v -iv -w -p pymc_marketing,pytensor\n\n\nLast updated: Wed Sep 03 2025\n\nPython implementation: CPython\nPython version       : 3.11.8\nIPython version      : 8.30.0\n\npymc_marketing: 0.15.1\npytensor      : 2.31.7\n\narviz         : 0.21.0\npandas        : 2.2.3\npytensor      : 2.31.7\nnumpy         : 2.1.3\nmatplotlib    : 3.10.1\npreliz        : 0.20.0\nseaborn       : 0.13.2\npymc          : 5.25.1\nIPython       : 8.30.0\ngraphviz      : 0.20.3\npymc_marketing: 0.15.1\n\nWatermark: 2.5.0"
  },
  {
    "objectID": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html",
    "href": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html",
    "title": "Baby Steps for Causal Discovery",
    "section": "",
    "text": "In this notebook, we‚Äôll dive into how to uncover causal relationships in marketing data, a crucial step for understanding the true impact of various channels on business outcomes. We‚Äôll start by generating synthetic data that mimics real-world marketing scenarios, complete with confounding variables and complex causal structures.\nNext, we‚Äôll fit a Bayesian marketing mix model using PyMC-Marketing, check causal directions between variables, and perform mediation analysis to explore indirect effects. Finally, we‚Äôll use structure discovery techniques to infer potential causal graphs. By the end, you‚Äôll have a solid grasp of how to apply these techniques to reveal hidden causal insights in your marketing data.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pymc_marketing.mmm.transformers import geometric_adstock, michaelis_menten\n\nfrom pymc_marketing.mmm import MMM, GeometricAdstock, MichaelisMentenSaturation\nfrom pymc_marketing.prior import Prior\n\nimport networkx as nx\nfrom graphviz import Digraph\nimport pydot\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nfrom IPython.display import SVG, display\n\nimport numpy as np\nimport pandas as pd\n\nimport preliz as pz\nimport pymc as pm\n\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom causallearn.graph.Endpoint import Endpoint\nfrom causallearn.utils.GraphUtils import GraphUtils\nfrom causallearn.search.ScoreBased.GES import ges\nfrom causallearn.search.ConstraintBased.PC import pc\n\naz.style.use(\"arviz-darkgrid\")\nplt.rcParams[\"figure.figsize\"] = [8, 4]\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"axes.labelsize\"] = 6\nplt.rcParams[\"xtick.labelsize\"] = 6\nplt.rcParams[\"ytick.labelsize\"] = 6\nplt.rcParams.update({\"figure.constrained_layout.use\": True})\n\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = \"retina\"\n\nseed = sum(map(ord, \"Estimating effects despite having Confounding Variables\"))\nrng = np.random.default_rng(seed)\n\nprint(seed)\nprint(rng)\n\n\n5395\nGenerator(PCG64)"
  },
  {
    "objectID": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#data-generation",
    "href": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#data-generation",
    "title": "Baby Steps for Causal Discovery",
    "section": "Data generation",
    "text": "Data generation\nBased on the provided DAG, we can create some synthetic data to test how our model performs when we have a complex causal structures. Using the same data, we can test different model compositions and see how could we improve our model to uncover the true causal impact of each channel on the target variable.\nWe‚Äôll start by setting the date range. Here we‚Äôll use a date range from 2022-01-01 to 2024-11-06, meaning we have almost 3 years of data (1041 days).\n\n\nCode\n# date range\nmin_date = pd.to_datetime(\"2022-01-01\")\nmax_date = pd.to_datetime(\"2024-11-06\")\ndate_range = pd.date_range(start=min_date, end=max_date, freq=\"D\")\n\ndf = pd.DataFrame(data={\"date_week\": date_range}).assign(\n    year=lambda x: x[\"date_week\"].dt.year,\n    month=lambda x: x[\"date_week\"].dt.month,\n    dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n)\n\nn = df.shape[0]\nprint(f\"Number of observations: {n}\")\n\n\nNumber of observations: 1041\n\n\n\nHoliday signal\nCertain holidays, like Christmas, can have a significant impact on consumer behavior before and after the specific date, leading to seasonal spikes in sales. To capture these effects, we introduce a holiday signal based on Gaussian (normal) distributions centered around specific holiday dates.\nThe function used to model the holiday effect is defined as follows:\n\\[\nH_{t} = \\exp\\left(-0.5 \\left(\\frac{\\Delta t}{\\sigma}\\right)^2\\right)\n\\]\nWhere: - \\(\\Delta t\\) is the time difference (in days) between the current date and the holiday date. - \\(\\sigma\\) is the standard deviation that controls the spread of the effect around the holiday date.\nFor each holiday, we calculate the holiday signal across the date range and add a holiday contribution by scaling the signal with a holiday-specific coefficient. This approach models seasonal holiday spikes using Gaussian functions, which capture the transient increase in market activity around holidays, and their respective decay over time.\n\nNote: Here we assume a normally distributed signal, nevertheless the signal could be skew or not normal distributed.\n\n\n\nCode\nholiday_dates = [\"24-12\", \"31-12\", \"08-06\", \"07-09\"]  # List of holidays as month-day strings\nstd_devs = [5, 5, 3, 3]  # List of standard deviations for each holiday\nholidays_coefficients = [2, 3, 4, 6]\n\n# Initialize the holiday effect array\nholiday_signal = np.zeros(len(date_range))\nholiday_contributions = np.zeros(len(date_range))\n\n# Generate holiday signals\nfor holiday, std_dev, holiday_coef in zip(\n    holiday_dates, std_devs, holidays_coefficients, strict=False\n):\n    # Find all occurrences of the holiday in the date range\n    holiday_occurrences = date_range[date_range.strftime(\"%d-%m\") == holiday]\n\n    for occurrence in holiday_occurrences:\n        # Calculate the time difference in days\n        time_diff = (date_range - occurrence).days\n\n        # Generate the Gaussian basis for the holiday\n        _holiday_signal = np.exp(-0.5 * (time_diff / std_dev) ** 2)\n\n        # Add the holiday signal to the holiday effect\n        holiday_signal += _holiday_signal\n\n        holiday_contributions += _holiday_signal * holiday_coef\n\ndf[\"holiday_signal\"] = holiday_signal\ndf[\"holiday_contributions\"] = holiday_contributions\n\n# Plot the holiday effect\nfig, ax = plt.subplots()\nsns.lineplot(x=date_range, y=holiday_signal, ax=ax)\nax.set(title=\"Holiday Effect Signal\", xlabel=\"Date\", ylabel=\"Signal Intensity\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGenerating inflation\nNext, we generate the data for Inflation. We assume the inflation follows a power-law trend, meaning that growth accelerates over time rather than remaining constant. This can be mathematically defined as:\n\\[\nIN_{t} = (t + \\text{baseline})^{\\text{exponent}} - 1\n\\]\nWhere: - \\(t\\): The time index, representing days since the start of the date range. - \\(baseline\\): A constant added to \\(t\\) to shift the starting point of the trend. This value affects the initial level of market growth. The starting value of the function will be \\((baseline)^{exponent} - 1\\), not 0. - \\(exponent\\): The power to which the time index is raised, determining the rate at which the trend accelerates over time.\n\n\nCode\ndf[\"inflation\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (2 / 4) - 1\n\nfig, ax = plt.subplots()\nsns.lineplot(\n    x=\"date_week\", y=\"inflation\", color=\"C2\", label=\"trend\", data=df, ax=ax\n)\nax.legend(loc=\"upper left\")\nax.set(title=\"Inflation Components\", xlabel=\"date\", ylabel=None);\n\n\n\n\n\n\n\n\n\n\n\nModeling Marketing Channels\nIn this section, we simulate three marketing channels, \\(x1\\), \\(x2\\), and \\(x3\\) which represent different advertising channels (e.g., Internal Marketing, Social Marketing, Offline Marketing). Each channel‚Äôs behavior is influenced by random variability and confounding effects from seasonal holidays. Here‚Äôs how we model each channel mathematically:\nChannel \\(x1\\): As mentioned before, we generate \\(x1\\) which is affected by the holiday signal, we could define it as:\n\\[\nI_{x1_t} = S_{x1_t} + e_{x1}\n\\]\nChannel \\(x2\\): On the other hand, we generate \\(x2\\) which is affected by the holiday signal, and the influence of \\(x1\\). We could define it as:\n\\[\nI_{x2_t} = S_{x2_t} + H_{t} \\times \\alpha_{x2} + (I_{x1_t} \\times \\alpha_{x1_x2}) + e_{x2}\n\\]\nChannel \\(x3\\): For the last variable, we generate \\(x3\\) which is affected by \\(x1\\) only.\n\\[\nI_{x3_t} = S_{x3_t} + (I_{x1_t} \\times \\alpha_{x1_x3}) + e_{x3}\n\\]\nThese equations allow us to capture the complex dynamics influencing each marketing channel: - Holiday Effects increase channel activity around specific dates, simulating seasonal spikes. - Cross-channel Influences introduce interdependencies, modeling how one channel‚Äôs success can amplify another‚Äôs.\n\nNote: Here we are assuming an additive impact for the channel interactions.\n\n\n\nCode\nx1 = pz.Gamma(mu=1, sigma=3).rvs(n, random_state=rng)\ncofounder_effect_holiday_x1 = 2.5\nx1_conv = np.convolve(x1, np.ones(14) / 14, mode=\"same\")\nnoise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\nx1_conv[:14] = x1_conv.mean() + noise[:14]\nx1_conv[-14:] = x1_conv.mean() + noise[14:]\ndf[\"x1\"] = x1_conv\n\nx2 = pz.Gamma(mu=2, sigma=2).rvs(n, random_state=rng)\ncofounder_effect_holiday_x2 = 2.2\ncofounder_effect_x1_x2 = 1.3\nx2_conv = np.convolve(x2, np.ones(18) / 12, mode=\"same\")\nnoise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\nx2_conv[:14] = x2_conv.mean() + noise[:14]\nx2_conv[-14:] = x2_conv.mean() + noise[14:]\ndf[\"x2\"] = (\n    x2_conv\n    + (holiday_signal * cofounder_effect_holiday_x2)\n    + (df[\"x1\"] * cofounder_effect_x1_x2)\n) # digital ads\n\nx3 = pz.Gamma(mu=5, sigma=1).rvs(n, random_state=rng)\ncofounder_effect_x1_x3 = 1.5\nx3_conv = np.convolve(x3, np.ones(16) / 10, mode=\"same\")\nnoise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\nx3_conv[:14] = x3_conv.mean() + noise[:14]\nx3_conv[-14:] = x3_conv.mean() + noise[14:]\ndf[\"x3\"] = (\n    x3_conv\n    + (df[\"x1\"] * cofounder_effect_x1_x3)\n) # internal marketing\n\n\nWe‚Äôll assume all of marketing activities suffer the same transformations Adstock and Saturation. This means, each channel will have individual parameters for the selected transformations, in this case Geometrick adstock and michaelis menten.\n\n\nCode\n# apply geometric adstock transformation\nalpha2: float = 0.4\nalpha3: float = 0.3\n\ndf[\"x2_adstock\"] = (\n    geometric_adstock(x=df[\"x2\"].to_numpy(), alpha=alpha2, l_max=24, normalize=True)\n    .eval()\n    .flatten()\n)\n\ndf[\"x3_adstock\"] = (\n    geometric_adstock(x=df[\"x3\"].to_numpy(), alpha=alpha3, l_max=24, normalize=True)\n    .eval()\n    .flatten()\n)\n\n\n# apply saturation transformation\nlam2: float = 6.0\nlam3: float = 12.0\n\nalpha_mm2: float = 12\nalpha_mm3: float = 18\n\ndf[\"x2_adstock_saturated\"] = michaelis_menten(\n    x=df[\"x2_adstock\"].to_numpy(), lam=lam2, alpha=alpha_mm2\n)\n\ndf[\"x3_adstock_saturated\"] = michaelis_menten(\n    x=df[\"x3_adstock\"].to_numpy(), lam=lam3, alpha=alpha_mm3\n)\n\nfig, ax = plt.subplots(\n    nrows=3, ncols=2, sharex=True, sharey=False, layout=\"constrained\"\n)\nsns.lineplot(x=\"date_week\", y=\"x2\", data=df, color=\"C1\", ax=ax[0, 0])\nsns.lineplot(x=\"date_week\", y=\"x3\", data=df, color=\"C2\", ax=ax[0, 1])\n\nsns.lineplot(x=\"date_week\", y=\"x2_adstock\", data=df, color=\"C1\", ax=ax[1, 0])\nsns.lineplot(x=\"date_week\", y=\"x3_adstock\", data=df, color=\"C2\", ax=ax[1, 1])\n\nsns.lineplot(x=\"date_week\", y=\"x2_adstock_saturated\", data=df, color=\"C1\", ax=ax[2, 0])\nsns.lineplot(x=\"date_week\", y=\"x3_adstock_saturated\", data=df, color=\"C2\", ax=ax[2, 1])\n\nfig.suptitle(\"Media Costs Data - Transformed\", fontsize=16)\n# adjust size of X axis\nax[2, 0].tick_params(axis=\"x\", labelsize=8)\nax[2, 1].tick_params(axis=\"x\", labelsize=8)\n\n# adjust size of x axis labels\nfor ax in ax.flat:\n    ax.tick_params(axis=\"x\", labelsize=6)\n\n\n\n\n\n\n\n\n\nThe previous plot shows how the transformations affect each variable, and what would be the true contribution after each transformation.\n\n\nTarget variable\nThe target variable is a combination of all variables before. The mathematical formula can be expressed as:\n\\[\ny_{t} = Intercept - f(IN_{t}) + f(H_{t}) + m(I_{x3_t}) + m(I_{x2_t}) + \\epsilon\n\\]\nWhere: - Intercept: A baseline level of sales, set to 1.5, representing the base sales level in the absence of other effects. - Inflation: Represents the underlying market inflation, with an implicit negative coefficient of 1, adding a steady downward influence. - Holiday Contributions: Adds sales spikes around holiday periods, capturing the seasonal increase in consumer demand. - \\(m(Impressions_{x3_t})\\) and \\(m(Impressions_{x2_t})\\): Represent the saturated adstock values for the marketing channels \\(x3\\) and \\(x2\\). - Noise \\(\\epsilon\\): A small random error term, drawn from a normal distribution with mean 0 and standard deviation 0.08, to account for unexplained variability in sales.\n\n\nCode\ndf[\"intercept\"] = 1.5\ndf[\"epsilon\"] = rng.normal(loc=0.0, scale=0.08, size=n)\n\ndf[\"y\"] = (\n    df[\"intercept\"]\n    + df[\"holiday_contributions\"]\n    + df[\"x2_adstock_saturated\"]\n    + df[\"x3_adstock_saturated\"]\n    + df[\"epsilon\"]  # Noise\n) - df[\"inflation\"]\n\nfig, ax = plt.subplots()\nsns.lineplot(x=\"date_week\", y=\"y\", color=\"black\", data=df, ax=ax)\nax.set(title=\"Sales (Target Variable)\", xlabel=\"date\", ylabel=\"y (thousands)\");\n\n\n\n\n\n\n\n\n\nWe can scale the full dataset and we‚Äôll have finally something very similar to reality.\n\n\nCode\n# scale df by abs max per column\ndf[\"date\"] = pd.to_datetime(df[\"date_week\"])\nscaled_df = df.copy()\nfor col in scaled_df.columns:\n    if col != 'date' and col != 'date_week':\n        scaled_df[col] = scaled_df[col] / scaled_df[col].abs().max()\n\nscaled_df[[\"date\", \"x1\", \"x2\", \"x3\", \"y\"]].head()\n\n\n\n\n\n\n\n\n\ndate\nx1\nx2\nx3\ny\n\n\n\n\n0\n2022-01-01\n0.311103\n0.416608\n0.709209\n0.523628\n\n\n1\n2022-01-02\n0.346200\n0.417910\n0.718519\n0.641921\n\n\n2\n2022-01-03\n0.310798\n0.404066\n0.690251\n0.664626\n\n\n3\n2022-01-04\n0.301643\n0.395861\n0.693495\n0.672532\n\n\n4\n2022-01-05\n0.248688\n0.379114\n0.679055\n0.668596"
  },
  {
    "objectID": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#causal-discovery-algorithms",
    "href": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#causal-discovery-algorithms",
    "title": "Baby Steps for Causal Discovery",
    "section": "Causal Discovery Algorithms",
    "text": "Causal Discovery Algorithms\nThe Peter-Clark algorithm is a constraint-based method that infers causal structures from observational data using conditional independence tests. It starts with a fully connected undirected graph where every variable is initially connected to every other variable. The algorithm systematically tests conditional independence between pairs of variables, conditioning on increasingly larger subsets of other variables. When a conditional independence is detected, the corresponding edge is removed from the graph.\nOn the other hand, Greedy Search is a score-based method that iteratively improves a candidate causal model by locally modifying its structure. It begins with an initial directed acyclic graph and evaluates a scoring metric that balances goodness-of-fit with model complexity. The algorithm explores modifications such as adding, deleting, or reversing edges to find local improvements in the score. At each iteration, it selects the change that produces the highest increase in the score, following a step-by-step improvement strategy. The search continues until no single modification can further enhance the model‚Äôs score. This method efficiently navigates the combinatorial search space of possible graphs by making locally optimal choices."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm a globe-trotting data professional with a passion for statistics and mathematics. From studying in Venezuela to collaborating with teams across continents, my journey has been a whirlwind of diverse experiences. I‚Äôm all about using data to transform businesses and solve complex problems.\nInterested in learning more about how to navigate the digital landscape and harness the power of data? Check out my articles below and let‚Äôs connect for some data-driven discussions and creative problem-solving. Looking forward to hearing from you! üëáüèª"
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About Me",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n2024 - Present\n\n\nMarketing Scientist\n\n\nWise\n\n\nTallinn, Estonia\n\n\nData scientist for the Marketing team, dedicated to measuring and improving the region‚Äôs global strategies through the use of data.\n\n\nCareer Progression at Wise\n\nLead Data Scientist (March 2025 - Present)\n\n[Role responsibilities to be added]\n\nSenior Marketing Scientist (January 2024 - March 2025)\n\n[Role responsibilities to be added]\n\n\n\n\nRead more\n\n\n\n\n2024 - Present\n\n\nPrincipal Data Scientist\n\n\nPyMC Labs\n\n\nRemote\n\n\nSolving complex business problems through advanced Bayesian statistical methods.\n\n\n\nSolving business problems through Bayesian Statistics\n[Additional responsibilities to be added]\n\n\n\nRead more\n\n\n\n\n2022 - 2024\n\n\nMarketing ads, Functional Analyst\n\n\nBolt\n\n\nTallinn, Estonia\n\n\nI relocated from Chile to Estonia and am currently applying data analytics to determine the most effective strategies for optimizing acquisition marketing efforts based on profitability.\n\n\n\nApplied data analytics to determine best strategies to optimize acquisition marketing efforts based on profitability across Europe and South Africa\nPerformed daily analysis using Python or R to run marketing mix, causal impact and budget allocation models\nEstimated marketing effects around the world to achieve company goals\nRelocated from Chile to Estonia\n\n\n\nRead more\n\n\n\n\n2020 - 2022\n\n\nHead of Data Analytics\n\n\nOmnicom Media Group\n\n\nSantiago, Chile\n\n\nLed the data department of the Marketing Science LATAM group, developing value solutions and creating analytical services around the region.\n\n\nCareer Progression at Omnicom Media Group\n\nHead of Data Analytics (October 2021 - January 2022)\n\nLed the data department of the Marketing Science LATAM group\nDeveloped value solutions and created analytical services around the region\nUtilized cloud technologies (AWS/GCP) to develop sustainable data structures\nMaintained direct communication with Google and Facebook teams to develop use cases and service optimization\nCreated innovation proposals and fostered data culture to optimize operational processes\n\nData Analytics Manager (July 2020 - October 2021)\n\nManaged analyst team and ensured correct delivery of dashboards on platforms such as Tableau, Power BI, and Data Studio\nDeveloped architectures in GCP and managed databases in AWS\nMaintained direct contact with OMG teams in Miami for data centralization and optimization of information flows\nUtilized visualization tools, Python/R, SQL, and NoSQL for data pre-processing\n\n\n\n\nRead more\n\n\n\n\n2019 - 2020\n\n\nData Analyst\n\n\nRaya\n\n\nSantiago, Chile\n\n\nManaged analysis and reporting for different agency brands including Huawei, MercadoLibre, Concha y Toro, and Walmart Chile.\n\n\n\nResponsible for interpretation, analysis, and reporting for different agency brands\nManaged Brandwatch, Google Analytics, Tag Manager and similar tools to determine guidelines for paid or organic campaigns\nDeveloped categorization and prediction models in Python for process optimization and advanced variable analysis\nWorked with brands including Huawei, MercadoLibre, Concha y Toro, and Walmart Chile\n\n\n\nRead more\n\n\n\n\n2018 - 2019\n\n\nPerformance Manager\n\n\nOYM Agencia\n\n\nSantiago, Chile\n\n\nManaged digital marketing strategies for multiple brands, including analysis, optimization and investment across major platforms.\n\n\n\nManaged analysis, optimization and investment in digital platforms including Facebook, Google Ads, and LinkedIn\nDeveloped dashboards in Cyfe and DataStudio\nWorked with brands including Jansport, BodyAndSoul, Ursus Trotter, and over 15 Chilean real estate companies\n\n\n\nRead more"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\nData Scientist\nAc√°mica - 2020\n\nSpecialized in Data Science\n\n\n\nBachelor‚Äôs in Market Studies\nUniversidad Jos√© Antonio P√°ez - 2015-2019\n\nSpecialized in Market Research"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\n\n\nüßëüèª‚Äçüíª\n\n\nProfessional Skills\n\n\n\n\nSEO/SEM Marketing\n\n\nProject Management\n\n\nPaid Social Media\n\n\nStatistical Analysis\n\n\nCausal Analysis\n\n\nLifetime Value Analysis\n\n\n\n\nüìä\n\n\nLanguages\n\n\n\n\nHTML\n\n\nNon-SQL\n\n\nSQL\n\n\nPython\n\n\nJulia\n\n\nR\n\n\n\n\n‚öôÔ∏è\n\n\nTechnologies\n\n\n\n\nGoogle Cloud Platform\n\n\nAWS\n\n\nDocker\n\n\nGit/GitHub\n\n\nJupyter\n\n\nRStudio"
  },
  {
    "objectID": "about.html#download-cv",
    "href": "about.html#download-cv",
    "title": "About Me",
    "section": "Download CV",
    "text": "Download CV\nDownload CV (PDF)"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let‚Äôs Connect",
    "text": "Let‚Äôs Connect\nHave a project in mind or just want to chat about data? There are two easy ways to get in touch:\n\n\nContact Me\n\n\nBook 30 Minutes"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Welcome to my collection of articles. Here you‚Äôll find my thoughts, tutorials, and research on various topics."
  },
  {
    "objectID": "articles.html#featured-articles",
    "href": "articles.html#featured-articles",
    "title": "Articles",
    "section": "Featured Articles",
    "text": "Featured Articles\n\n\n\nBayesian Models and Risk Optimization\n\n\nAn article discussing the importance of causality in experiments. Talk given in PyData Berlin 2025.\n\nRead More\n\n\n\n\n\n\n\n\nNo More Experiments Without Causality\n\n\nAn article discussing the importance of causality in experiments. Talk given in PyData DE Darmstadt 2025.\n\nRead More\n\n\n\n\n\n\n\n\nBaby Steps for Causal Discovery\n\n\nAn article discussing the importance of causality in experiments. Talk given in PyData Tallinn 2025.\n\nRead More"
  },
  {
    "objectID": "articles.html#all-articles",
    "href": "articles.html#all-articles",
    "title": "Articles",
    "section": "All Articles",
    "text": "All Articles\n\nBayesian Models and Risk Optimization - August 2025\nNo More Experiments Without Causality - April 2025\nBaby Steps for Causal Discovery - February 2025"
  },
  {
    "objectID": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html",
    "href": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html",
    "title": "Bayesian Models and Risk Optimization",
    "section": "",
    "text": "This article explores how Bayesian Media Mix Modeling (MMM) represents uncertainty and how we can optimize budget decisions under risk. We build a generative view of media response (carryover via adstock, diminishing returns via saturation, trend and seasonality) and use full posterior predictive distributions to compare allocations not only by expected outcomes but also by dispersion and tail risk.\nThis material accompanies my PyData Berlin 2025 talk, where I discuss practical risk-aware optimization for MMM: moving beyond mean-only plans to objectives that explicitly incorporate uncertainty‚Äîand how to communicate these trade-offs to stakeholders."
  },
  {
    "objectID": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#date-range",
    "href": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#date-range",
    "title": "Bayesian Models and Risk Optimization",
    "section": "üìÜ Date range",
    "text": "üìÜ Date range\nWe start by defining the date range.\n\n\nCode\n# date range\nmin_date = pd.to_datetime(\"2024-09-01\")\nmax_date = pd.to_datetime(\"2025-09-01\")\n\ndf = pd.DataFrame(\n    data={\"date_week\": pd.date_range(start=min_date, end=max_date, freq=\"W-MON\")}\n)\n\nn = df.shape[0]\nprint(f\"Number of observations: {n}\")\nprint(\"Date Range: {} to {}\".format(df.date_week.min(), df.date_week.max()))\n\n\nNumber of observations: 53\nDate Range: 2024-09-02 00:00:00 to 2025-09-01 00:00:00"
  },
  {
    "objectID": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#media-data",
    "href": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#media-data",
    "title": "Bayesian Models and Risk Optimization",
    "section": "üì£ Media data",
    "text": "üì£ Media data\n\n\nCode\n# media data\nscaler_x1 = 300\nscaler_x2 = 280\nscaler_x3 = 50\nscaler_x4 = 100\ny_scaler = 1000\n\n# media data\nx1 = rng.uniform(low=0.0, high=1.0, size=n)\ndf[\"x1\"] = np.where(x1 &gt; 0.8, x1, x1 / 2)\n\nx2 = rng.uniform(low=0.0, high=0.6, size=n)\ndf[\"x2\"] = np.where(x2 &gt; 0.5, x2, 0)\n\nx3 = rng.uniform(low=0.0, high=0.8, size=n)\ndf[\"x3\"] = np.where(x3 &gt; 0.7, x3, x3 / 6)\n\nx4 = rng.uniform(low=0.0, high=0.2, size=n)\ndf[\"x4\"] = np.where(x4 &gt; 0.15, x4, x4 / 2)\n\n\nfig, ax = plt.subplots(\n    nrows=4, ncols=1, sharex=True, sharey=True, layout=\"constrained\"\n)\nsns.lineplot(x=\"date_week\", y=\"x1\", data=df, color=\"C0\", ax=ax[0])\nsns.lineplot(x=\"date_week\", y=\"x2\", data=df, color=\"C1\", ax=ax[1])\nsns.lineplot(x=\"date_week\", y=\"x3\", data=df, color=\"C2\", ax=ax[2])\nsns.lineplot(x=\"date_week\", y=\"x4\", data=df, color=\"C3\", ax=ax[3])\nax[3].set(xlabel=\"date\")\nfig.suptitle(\"Media Costs Data\", fontsize=16);"
  },
  {
    "objectID": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#trend-and-seasonality-components",
    "href": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#trend-and-seasonality-components",
    "title": "Bayesian Models and Risk Optimization",
    "section": "üìà Trend and seasonality components",
    "text": "üìà Trend and seasonality components\nWe define trend and seasonality. Seasonality follows a 4-week cycle modeled with a Fourier basis; trend is linear.\n\n\nCode\n# Create Fourier components for monthly seasonality\nmonthly_period = 4  # 4-week cycle\nt = np.arange(n)\n\n# Create sin-cos signals for fourier components\nmonthly_sin = np.sin(2 * np.pi * t / monthly_period)\nmonthly_cos = np.cos(2 * np.pi * t / monthly_period)\n\n# Combine sin-cos to create the desired pattern\n# Use coefficients to shape the pattern\nmonthly_pattern = 0.6 * monthly_sin + 0.4 * monthly_cos\n\n# Apply smoothing using ndimage to reduce sharp transitions\nmonthly_pattern = ndimage.gaussian_filter1d(monthly_pattern, sigma=0.2)\n\n# Normalize to [-1, 1] range\nmonthly_pattern = (monthly_pattern / np.max(np.abs(monthly_pattern))) * .18\n\ndf[\"monthly_effect\"] = monthly_pattern\ndf[\"trend\"] = (np.linspace(start=0.0, stop=10, num=n) + 10) ** (1 / 8) - 1\n\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\n# Plot monthly pattern\nax1.plot(df[\"date_week\"], monthly_pattern, label=\"Monthly Pattern (Smoothed)\", linewidth=2, color='blue')\nax1.set_ylabel(\"Pattern Value\")\nax1.set_title(\"Monthly Fourier Pattern (4-week cycle, Smoothed)\")\nax1.grid(True, alpha=0.3)\nax1.legend()\n\n# Plot trend\nax2.plot(df[\"date_week\"], df[\"trend\"], label=\"Trend\", linewidth=2, color='red')\nax2.set_xlabel(\"Date\")\nax2.set_ylabel(\"Trend Value\")\nax2.set_title(\"Linear Trend Component\")\nax2.grid(True, alpha=0.3)\nax2.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#adstock-and-saturation-transformations",
    "href": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#adstock-and-saturation-transformations",
    "title": "Bayesian Models and Risk Optimization",
    "section": "üîÅ Adstock and saturation transformations",
    "text": "üîÅ Adstock and saturation transformations\nFirst, we apply the adstock transformation to the media data.\n\n\nCode\n# apply geometric adstock transformation\nalpha: float = 0.55\n\ndf[\"x1_adstock\"] = (\n    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x1\"].to_numpy(), alpha=alpha)\n    .eval()\n)\n\ndf[\"x2_adstock\"] = (\n    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x2\"].to_numpy(), alpha=alpha)\n    .eval()\n)\n\ndf[\"x3_adstock\"] = (\n    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x3\"].to_numpy(), alpha=alpha)\n    .eval()\n)\n\ndf[\"x4_adstock\"] = (\n    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x4\"].to_numpy(), alpha=alpha)\n    .eval()\n)\n\ndf.head()\n\n\n\n\n\n\n\n\n\ndate_week\nx1\nx2\nx3\nx4\nmonthly_effect\ntrend\nx1_adstock\nx2_adstock\nx3_adstock\nx4_adstock\n\n\n\n\n0\n2024-09-02\n0.342627\n0.581017\n0.009625\n0.072294\n0.120001\n0.333521\n0.155484\n0.263665\n0.004368\n0.032807\n\n\n1\n2024-09-09\n0.349177\n0.000000\n0.018831\n0.007122\n0.180000\n0.336700\n0.243973\n0.145016\n0.010948\n0.021276\n\n\n2\n2024-09-16\n0.292217\n0.000000\n0.029268\n0.011185\n-0.120000\n0.339827\n0.266793\n0.079759\n0.019303\n0.016778\n\n\n3\n2024-09-23\n0.893449\n0.000000\n0.073220\n0.056761\n-0.180000\n0.342904\n0.552183\n0.043867\n0.043844\n0.034986\n\n\n4\n2024-09-30\n0.197823\n0.000000\n0.073854\n0.175182\n0.120000\n0.345932\n0.393473\n0.024127\n0.057629\n0.098740\n\n\n\n\n\n\n\nThen we apply the saturation transformation to the adstock transformed media data.\n\n\nCode\nalpha_sat_x1: float = 0.3\nlam_sat_x1: float = 1.1\n\nalpha_sat_x2: float = 0.1\nlam_sat_x2: float = 1.5\n\nalpha_sat_x3: float = 0.2\nlam_sat_x3: float = 0.3\n\nalpha_sat_x4: float = 0.8\nlam_sat_x4: float = 0.8\n\ndf[\"x1_adstock_saturated\"] = (\n    MichaelisMentenSaturation().function(\n        x=df[\"x1_adstock\"].to_numpy(),\n        alpha=alpha_sat_x1,\n        lam=lam_sat_x1,\n    ).eval()\n)\n\ndf[\"x2_adstock_saturated\"] = (\n    MichaelisMentenSaturation().function(\n        x=df[\"x2_adstock\"].to_numpy(),\n        alpha=alpha_sat_x2,\n        lam=lam_sat_x2,\n    ).eval()\n)\n\ndf[\"x3_adstock_saturated\"] = (\n    MichaelisMentenSaturation().function(\n        x=df[\"x3_adstock\"].to_numpy(),  \n        alpha=alpha_sat_x3,\n        lam=lam_sat_x3,\n    ).eval()\n)\n\ndf[\"x4_adstock_saturated\"] = (\n    MichaelisMentenSaturation().function(\n        x=df[\"x4_adstock\"].to_numpy(),\n        alpha=alpha_sat_x4,\n        lam=lam_sat_x4,\n    ).eval()\n)\n\ndf.head()\n\n\n\n\n\n\n\n\n\ndate_week\nx1\nx2\nx3\nx4\nmonthly_effect\ntrend\nx1_adstock\nx2_adstock\nx3_adstock\nx4_adstock\nx1_adstock_saturated\nx2_adstock_saturated\nx3_adstock_saturated\nx4_adstock_saturated\n\n\n\n\n0\n2024-09-02\n0.342627\n0.581017\n0.009625\n0.072294\n0.120001\n0.333521\n0.155484\n0.263665\n0.004368\n0.032807\n0.037153\n0.014950\n0.002870\n0.031515\n\n\n1\n2024-09-09\n0.349177\n0.000000\n0.018831\n0.007122\n0.180000\n0.336700\n0.243973\n0.145016\n0.010948\n0.021276\n0.054459\n0.008815\n0.007042\n0.020725\n\n\n2\n2024-09-16\n0.292217\n0.000000\n0.029268\n0.011185\n-0.120000\n0.339827\n0.266793\n0.079759\n0.019303\n0.016778\n0.058559\n0.005049\n0.012091\n0.016433\n\n\n3\n2024-09-23\n0.893449\n0.000000\n0.073220\n0.056761\n-0.180000\n0.342904\n0.552183\n0.043867\n0.043844\n0.034986\n0.100264\n0.002841\n0.025502\n0.033520\n\n\n4\n2024-09-30\n0.197823\n0.000000\n0.073854\n0.175182\n0.120000\n0.345932\n0.393473\n0.024127\n0.057629\n0.098740\n0.079038\n0.001583\n0.032228\n0.087892\n\n\n\n\n\n\n\nLet‚Äôs visualize how the media data look after adstock and saturation, and how they translate into units of Y (app installs or revenue).\n\n\nCode\nfig, ax = plt.subplots(\n    nrows=3, ncols=4, sharex=True, sharey=False, layout=\"constrained\"\n)\nsns.lineplot(x=\"date_week\", y=\"x1\", data=df, color=\"C0\", ax=ax[0, 0])\nsns.lineplot(x=\"date_week\", y=\"x2\", data=df, color=\"C1\", ax=ax[0, 1])\nsns.lineplot(x=\"date_week\", y=\"x1_adstock\", data=df, color=\"C0\", ax=ax[1, 0])\nsns.lineplot(x=\"date_week\", y=\"x2_adstock\", data=df, color=\"C1\", ax=ax[1, 1])\nsns.lineplot(x=\"date_week\", y=\"x1_adstock_saturated\", data=df, color=\"C0\", ax=ax[2, 0])\nsns.lineplot(x=\"date_week\", y=\"x2_adstock_saturated\", data=df, color=\"C1\", ax=ax[2, 1])\nsns.lineplot(x=\"date_week\", y=\"x3\", data=df, color=\"C2\", ax=ax[0, 2])\nsns.lineplot(x=\"date_week\", y=\"x3_adstock\", data=df, color=\"C2\", ax=ax[1, 2])\nsns.lineplot(x=\"date_week\", y=\"x3_adstock_saturated\", data=df, color=\"C2\", ax=ax[2, 2])\nsns.lineplot(x=\"date_week\", y=\"x4\", data=df, color=\"C3\", ax=ax[0, 3])\nsns.lineplot(x=\"date_week\", y=\"x4_adstock\", data=df, color=\"C3\", ax=ax[1, 3])\nsns.lineplot(x=\"date_week\", y=\"x4_adstock_saturated\", data=df, color=\"C3\", ax=ax[2, 3])\nfig.suptitle(\"Media Costs Data - Transformed\", fontsize=16);\n\n\n\n\n\n\n\n\n\nWe now add the intercept and noise, and sum the transformed media, trend, and seasonality components.\n\n\nCode\ndf[\"intercept\"] = 0.15\ndf[\"epsilon\"] = rng.normal(loc=0.0, scale=0.075, size=n)\n\ndf[\"app_installs\"] = df[[\"intercept\", \"x1_adstock_saturated\", \"x2_adstock_saturated\", \"x3_adstock_saturated\", \"x4_adstock_saturated\", \"trend\", \"monthly_effect\", \"epsilon\"]].sum(axis=1)\ndf[\"app_installs\"] *= y_scaler\ndf.head()\n\n\n\n\n\n\n\n\n\ndate_week\nx1\nx2\nx3\nx4\nmonthly_effect\ntrend\nx1_adstock\nx2_adstock\nx3_adstock\nx4_adstock\nx1_adstock_saturated\nx2_adstock_saturated\nx3_adstock_saturated\nx4_adstock_saturated\nintercept\nepsilon\napp_installs\n\n\n\n\n0\n2024-09-02\n0.342627\n0.581017\n0.009625\n0.072294\n0.120001\n0.333521\n0.155484\n0.263665\n0.004368\n0.032807\n0.037153\n0.014950\n0.002870\n0.031515\n0.15\n0.154459\n844.469551\n\n\n1\n2024-09-09\n0.349177\n0.000000\n0.018831\n0.007122\n0.180000\n0.336700\n0.243973\n0.145016\n0.010948\n0.021276\n0.054459\n0.008815\n0.007042\n0.020725\n0.15\n0.177488\n935.229200\n\n\n2\n2024-09-16\n0.292217\n0.000000\n0.029268\n0.011185\n-0.120000\n0.339827\n0.266793\n0.079759\n0.019303\n0.016778\n0.058559\n0.005049\n0.012091\n0.016433\n0.15\n0.052790\n514.748757\n\n\n3\n2024-09-23\n0.893449\n0.000000\n0.073220\n0.056761\n-0.180000\n0.342904\n0.552183\n0.043867\n0.043844\n0.034986\n0.100264\n0.002841\n0.025502\n0.033520\n0.15\n-0.034546\n440.486094\n\n\n4\n2024-09-30\n0.197823\n0.000000\n0.073854\n0.175182\n0.120000\n0.345932\n0.393473\n0.024127\n0.057629\n0.098740\n0.079038\n0.001583\n0.032228\n0.087892\n0.15\n0.090876\n907.549889\n\n\n\n\n\n\n\nThis is how the target looks.\n\n\nCode\ndf.set_index(\"date_week\").app_installs.plot();\n\n\n\n\n\n\n\n\n\nWe also add the original media to the DataFrame so we can visualize it before any transformations and use it as model input.\n\n\nCode\ndf[[\"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"x4_original_scale\"]] = df[[\"x1\", \"x2\", \"x3\", \"x4\"]]\ndf[\"x1_original_scale\"] *= scaler_x1\ndf[\"x2_original_scale\"] *= scaler_x2\ndf[\"x3_original_scale\"] *= scaler_x3\ndf[\"x4_original_scale\"] *= scaler_x4\n\ndf[[\"date_week\", \"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"app_installs\"]].head()\n\n\n\n\n\n\n\n\n\ndate_week\nx1_original_scale\nx2_original_scale\nx3_original_scale\napp_installs\n\n\n\n\n0\n2024-09-02\n102.788211\n162.684671\n0.481258\n844.469551\n\n\n1\n2024-09-09\n104.753127\n0.000000\n0.941552\n935.229200\n\n\n2\n2024-09-16\n87.665114\n0.000000\n1.463382\n514.748757\n\n\n3\n2024-09-23\n268.034637\n0.000000\n3.661005\n440.486094\n\n\n4\n2024-09-30\n59.346861\n0.000000\n3.692695\n907.549889"
  },
  {
    "objectID": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#define-the-optimizer",
    "href": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#define-the-optimizer",
    "title": "Bayesian Models and Risk Optimization",
    "section": "üõ†Ô∏è Define the optimizer",
    "text": "üõ†Ô∏è Define the optimizer\nInitializing the optimizer is straightforward: pass the model and the date range.\n\n\nCode\noptimizable_model = MultiDimensionalBudgetOptimizerWrapper(\n    model=mmm, \n    start_date=df_test.date_week.min().strftime(\"%Y-%m-%d\"), \n    end_date=df_test.date_week.max().strftime(\"%Y-%m-%d\")\n)\nprint(f\"Start date: {optimizable_model.start_date}\")\nprint(f\"End date: {optimizable_model.end_date}\")\n\n\nStart date: 2025-09-01\nEnd date: 2025-09-01\n\n\nWe‚Äôll use the test set to define the budget and optimization period so we can compare the resulting allocation to our current plan.\n\n\nCode\nchannels = [\"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"x4_original_scale\"]\nnum_periods = optimizable_model.num_periods\ntime_unit_budget = df_test[channels].sum(axis=1).mean()\nprint(f\"Total budget to allocate: {num_periods * time_unit_budget:,.0f}\")\n\n\nTotal budget to allocate: 59\n\n\nGiven the budget and channels, we can estimate the response for our initial plan.\n\n\nCode\ninitial_budget = df_test[channels].sum(axis=0).to_xarray().rename({\"index\":\"channel\"})\ninitial_posterior_response = optimizable_model.sample_response_distribution(\n    allocation_strategy=initial_budget,\n    include_carryover=True,\n    include_last_observations=False,\n    additional_var_names=[\"y_original_scale\"]\n)\n\nfig, ax = optimizable_model.plot.budget_allocation(\n    samples=initial_posterior_response,\n    figsize=default_figsize,\n)\n\n\n\n\n\n\n\n\n\nThe default plot.budget_allocation makes a bar chart with allocation and response per channel. In order to see totals, we can sum and create a simple scatter plot with a label for the ROAS.\n\n\nCode\n# Create scatterplot with spend and mean response\nspend = initial_posterior_response.allocation.sum().values\nmean_response = initial_posterior_response.total_media_contribution_original_scale.mean(dim='sample').values\n\ninitial_roas = mean_response / spend\n\nplt.scatter(spend, mean_response, alpha=0.7, s=100, label=f\"ROAS: {initial_roas:.2f}\")\nplt.xlabel('Spend')\nplt.ylabel('Mean Response')\nplt.title('Spend vs Mean Response')\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize='small', loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe got a ROAS of \\(3.9\\) for the initial plan, which is below our target ROAS (let‚Äôs say \\(8\\)). Now we can run a vanilla optimization to response the question: can we reallocate to achieve a higher response given the same budget?\n\n\nCode\nallocation_strategy, optimization_result = optimizable_model.optimize_budget(\n    budget=time_unit_budget,\n)\n\nnaive_posterior_response = optimizable_model.sample_response_distribution(\n    allocation_strategy=allocation_strategy,\n    include_carryover=True,\n    include_last_observations=False,\n    additional_var_names=[\"y_original_scale\"]\n)\n\nprint(\"Budget allocation by channel:\")\nfor channel in channels:\n    print(\n        f\"  {channel}: {naive_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n    )\nprint(\n    f\"Total Allocated Budget: {np.sum(naive_posterior_response.allocation.to_numpy()):,.0f}\"\n)\n\nfig, ax = optimizable_model.plot.budget_allocation(\n    samples=naive_posterior_response,\n    figsize=default_figsize,\n)\n\n\nBudget allocation by channel:\n  x1_original_scale: 0\n  x2_original_scale: 0\n  x3_original_scale: 16\n  x4_original_scale: 42\nTotal Allocated Budget: 59\n\n\n\n\n\n\n\n\n\nYes, we do. Let‚Äôs compare the optimized response against the baseline plan.\n\n\nCode\n# Create scatterplot with spend and mean response\nmean_response_v2 = naive_posterior_response.total_media_contribution_original_scale.mean(dim='sample').values\nroas_v2 = mean_response_v2 / spend\n\n# Calculate the delta in response\nresponse_delta = mean_response_v2.sum() - mean_response.sum()\n\nplt.scatter(spend, mean_response_v2, alpha=0.7, s=100, color=\"blue\", label=f\"Optimized allocation (+{response_delta:.1f} response, ROAS: {roas_v2:.2f})\")\nplt.scatter(spend, mean_response, alpha=0.7, s=100, color=\"red\", label=\"Guessed allocation\")\n\nplt.xlabel('Spend')\nplt.ylabel('Mean Response')\nplt.title('Spend vs Mean Response')\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize='small', loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe optimized allocation is ~\\(500\\) units higher than the guessed allocation, and the new estimated ROAS is \\(13\\) which it‚Äôs over our expectations. As consequence, we assume we‚Äôll get an estimate Y revenue in the next N periods and planning against this incoming cashflow we‚Äôll get back.\nThe plotwist? We got a lower response, which mean a lower ROAS and we got in serious financial problems because we don‚Äôt have enough cash to payback providers or services.\nWhy this happen? Maximizing the mean response is risk-neutral. It often reallocates budget toward regions with potential high returns even if they are weakly identified, increasing dispersion of outcomes. This is rational when stakeholders are indifferent to risk. However, that‚Äôs not the case for every company. Sometimes our stakeholders need to know how certain we are about expected outcomes.\nBy inspecting posterior predictive samples under each allocation, we can quantify uncertainty and answer this question, based on the model current understanding.\nLet‚Äôs plot the response distributions for both baseline and optimized allocations.\n\n\nCode\nfig, ax = plt.subplots()\n\n# Get the values\noptimized_values = naive_posterior_response.total_media_contribution_original_scale.values\nguessed_values = initial_posterior_response.total_media_contribution_original_scale.values\n\n# Plot distributions\naz.plot_dist(\n    optimized_values,\n    color=\"blue\",\n    label=\"Optimized allocation\",\n    ax=ax,\n)\naz.plot_dist(\n    guessed_values,\n    color=\"red\",\n    label=\"Guessed allocation\",\n    ax=ax,\n)\n\n# Calculate means\noptimized_mean = optimized_values.mean()\nguessed_mean = guessed_values.mean()\n\n# Add vertical lines for means\nax.axvline(optimized_mean, color=\"blue\", linestyle=\"--\", alpha=0.8)\nax.axvline(guessed_mean, color=\"red\", linestyle=\"--\", alpha=0.8)\n\n# Add text boxes with mean values\nax.text(optimized_mean + 10, ax.get_ylim()[1] * 0.8, \n        f'Optimized Mean:\\n{optimized_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n        ha='left', va='center')\n\nax.text(guessed_mean - 10, ax.get_ylim()[1] * 0.6, \n        f'Guessed Mean:\\n{guessed_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.7),\n        ha='right', va='center')\n\nplt.title(\"Response Distribution\")\nplt.xlabel(\"Total Media Contribution\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs expected, the means differ (we optimized to increase it), and so does the certainty around the mean. Like an excersise, lets observe how probable is to get a response higher and lower than the mean.\n\n\nCode\naz.plot_posterior(\n    optimized_values,\n    figsize=default_figsize,\n    ref_val=optimized_mean,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis makes everything clear now, the chances of getting some higher or equal than the mean where 43% but the chances of getting some lower than the mean where 56%. It‚Äôs no surprise that we got a lower response and a lower ROAS.\n\n\n\n\n\n\nüí° First Takeaway\n\n\n\nComparing full distributions makes risk transparent: width quantifies forecast reliability, skewness reveals asymmetry of upside vs downside, and overlaps show practical indistinguishability.\n\n\nWhere this risk is coming from? The new allocation is riskier, but why? Let‚Äôs look at each spend level relative to its saturation curve.\n\n\nCode\ncurve = mmm.saturation.sample_curve(\n    mmm.idata.posterior[[\"saturation_alpha\", \"saturation_lam\"]], max_value=10\n)\n\nfig, axes = mmm.plot.saturation_curves(\n    curve,\n    original_scale=True,\n    n_samples=10,\n    hdi_probs=0.85,\n    random_seed=rng,\n    subplot_kwargs={\"figsize\": default_figsize, \"ncols\": 4, \"sharey\": True},\n    rc_params={\n        \"xtick.labelsize\": 10,\n        \"ytick.labelsize\": 10,\n        \"axes.labelsize\": 10,\n        \"axes.titlesize\": 10,\n    },\n\n)\n\n# Add vertical lines for optimal allocation on each subplot\nallocation_values = naive_posterior_response.allocation.values\nchannel_names = naive_posterior_response.allocation.channel.values\n\nfor i, (ax, allocation_value) in enumerate(zip(axes.ravel(), allocation_values)):\n    ax.axvline(allocation_value, color=\"red\", linestyle=\"--\", alpha=0.8, linewidth=2, label=\"Optimal allocation\")\n    ax.title.set_fontsize(10)\n\nif fig._suptitle is not None:\n    fig._suptitle.set_fontsize(12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertical lines are located at the spend allocation given for each channel. For channels as x4, the model has few observations at those spend levels, so posterior bands are wide and the induced response distribution is diffuse. Risk-aware optimization tends to pull spend toward well-identified regions (often near inflection), trading a small mean decrease for a large reduction in variance.\nCould we understand this in advance? and if so, would we prefer a different type of allocation? Which get us closer to our objective in a safer way? -A narrower distribution with a slightly lower mean can be preferable when shortfall risk is costly-.\nThe short answer is definetly. Let‚Äôs create now a new optimization process that will be risk-aware."
  },
  {
    "objectID": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#mean-tightness-score",
    "href": "articles/bayesian_models_and_risk_optimization/bayesian_models_and_risk_optimization.html#mean-tightness-score",
    "title": "Bayesian Models and Risk Optimization",
    "section": "üéØ Mean Tightness Score",
    "text": "üéØ Mean Tightness Score\n\n\nCode\nut.mean_tightness_score?\n\n\nAlpha here ecodes your risk profile (0-1 range), allocations with higher means even if they have big dispersion score better when your alpha is high. When your alpha is low, allocations with lower means but small dispersion score better.\nWe are using a lower alpha in the example, as expected, recommendations for every channel lie in well-known regions.\n\n\nCode\nmts_budget_allocation, mts_optimizer_result, callback_results = (\n    optimizable_model.optimize_budget(\n        budget=time_unit_budget,\n        utility_function=ut.mean_tightness_score(alpha=0.15),\n        callback=True,\n        minimize_kwargs={\"options\": {\"maxiter\": 2_000, \"ftol\": 1e-16}},\n    )\n)\n\nmts_posterior_response = optimizable_model.sample_response_distribution(\n    allocation_strategy=mts_budget_allocation,\n    include_carryover=True,\n    include_last_observations=False,\n    additional_var_names=[\"y_original_scale\"]\n)\n\n# Print budget allocation by channel\nprint(\"Budget allocation by channel:\")\nfor channel in channels:\n    print(\n        f\"  {channel}: {mts_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n    )\nprint(\n    f\"Total Allocated Budget: {np.sum(mts_posterior_response.allocation.to_numpy()):,.0f}\"\n)\n\n\nBudget allocation by channel:\n  x1_original_scale: 16\n  x2_original_scale: 38\n  x3_original_scale: 2\n  x4_original_scale: 1\nTotal Allocated Budget: 59\n\n\nGreat, it looks like the allocation shifts toward better-identified regions. Let‚Äôs plot the saturation curves to see where the allocation lands.\n\n\nCode\ncurve = mmm.saturation.sample_curve(\n    mmm.idata.posterior[[\"saturation_alpha\", \"saturation_lam\"]], max_value=4\n)\n\nfig, axes = mmm.plot.saturation_curves(\n    curve,\n    original_scale=True,\n    n_samples=10,\n    hdi_probs=0.85,\n    random_seed=rng,\n    subplot_kwargs={\"figsize\": default_figsize, \"ncols\": 4, \"sharey\": True},\n    rc_params={\n        \"xtick.labelsize\": 10,\n        \"ytick.labelsize\": 10,\n        \"axes.labelsize\": 10,\n        \"axes.titlesize\": 10,\n    },\n\n)\n\n# Add vertical lines for optimal allocation on each subplot\nallocation_values = mts_posterior_response.allocation.values\n\nfor i, (ax, allocation_value) in enumerate(zip(axes.ravel(), allocation_values)):\n    ax.axvline(allocation_value, color=\"red\", linestyle=\"--\", alpha=0.8, linewidth=2, label=\"Optimal allocation\")\n    ax.title.set_fontsize(10)\n\nif fig._suptitle is not None:\n    fig._suptitle.set_fontsize(12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected, recommendations for every channel lie in well-known regions.\nThe consequence: the posterior distribution narrows because budget concentrates in well-learned, less-nonlinear regions.\nLet‚Äôs plot posterior distributions for this lower-risk allocation.\n\n\nCode\nfig, ax = plt.subplots()\n\n# Get the values\noptimized_risk_values = mts_posterior_response.total_media_contribution_original_scale.values\n\n# Plot distributions\naz.plot_dist(\n    optimized_values,\n    color=\"blue\",\n    label=\"Optimized allocation\",\n    ax=ax,\n)\naz.plot_dist(\n    guessed_values,\n    color=\"red\",\n    label=\"Guessed allocation\",\n    ax=ax,\n)\naz.plot_dist(\n    optimized_risk_values,\n    color=\"green\",\n    label=\"Risk-adjusted allocation\",\n    ax=ax,\n)\n\n# Calculate means\nrisk_adjusted_mean = optimized_risk_values.mean()\n\n# Add vertical lines for means\nax.axvline(optimized_mean, color=\"blue\", linestyle=\"--\", alpha=0.8)\nax.axvline(guessed_mean, color=\"red\", linestyle=\"--\", alpha=0.8)\nax.axvline(risk_adjusted_mean, color=\"green\", linestyle=\"--\", alpha=0.8)\n\n# Add text boxes with mean values\nax.text(optimized_mean + 10, ax.get_ylim()[1] * 0.8, \n        f'Optimized Mean:\\n{optimized_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n        ha='left', va='center')\n\nax.text(guessed_mean - 10, ax.get_ylim()[1] * 0.6, \n        f'Guessed Mean:\\n{guessed_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.7),\n        ha='right', va='center')\n\nax.text(risk_adjusted_mean - 10, ax.get_ylim()[1] * 0.4, \n        f'Risk-adjusted Mean:\\n{risk_adjusted_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7),\n        ha='right', va='center')\n\nplt.title(\"Response Distribution\")\nplt.xlabel(\"Total Media Contribution\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis makes it clear: we gained much more certainty‚Äîa risk-averse (lower-variance) response distribution. You may be thinking that playing in known regions tends to reduce the mean. Do you want to know why?\nPosterior response curves tend to be more certain at the origin because two sources of uncertainty are minimized there: structurally, we know that zero spend produces zero incremental effect, and empirically, the lowest spend region is often well supported in historical data. As spend increases, especially beyond historically observed levels, epistemic uncertainty about the saturation and curvature parameters dominates, widening the credible intervals.\nDoes that mean we are doomed to lower values if we want certainty? Not at all, we can change the utility to prefer riskier options üî•.\nLet‚Äôs run a more risk-seeking allocation üëÄ\n\n\nCode\ninverse_mts_budget_allocation, inverse_mts_optimizer_result, callback_results = (\n    optimizable_model.optimize_budget(\n        budget=time_unit_budget,\n        utility_function=ut.mean_tightness_score(alpha=0.95),\n        callback=True,\n        minimize_kwargs={\"options\": {\"maxiter\": 2_000, \"ftol\": 1e-16}},\n    )\n)\n\ninverse_mts_posterior_response = optimizable_model.sample_response_distribution(\n    allocation_strategy=inverse_mts_budget_allocation,\n    include_carryover=True,\n    include_last_observations=False,\n    additional_var_names=[\"y_original_scale\"]\n)\n\n# Print budget allocation by channel\nprint(\"Budget allocation by channel:\")\nfor channel in channels:\n    print(\n        f\"  {channel}: {inverse_mts_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n    )\nprint(\n    f\"Total Allocated Budget: {np.sum(inverse_mts_posterior_response.allocation.to_numpy()):,.0f}\"\n)\n\n\nBudget allocation by channel:\n  x1_original_scale: 38\n  x2_original_scale: 0\n  x3_original_scale: 8\n  x4_original_scale: 12\nTotal Allocated Budget: 59\n\n\nFlipping the tightness preference (alpha parameter) induces risk-seeking behavior, moving allocations toward higher-variance, high-upside regions.\nNow we choose an allocation that is less certain but with higher potential upside than the baseline. Let‚Äôs plot the response distributions.\n\n\n\n\n\n\nüí° Second Takeaway\n\n\n\nDiscover your risk preferences and adjust your objective function to reflect them. You don‚Äôt need to commit to a single function or approach, you can build a custom one which tailor your needs.\n\n\n\n\nCode\nfig, ax = plt.subplots()\n\n# Get the values\noptimized_inverse_risk_values = inverse_mts_posterior_response.total_media_contribution_original_scale.values\n\n# Plot distributions\naz.plot_dist(\n    optimized_values,\n    color=\"blue\",\n    label=\"Optimized allocation\",\n    ax=ax,\n)\naz.plot_dist(\n    guessed_values,\n    color=\"red\",\n    label=\"Guessed allocation\",\n    ax=ax,\n)\naz.plot_dist(\n    optimized_risk_values,\n    color=\"green\",\n    label=\"Risk-adjusted allocation\",\n    ax=ax,\n)\n\naz.plot_dist(\n    optimized_inverse_risk_values,\n    color=\"orange\",\n    label=\"Inverse Risk-adjusted allocation\",\n    ax=ax,\n)\n\n# Calculate means\ninverse_risk_adjusted_mean = optimized_inverse_risk_values.mean()\n\n# Add vertical lines for means\nax.axvline(optimized_mean, color=\"blue\", linestyle=\"--\", alpha=0.8)\nax.axvline(guessed_mean, color=\"red\", linestyle=\"--\", alpha=0.8)\nax.axvline(risk_adjusted_mean, color=\"green\", linestyle=\"--\", alpha=0.8)\nax.axvline(inverse_risk_adjusted_mean, color=\"orange\", linestyle=\"--\", alpha=0.8)\n\n# Add text boxes with mean values\nax.text(optimized_mean + 10, ax.get_ylim()[1] * 0.8, \n        f'Optimized Mean:\\n{optimized_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n        ha='left', va='center')\n\nax.text(guessed_mean - 10, ax.get_ylim()[1] * 0.6, \n        f'Guessed Mean:\\n{guessed_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.7),\n        ha='right', va='center')\n\nax.text(risk_adjusted_mean - 10, ax.get_ylim()[1] * 0.4, \n        f'Risk-adjusted Mean:\\n{risk_adjusted_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7),\n        ha='right', va='center')\n\nax.text(inverse_risk_adjusted_mean - 10, ax.get_ylim()[1] * 0.2, \n        f'Inverse Risk-adjusted Mean:\\n{inverse_risk_adjusted_mean:.1f}', \n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"orange\", alpha=0.7),\n        ha='right', va='center')\n\nplt.title(\"Response Distribution\")\nplt.xlabel(\"Total Media Contribution\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nGreat, the new allocation is riskier, and the mean is higher (still less uncertant than the risk-neutral allocation). We can go beyond visuals and quantify this.\nBecause all are posterior distributions, we can check the density that has each response distribution at their respective mean. We‚Äôll be using kernel density estimation (KDE). This value, denoted \\(\\hat{f}(\\mu)\\), represents the estimated height of the probability density function at the mean. Importantly, this is not itself a probability but a density, with units of ‚Äú1 over the units of the variable.‚Äù Higher values of \\(\\hat{f}(\\mu)\\) indicate that the distribution is sharply peaked around the mean, reflecting greater certainty that posterior draws will lie close to the central value. Conversely, lower values correspond to flatter, more diffuse posteriors, indicating higher uncertainty.\nLet‚Äôs check the density at the mean for each allocation.\n\n\nCode\nfrom scipy.stats import gaussian_kde\n\ndef kde_density_at_point(x, x0):\n    \"\"\"Gaussian KDE density estimate at x0 using Scott's rule bandwidth.\"\"\"\n    kde = gaussian_kde(x)  # Scott's rule by default\n    return float(kde.evaluate([x0])[0])\n\noptimized_density = kde_density_at_point(optimized_values, optimized_mean)\nguessed_density = kde_density_at_point(guessed_values, guessed_mean)\nrisk_adjusted_density = kde_density_at_point(optimized_risk_values, risk_adjusted_mean)\ninverse_risk_adjusted_density = kde_density_at_point(optimized_inverse_risk_values, inverse_risk_adjusted_mean)\n\nprint(f\"Optimized allocation response density at mean: {optimized_density:.3f}\")\nprint(f\"Guessed allocation response density at mean: {guessed_density:.3f}\")\nprint(f\"Risk-adjusted allocation response density at mean: {risk_adjusted_density:.3f}\")\nprint(f\"Inverse Risk-adjusted allocation response density at mean: {inverse_risk_adjusted_density:.3f}\")\n\n\nOptimized allocation response density at mean: 0.001\nGuessed allocation response density at mean: 0.005\nRisk-adjusted allocation response density at mean: 0.013\nInverse Risk-adjusted allocation response density at mean: 0.003\n\n\nThe density estimations tell the same story as the plots. How can we use this to take actions? For example, suppose we want to hit a target ROAS of \\(9.5\\), then we can check the density of the ROAS distribution at \\(9.5\\) for each response distribution given their respective allocation strategy.\n\n\nCode\noptimized_roas = optimized_values / (time_unit_budget*num_periods)\nguessed_roas = guessed_values / (time_unit_budget*num_periods)\nrisk_adjusted_roas = optimized_risk_values / (time_unit_budget*num_periods)\ninverse_risk_adjusted_roas = optimized_inverse_risk_values / (time_unit_budget*num_periods)\n\n# Calculate kde density at 9.5\n_target_roas = 9.5\noptimized_kde_density = kde_density_at_point(optimized_roas, _target_roas)\nguessed_kde_density = kde_density_at_point(guessed_roas, _target_roas)\nrisk_adjusted_kde_density = kde_density_at_point(risk_adjusted_roas, _target_roas)\ninverse_risk_adjusted_kde_density = kde_density_at_point(inverse_risk_adjusted_roas, _target_roas)\n\n#plot the ROAS distributions\nfig, ax = plt.subplots()\n\n# Plot distributions\naz.plot_dist(optimized_roas, color=\"blue\", label=f\"Optimized allocation: {optimized_kde_density:.3f}\", ax=ax)\naz.plot_dist(guessed_roas, color=\"red\", label=f\"Guessed allocation: {guessed_kde_density:.3f}\", ax=ax)\naz.plot_dist(risk_adjusted_roas, color=\"green\", label=f\"Risk-adjusted allocation: {risk_adjusted_kde_density:.3f}\", ax=ax)\naz.plot_dist(inverse_risk_adjusted_roas, color=\"orange\", label=f\"Inverse Risk-adjusted allocation: {inverse_risk_adjusted_kde_density:.3f}\", ax=ax)\n\n# Add vertical lines for means\nax.axvline(_target_roas, color=\"black\", linestyle=\"--\", alpha=0.8, label=\"Target ROAS\")\n\nplt.tight_layout()\nplt.title(\"ROAS Distribution\")\nplt.xlabel(\"ROAS\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nIf we want to hit a target ROAS of \\(9.5\\), the risk-neutral optimized allocation is the most certain, followed by the inverse risk-adjusted allocation.\nIf instead our target ROAS is \\(7\\), the inverse risk-adjusted allocation concentrates more density around that value, and the risk-neutral optimized allocation has less density around it.\n\n\nCode\n# Calculate kde density at 7\n_roas_target = 7\noptimized_kde_density = kde_density_at_point(optimized_roas, _roas_target)\nguessed_kde_density = kde_density_at_point(guessed_roas, _roas_target)\nrisk_adjusted_kde_density = kde_density_at_point(risk_adjusted_roas, _roas_target)\ninverse_risk_adjusted_kde_density = kde_density_at_point(inverse_risk_adjusted_roas, _roas_target)\n\n#plot the ROAS distributions\nfig, ax = plt.subplots()\n\n# Plot distributions\naz.plot_dist(optimized_roas, color=\"blue\", label=f\"Optimized allocation: {optimized_kde_density:.3f}\", ax=ax)\naz.plot_dist(guessed_roas, color=\"red\", label=f\"Guessed allocation: {guessed_kde_density:.3f}\", ax=ax)\naz.plot_dist(risk_adjusted_roas, color=\"green\", label=f\"Risk-adjusted allocation: {risk_adjusted_kde_density:.3f}\", ax=ax)\naz.plot_dist(inverse_risk_adjusted_roas, color=\"orange\", label=f\"Inverse Risk-adjusted allocation: {inverse_risk_adjusted_kde_density:.3f}\", ax=ax)\n\n# Add vertical lines for means\nax.axvline(_roas_target, color=\"black\", linestyle=\"--\", alpha=0.8, label=\"Target ROAS\")\n\nplt.tight_layout()\nplt.title(\"ROAS Distribution\")\nplt.xlabel(\"ROAS\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUnder this paradigm, you can define a target estimate and select an allocation that maximizes the probability of hitting it. One way is to create a function that reduces variance with respect to the target, favoring narrower distributions with density near the target.\nLet‚Äôs make this custom utility function, and see how it performs.\n\n\nCode\nimport pytensor.tensor as pt\nimport pymc_marketing.mmm.utility as ut\n\ndef target_hit_probability(target, num_periods):\n    \"\"\"\n    Target hit probability utility function.\n    \n    Minimizing the mean squared error between predicted ROAS and the target\n    Adding a variance penalty to encourage tighter distributions.\n    \n    Parameters\n    ----------\n    target : float\n        The target ROAS value to optimize towards\n    num_periods : int\n        Number of time periods for the budget allocation\n        \n    Returns\n    -------\n    callable\n        A utility function that can be used with the budget optimizer.\n        Returns negative objective (since optimizer minimizes).\n    \"\"\"\n    def _function(samples, budgets):\n        roas_samples = samples / (pt.sum(budgets) * num_periods)\n        \n        # Use mean squared error from target, which provides smoother gradients\n        mse_from_target = pt.mean((roas_samples - target) ** 2)\n        \n        # Add penalty for variance to encourage tighter distributions around target\n        variance_penalty = pt.var(roas_samples)\n        \n        # Combine MSE and variance penalty with weighting\n        # Higher variance penalty encourages narrower distributions\n        total_objective = mse_from_target + 0.1 * variance_penalty\n        \n        return -total_objective\n    return _function\n\n_roas_target = 10\ncustom_utility_budget_allocation, custom_utility_optimizer_result, callback_results = (\n    optimizable_model.optimize_budget(\n        budget=time_unit_budget,\n        utility_function=target_hit_probability(_roas_target, num_periods),\n        callback=True,\n        minimize_kwargs={\"options\": {\"maxiter\": 2_000}},\n    )\n)\n\ncustom_utility_posterior_response = optimizable_model.sample_response_distribution(\n    allocation_strategy=custom_utility_budget_allocation,\n    include_carryover=True,\n    include_last_observations=False,\n    additional_var_names=[\"y_original_scale\"]\n)\n\n# Print budget allocation by channel\nprint(\"Budget allocation by channel:\")\nfor channel in channels:\n    print(\n        f\"  {channel}: {custom_utility_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n    )\nprint(\n    f\"Total Allocated Budget: {np.sum(custom_utility_posterior_response.allocation.to_numpy()):,.0f}\"\n)\n\n\nBudget allocation by channel:\n  x1_original_scale: 23\n  x2_original_scale: 0\n  x3_original_scale: 23\n  x4_original_scale: 12\nTotal Allocated Budget: 59\n\n\nThe allocation is similar to those observed before. Let‚Äôs plot the ROAS distributions.\n\n\nCode\ncustom_utility_values = custom_utility_posterior_response.total_media_contribution_original_scale.values\ncustom_utility_roas = custom_utility_values / (time_unit_budget*num_periods)\noptimized_kde_density = kde_density_at_point(optimized_roas, _roas_target)\nguessed_kde_density = kde_density_at_point(guessed_roas, _roas_target)\nrisk_adjusted_kde_density = kde_density_at_point(risk_adjusted_roas, _roas_target)\ninverse_risk_adjusted_kde_density = kde_density_at_point(inverse_risk_adjusted_roas, _roas_target)\ncustom_utility_kde_density = kde_density_at_point(custom_utility_roas, _roas_target)\n\n#plot the ROAS distributions\nfig, ax = plt.subplots()\n\n# Plot distributions\naz.plot_dist(optimized_roas, color=\"blue\", label=f\"Optimized allocation: {optimized_kde_density:.3f}\", ax=ax)\naz.plot_dist(guessed_roas, color=\"red\", label=f\"Guessed allocation: {guessed_kde_density:.3f}\", ax=ax)\naz.plot_dist(risk_adjusted_roas, color=\"green\", label=f\"Risk-adjusted allocation: {risk_adjusted_kde_density:.3f}\", ax=ax)\naz.plot_dist(inverse_risk_adjusted_roas, color=\"orange\", label=f\"Inverse Risk-adjusted allocation: {inverse_risk_adjusted_kde_density:.3f}\", ax=ax)\naz.plot_dist(custom_utility_roas, color=\"purple\", label=f\"Custom utility allocation: {custom_utility_kde_density:.3f}\", ax=ax)\n\n# Add vertical lines for means\nax.axvline(_roas_target, color=\"black\", linestyle=\"--\", alpha=0.8, label=\"Target ROAS\")\n\nplt.tight_layout()\nplt.title(\"ROAS Distribution\")\nplt.xlabel(\"ROAS\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nGreat üôåüèª The initial optimized allocation which was risk neutral, had initially the higher density around the target ROAS, but the density around it was not high enough, the new allocation bring a more certain answer around the target, because the objective function was built for it.\n\n\n\n\n\n\nüí° Key Insight\n\n\n\nHere we have a custom utility function that allows us to optimize for a target ROAS. Nevertheless, you can build other utilities and use them as objectives. You can also introduce risk-aware constraints‚Äîwithout on top of your business constraints.\n\n\nLet‚Äôs observe our final posterior around the target ROAS.\n\n\nCode\naz.plot_posterior(\n    custom_utility_roas, \n    ref_val=_roas_target\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe could say: the probability of achieving ROAS ‚â• 10 with this allocation is 31%, and ROAS &lt; 10 is 69%. If we need more certainty, we can make the optimization more risk-averse.\nNow, if you want to think really bayesian, then you can define a region of practical equivalence, and check the probability of the ROAS being in that region. For example, you can ask yourself: Would I do something different if ROAS is 7, 9 or 11? If the answer it‚Äôs no, then you find your ROPE.\nLet‚Äôs say we want to know the probability of the ROAS being between \\(7\\) and \\(11\\).\n\n\nCode\n# Calculate probability of ROAS being between 7 and 9\nprob_7_to_11 = np.mean((custom_utility_roas &gt;= 7) & (custom_utility_roas &lt;= 11))\n\n# Plot posterior with reference values and region\nfig, ax = plt.subplots()\naz.plot_posterior(\n    custom_utility_roas, \n    ref_val=_roas_target,\n    ax=ax\n)\n\n# Add vertical lines for the region of interest\nax.axvline(7, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"ROAS = 7\")\nax.axvline(11, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"ROAS = 11\")\n\n# Add text box with probability\nax.text(0.02, 0.98, \n        f'P(7 ‚â§ ROAS ‚â§ 11) = {prob_7_to_11:.2%}', \n        transform=ax.transAxes,\n        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n        ha='left', va='top', fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüí° Key Insight\n\n\n\nYou can define a region of practical equivalence (ROPE), in order to be more precise with your decision making. This is quite natural way to think about a problem, and lightens the burden of the decision maker to commit to a single number. At the end of the day, we don‚Äôt need to be 99.999% precise around every single answer, or number, we can be 95% or 90% precise and sometimes that‚Äôs enough. Identify if thats your case, and think accordingly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marketing Science Blog",
    "section": "",
    "text": "Discovering ways to create impact with science\n\nContact Me Book 30 Minutes"
  },
  {
    "objectID": "index.html#marketing-science-enthusiast",
    "href": "index.html#marketing-science-enthusiast",
    "title": "Marketing Science Blog",
    "section": "",
    "text": "Discovering ways to create impact with science\n\nContact Me Book 30 Minutes"
  },
  {
    "objectID": "index.html#recent-articles",
    "href": "index.html#recent-articles",
    "title": "Marketing Science Blog",
    "section": "Recent Articles",
    "text": "Recent Articles\n\n\nNo More Experiments Without Causality\nLearn why you should not blindly trust on experiments to calibrate your media mix model.\n\n\nBayesian Models and Risk Optimization\nLearn how to use bayesian regression models to optimize your media mix model."
  },
  {
    "objectID": "index.html#upcoming-talks",
    "href": "index.html#upcoming-talks",
    "title": "Marketing Science Blog",
    "section": "Upcoming Talks",
    "text": "Upcoming Talks\nPyData Berlin 2025: Risk Budget Optimization for Causal Mix Models - Conference - 2025\nCheck out my Talks page for more videos and presentations."
  }
]