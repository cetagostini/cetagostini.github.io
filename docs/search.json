[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks & Presentations",
    "section": "",
    "text": "Here you’ll find recordings of my conference talks, workshops, and other presentations.\n\n\nVirtual Presentation\nDiscover how at Bolt I have been applying MMM to optimize our marketing budgets.\n\n\n\n\n\n\n\n\nVirtual Community Connect - 22 November 2023\nPresentation on open source marketing science techniques used by Meta.\n\n\n\n\n\n\n\n\nVirtual Webinar - 21 December 2023\nAnnual review of PyMC Labs’ open source contributions and achievements.\n\n\n\n\n\n\n\n\nVirtual Webinar - 30 January 2024\nPractical guide to implementing and operationalizing Marketing Mix Modeling in technology companies.\n\n\n\n\n\n\n\n\nVirtual Webinar - 28 February 2024\nPanel discussion on Marketing Mix Modeling with industry experts Jim Gianoglio and Michael Kaminsky.\n\n\n\n\n\n\n\nHave a conference or workshop where you’d like me to speak? Let’s connect!\n\n\nContact Me\n\n\nBook 30 Minutes"
  },
  {
    "objectID": "talks.html#interested-in-collaboration",
    "href": "talks.html#interested-in-collaboration",
    "title": "Talks & Presentations",
    "section": "",
    "text": "Have a conference or workshop where you’d like me to speak? Let’s connect!\n\n\nContact Me\n\n\nBook 30 Minutes"
  },
  {
    "objectID": "articles/nomore_experiments_without_causality/nomore_experiments_without_causality.html",
    "href": "articles/nomore_experiments_without_causality/nomore_experiments_without_causality.html",
    "title": "Media Mix Model calibration is useless without causal knowledge",
    "section": "",
    "text": "Introduction\nImagine you just shipped a shiny new Bayesian Media-Mix Model (MMM) that perfectly back-fits years of marketing data. A/B-lift experiments then tell you channel-A is worth €2.7 M, but your model insists it is worth €7 M. “Easy fix,” you think: calibrate the MMM with the lift tests—add an extra likelihood term, rerun, publish.\nYet the calibrated model still over/under-values the channel based on the experimental evidence. Looks like it can’t reconcile the experimental evidence with the data, and adding new calibration for other channels actually makes it worse.\nThat is the calibration trap: without causal structure the posterior can’t happily reconcile observations and clean experiments at the same time.\nIn this article we will build a PyMC MMM, add lift-test calibration, and then show—step-by-step—why calibration alone cannot save a misspecified causal story.\n\n\n\nWhy marketers love calibration\n\nGround-truth anchor. Lift tests are randomised, so their incremental effects are (almost) unbiased.\n\nSample-size boost. MMMs see every day and every channel; experiments see only a slice. Combining them promises lower variance.\n\nStorytelling power. “Our model matches the experiments” is an executive-friendly sound-bite.\n\nCalibration therefore feels like catching two Bayesian birds with one conjugate stone.\n\n\n\nWhat is calibration—mathematically?\nFor each experiment \\(i\\) the model predicts a lift\n\\[\n\\widehat{\\Delta y_i}(\\theta)\\;=\\;\ns\\bigl(x_i+\\Delta x_i;\\,\\theta_{c(i)}\\bigr)\n\\;-\\;\ns\\bigl(x_i;\\,\\theta_{c(i)}\\bigr),\n\\]\nwhere\n\n\\(x_i\\) – baseline spend before the experiment,\n\n\\(\\Delta x_i\\) – change in spend during the experiment,\n\n\\(s(\\cdot;\\theta_{c(i)})\\) – saturation curve for the channel that experiment \\(i\\) targets,\n\n\\(\\theta\\) – all saturation-curve parameters,\n\n\\(\\widehat{\\Delta y_i}(\\theta)\\) – model-predicted incremental outcome.\n\nWe then attach the observed lift \\(\\Delta y_i\\) and its error \\(\\sigma_i\\) through an additional likelihood\n\\[\np\\!\\bigl(\\Delta y_i \\mid \\theta\\bigr)\\;=\\;\n\\operatorname{Gamma}\\!\\bigl(\n\\mu=\\lvert\\widehat{\\Delta y_i}(\\theta)\\rvert,\\;\n\\sigma=\\sigma_i\n\\bigr),\n\\]\nwhere\n\n\\(\\Delta y_i\\) – experimentally measured incremental outcome,\n\n\\(\\sigma_i\\) – reported standard error of \\(\\Delta y_i\\),\n\n\\(\\mu\\) – mean parameter set to the absolute predicted lift so the Gamma remains non-negative.\n\nStacking all \\(n_{\\text{lift}}\\) experiments gives the calibrated posterior\n\\[\np\\!\\bigl(\\theta \\mid \\mathbf y,\\mathcal L\\bigr)\n\\;\\propto\\;\np\\!\\bigl(\\mathbf y \\mid \\theta\\bigr)\\;\n\\prod_{i=1}^{n_{\\text{lift}}}\np\\!\\bigl(\\Delta y_i \\mid \\theta\\bigr)\\;\np(\\theta),\n\\]\nwhere\n\n\\(\\mathbf y\\) – full time-series of observed outcomes (sales, sign-ups …),\n\n\\(\\mathcal L\\) – the collection of lift-test observations \\((\\Delta y_i,\\sigma_i)\\),\n\n\\(p(\\theta)\\) – priors for all parameters.\n\nPyMC turns this into a three-liner:\nadd_lift_measurements_to_likelihood_from_saturation(\n    model=mmm,\n    df_lift=df_lifts,     # experiment data-frame\n    dist=pm.Gamma,\n)\nIn simple terms, calibration appends one extra likelihood per experiment: for lift i we run the channel’s saturation curve at the pre-spend and post-spend levels, subtract the two, and call that result the model-expected incremental response for experiment i (a deterministic function of the saturation parameter vector \\(\\theta\\)). We then treat the observed lift \\(\\Delta y_i\\) as a Gamma-distributed draw whose mean is the absolute value of that model-expected increment and whose dispersion is the experiment’s reported standard error \\(\\sigma_i\\).\nThese independent \\(\\Gamma(\\mu = |\\text{model-expected increment}|, \\sigma = \\sigma_i)\\) factors multiply into the original time-series likelihood, yielding a posterior where \\(\\theta\\) is pulled toward values that keep every model-expected increment within the experimental noise band. In effect, each lift test imposes a Bayesian anchor that penalises any parameter setting whose predicted causal effect disagrees with ground-truth, while still allowing the full sales history to inform the remaining uncertainty.\nLet’s see how this works in practice, by creating a synthetic dataset and fitting a simple MMM.\n\n\nGetting started\nWe’ll use Pytensor to run our data-generation-process (DGP). Let’s set the seed for reproducibility, and define the number of observations, and finally add some default configurations for the notebook.\n\n\nCode\nimport warnings\nimport pymc as pm\nimport arviz as az\nimport pytensor.tensor as pt\nfrom pytensor.graph import rewrite_graph\nimport preliz as pz\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\n\nfrom pymc_marketing.mmm import GeometricAdstock, MichaelisMentenSaturation, MMM\nfrom pymc_marketing.prior import Prior\n\nSEED = 42\nn_observations = 1050\n\nwarnings.filterwarnings(\"ignore\")\n\n# Set the style\naz.style.use(\"arviz-darkgrid\")\nplt.rcParams[\"figure.figsize\"] = [8, 4]\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"axes.labelsize\"] = 6\nplt.rcParams[\"xtick.labelsize\"] = 6\nplt.rcParams[\"ytick.labelsize\"] = 6\n\n%config InlineBackend.figure_format = \"retina\"\n\n\nNow, we can define the date range.\n\n\nCode\nmin_date = pd.to_datetime(\"2022-01-01\")\nmax_date = min_date + pd.Timedelta(days=n_observations)\n\ndate_range = pd.date_range(start=min_date, end=max_date, freq=\"D\")\n\ndf = pd.DataFrame(data={\"date_week\": date_range}).assign(\n    year=lambda x: x[\"date_week\"].dt.year,\n    month=lambda x: x[\"date_week\"].dt.month,\n    dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n)\n\n\nWe can start by creating the spend vectors for each channel. These are the will define later the amount of impressions or exposition we get from each channel, which by the end will transform into sales.\n\n\nCode\nspend_x1 = pt.vector(\"spend_x1\")\nspend_x2 = pt.vector(\"spend_x2\")\nspend_x3 = pt.vector(\"spend_x3\")\nspend_x4 = pt.vector(\"spend_x4\")\n\n# Create sample inputs for demonstration using preliz distributions:\npz_spend_x1 = np.convolve(\n    pz.Gamma(mu=.8, sigma=.3).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x1[:14] = pz_spend_x1.mean()\npz_spend_x1[-14:] = pz_spend_x1.mean()\n\npz_spend_x2 = np.convolve(\n    pz.Gamma(mu=.6, sigma=.4).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x2[:14] = pz_spend_x2.mean()\npz_spend_x2[-14:] = pz_spend_x2.mean()\n\npz_spend_x3 = np.convolve(\n    pz.Gamma(mu=.2, sigma=.2).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x3[:14] = pz_spend_x3.mean()\npz_spend_x3[-14:] = pz_spend_x3.mean()\n\npz_spend_x4 = np.convolve(\n    pz.Gamma(mu=.1, sigma=.03).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x4[:14] = pz_spend_x4.mean()\npz_spend_x4[-14:] = pz_spend_x4.mean()\n\nfig, ax = plt.subplots()\nax.plot(date_range[1:], pz_spend_x1, label='Channel 1')\nax.plot(date_range[1:], pz_spend_x2, label='Channel 2')\nax.plot(date_range[1:], pz_spend_x3, label='Channel 3')\nax.plot(date_range[1:], pz_spend_x4, label='Channel 4')\nax.set_xlabel('Time')\nax.set_ylabel('Spend')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUsing the same logic we can create other components such as trend, noise, seasonality, and certain events.\n\n\nCode\n## Trend\ntrend = pt.vector(\"trend\")\n# Create a sample input for the trend\nnp_trend = (np.linspace(start=0.0, stop=.50, num=n_observations) + .10) ** (.1 / .4)\n\n## NOISE \nglobal_noise = pt.vector(\"global_noise\")\n# Create a sample input for the noise\npz_global_noise = pz.Normal(mu=0, sigma=.005).rvs(size=n_observations, random_state=SEED)\n\n# EVENTS EFFECT\npt_event_signal = pt.vector(\"event_signal\")\npt_event_contributions = pt.vector(\"event_contributions\")\n\nevent_dates = [\"24-12\", \"09-07\"]  # List of events as month-day strings\nstd_devs = [25, 15]  # List of standard deviations for each event\nevents_coefficients = [.094, .018]\n\nsignals_independent = []\n\n# Initialize the event effect array\nevent_signal = np.zeros(len(date_range))\nevent_contributions = np.zeros(len(date_range))\n\n# Generate event signals\nfor event, std_dev, event_coef in zip(\n    event_dates, std_devs, events_coefficients, strict=False\n):\n    # Find all occurrences of the event in the date range\n    event_occurrences = date_range[date_range.strftime(\"%d-%m\") == event]\n\n    for occurrence in event_occurrences:\n        # Calculate the time difference in days\n        time_diff = (date_range - occurrence).days\n\n        # Generate the Gaussian basis for the event\n        _event_signal = np.exp(-0.5 * (time_diff / std_dev) ** 2)\n\n        # Add the event signal to the event effect\n        signals_independent.append(_event_signal)\n        event_signal += _event_signal\n\n        event_contributions += _event_signal * event_coef\n\nnp_event_signal = event_signal\nnp_event_contributions = event_contributions\n\nplt.plot(pz_global_noise, label='Global Noise')\nplt.plot(np_trend, label='Trend')\nplt.plot(np_event_signal, label='Event Contributions')\nplt.title('Components of the Time Series Model')\nplt.xlabel('Time (days)')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn order to make it more interesting, lets add a price variable. Usually, price creates more impact as it’s slower. The product price contribution function we’ll use is a diminishing returns function:\n\\[f(X, \\alpha, \\lambda) = \\frac{\\alpha}{1 + (X / \\lambda)}\\]\nwhere \\(\\alpha\\) represents the maximum contribution and \\(\\lambda\\) is a scaling parameter that controls how quickly the contribution diminishes as price increases.\n\n\nCode\ndef product_price_contribution(X, alpha, lam):\n    return alpha / (1 + (X / lam))\n    \n# Create a product price vector.\nproduct_price = pt.vector(\"product_price\")\nproduct_price_alpha = pt.scalar(\"product_price_alpha\")\nproduct_price_lam = pt.scalar(\"product_price_lam\")\n\n# Create a sample input for the product price\npz_product_price = np.convolve(\n    pz.Gamma(mu=.05, sigma=.02).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_product_price[:14] = pz_product_price.mean()\npz_product_price[-14:] = pz_product_price.mean()\n\nproduct_price_alpha_value = .08\nproduct_price_lam_value = .03\n\n# Direct contribution to the target.\npt_product_price_contribution = product_price_contribution(\n    product_price, \n    product_price_alpha, \n    product_price_lam\n)\n\n# plot the product price contribution\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\n# Plot the raw price data\nax1.plot(pz_product_price, color=\"green\")\nax1.set_title('Product Price')\nax1.set_xlabel('Time (days)')\nax1.set_ylabel('Price')\nax1.grid(True, alpha=0.3)\n\n# Plot the price contribution\nprice_contribution = pt_product_price_contribution.eval({\n    \"product_price\": pz_product_price,\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value\n})\nax2.plot(price_contribution, color=\"black\")\nax2.set_title('Price Contribution')\nax2.set_xlabel('Time (days)')\nax2.set_ylabel('Contribution')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWith all the principal components in place, all parent nodes we can start to write down our causal DAG to define the relationships we want to explain.\n\n\nCode\n# Plot causal graph of the vars x1, x2, x3, x4 using graphviz\ncdag_impressions = graphviz.Digraph(comment='Causal DAG for Impressions')\n\ncdag_impressions.node('spend_x1', 'Spend X1')\ncdag_impressions.node('spend_x2', 'Spend X2')\ncdag_impressions.node('spend_x3', 'Spend X3')\ncdag_impressions.node('spend_x4', 'Spend X4')\ncdag_impressions.node('events', 'Events')\n\ncdag_impressions.edge('spend_x1', 'impressions_x1')\ncdag_impressions.edge('spend_x2', 'impressions_x2')\ncdag_impressions.edge('spend_x3', 'impressions_x3')\ncdag_impressions.edge('spend_x4', 'impressions_x4')\n\ncdag_impressions.edge('impressions_x1', 'impressions_x3')\ncdag_impressions.edge('impressions_x2', 'impressions_x3')\ncdag_impressions.edge('impressions_x2', 'impressions_x4')\n\ncdag_impressions.edge('events', 'impressions_x2')\ncdag_impressions.edge('events', 'impressions_x3')\n\ncdag_impressions\n\n\n\n\n\n\n\n\n\nOnce our causal graph is defined, we can start to write down in pytensor the structure and relationships.\n\n\nCode\n# Create a impressions vector, result of x1, x2, x3, x4. by some beta with daily values.\n# Define all parameters as PyTensor variables\nbeta_x1 = pt.vector(\"beta_x1\")\nimpressions_x1 = spend_x1 * beta_x1\n\nbeta_x2 = pt.vector(\"beta_x2\")\nalpha_event_x2 = pt.scalar(\"alpha_event_x2\")\nimpressions_x2 = spend_x2 * beta_x2 + pt_event_signal * alpha_event_x2\n\nbeta_x3 = pt.vector(\"beta_x3\")\nalpha_event_x3 = pt.scalar(\"alpha_event_x3\")\nalpha_x1_x3 = pt.scalar(\"alpha_x1_x3\")\nalpha_x2_x3 = pt.scalar(\"alpha_x2_x3\")\nimpressions_x3 = spend_x3 * beta_x3 + pt_event_signal * alpha_event_x3 + (\n    impressions_x2 * alpha_x2_x3\n    + impressions_x1 * alpha_x1_x3\n)\n\nbeta_x4 = pt.vector(\"beta_x4\")\nalpha_x2_x4 = pt.scalar(\"alpha_x2_x4\")\nimpressions_x4 = spend_x4 * beta_x4 + impressions_x2 * alpha_x2_x4\n\n# Create sample values for the parameters (to be used in eval)\npz_beta_x1 = pz.Beta(alpha=0.05, beta=.1).rvs(size=n_observations, random_state=SEED)\npz_beta_x2 = pz.Beta(alpha=.015, beta=.05).rvs(size=n_observations, random_state=SEED)\npz_alpha_event_x2 = 0.015\npz_beta_x3 = pz.Beta(alpha=.1, beta=.1).rvs(size=n_observations, random_state=SEED)\npz_alpha_event_x3 = 0.001\npz_alpha_x1_x3 = 0.005\npz_alpha_x2_x3 = 0.12\npz_beta_x4 = pz.Beta(alpha=.125, beta=.05).rvs(size=n_observations, random_state=SEED)\npz_alpha_x2_x4 = 0.01\n\n# plot all impressions\n# Define dependencies for each variable\nx1_deps = {\n    \"beta_x1\": pz_beta_x1,\n    \"spend_x1\": pz_spend_x1,\n}\n\nx2_deps = {\n    \"beta_x2\": pz_beta_x2,\n    \"spend_x2\": pz_spend_x2,\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"event_signal\": event_signal[:-1],  # Slice to match 1050 length\n}\n\n# For x3, we need all dependencies from x1 and x2 plus its own\nx3_deps = {\n    \"beta_x3\": pz_beta_x3,\n    \"spend_x3\": pz_spend_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    **x1_deps,\n    **x2_deps,\n}\n\n# For x4, we need dependencies from x2 plus its own\nx4_deps = {\n    \"beta_x4\": pz_beta_x4,\n    \"spend_x4\": pz_spend_x4,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    **x2_deps,\n}\n\n# Plot each impression series\nfig, axs = plt.subplots(2, 2, sharex='row', sharey='row')\n\n# Channel 1\naxs[0, 0].plot(impressions_x1.eval(x1_deps), color='blue')\naxs[0, 0].set_title('Channel 1')\naxs[0, 0].set_ylabel('Impressions')\n\n# Channel 2\naxs[0, 1].plot(impressions_x2.eval(x2_deps), color='orange')\naxs[0, 1].set_title('Channel 2')\n\n# Channel 3\naxs[1, 0].plot(impressions_x3.eval(x3_deps), color='green')\naxs[1, 0].set_title('Channel 3')\naxs[1, 0].set_xlabel('Time')\naxs[1, 0].set_ylabel('Impressions')\n\n# Channel 4\naxs[1, 1].plot(impressions_x4.eval(x4_deps), color='red')\naxs[1, 1].set_title('Channel 4')\naxs[1, 1].set_xlabel('Time')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the computational graph\n\n\n\nIn order to check we write down the process properly, we can ask PyTensor to print our structural causal model. This is not necessary for the analysis, but can be helpful for debugging and understanding the model structure.\n\n\nCode\nimport pytensor.printing as printing\n# Plot the graph of our model using pytensor\nprinting.pydotprint(rewrite_graph(impressions_x4), outfile=\"images/impressions.png\", var_with_name_simple=True)\n# Display the generated graph\nfrom IPython.display import Image\nImage(filename=\"images/impressions.png\")\n\n\nThe output file is available at images/impressions.png\n\n\n\n\n\n\n\n\n\nIf, you don’t like to see the graphical version, you can ask for the string representation.\n\n\nCode\n# dprint the target_var\nrewrite_graph(impressions_x4).dprint(depth=5);\n\n\nAdd [id A]\n ├─ Mul [id B]\n │  ├─ spend_x4 [id C]\n │  └─ beta_x4 [id D]\n └─ Mul [id E]\n    ├─ Add [id F]\n    │  ├─ Mul [id G]\n    │  │  ├─ spend_x2 [id H]\n    │  │  └─ beta_x2 [id I]\n    │  └─ Mul [id J]\n    │     ├─ event_signal [id K]\n    │     └─ ExpandDims{axis=0} [id L]\n    └─ ExpandDims{axis=0} [id M]\n       └─ alpha_x2_x4 [id N]\n\n\n\n\nNow, let’s define our forward pass - how media exposure actually impacts our target variable. In marketing, we typically see two key effects: saturation (diminishing returns) and lagging (delayed impact). We’ll model these using the Michaelis-Menten function for saturation and Geometric Adstock for the lagging effects.\n\n\nCode\n# Creating forward pass for impressions\ndef forward_pass(x, adstock_alpha, saturation_lam, saturation_alpha):\n    # return type pytensor.tensor.variable.TensorVariable\n    return MichaelisMentenSaturation.function(\n        MichaelisMentenSaturation, \n        x=GeometricAdstock(\n            l_max=24, normalize=False\n        ).function(\n            x=x, alpha=adstock_alpha,\n        ), lam=saturation_lam, alpha=saturation_alpha,\n    )\n\n# Applying forward pass to impressions\n# Create scalars variables for the parameters x2, x3, x4\npt_saturation_lam_x2 = pt.scalar(\"saturation_lam_x2\")\npt_saturation_alpha_x2 = pt.scalar(\"saturation_alpha_x2\")\n\npt_saturation_lam_x3 = pt.scalar(\"saturation_lam_x3\")\npt_saturation_alpha_x3 = pt.scalar(\"saturation_alpha_x3\")\n\npt_saturation_lam_x4 = pt.scalar(\"saturation_lam_x4\")\npt_saturation_alpha_x4 = pt.scalar(\"saturation_alpha_x4\")\n\npt_global_adstock_effect = pt.scalar(\"global_adstock_alpha\")\n\n# Apply forward pass to impressions\nimpressions_x2_forward = forward_pass(\n    impressions_x2, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x2, \n    pt_saturation_alpha_x2\n)\n\nimpressions_x3_forward = forward_pass(\n    impressions_x3, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x3, \n    pt_saturation_alpha_x3\n)\n\nimpressions_x4_forward = forward_pass(\n    impressions_x4, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x4, \n    pt_saturation_alpha_x4\n)\n\n\nWith all of the following in place, we can define the causal DAG for the target variable and the structural equation as the sum of all previous variables.\n\n\nCode\n# Plot graphviz causal dag for the target_var\n# Create a Graphviz object\ndot = graphviz.Digraph(comment='Causal DAG for Target Variable')\n\n# Add nodes for each variable\ndot.node('spend_x1', 'Spend X1')\ndot.node('spend_x2', 'Spend X2')\ndot.node('spend_x3', 'Spend X3')\ndot.node('spend_x4', 'Spend X4')\ndot.node('trend', 'Trend')\ndot.node('global_noise', 'Global Noise')\ndot.node('event_contributions', 'Events')\ndot.node('product_price_contribution', 'Product Price Contribution')\n\ndot.edge('spend_x1', 'impressions_x1')\ndot.edge('spend_x2', 'impressions_x2')\ndot.edge('spend_x3', 'impressions_x3')\ndot.edge('spend_x4', 'impressions_x4')\n\ndot.edge('impressions_x1', 'impressions_x3')\ndot.edge('impressions_x2', 'impressions_x3')\ndot.edge('impressions_x2', 'impressions_x4')\ndot.edge('event_contributions', 'impressions_x2')\ndot.edge('event_contributions', 'impressions_x3')\n\ndot.edge('trend', 'target_var')\ndot.edge('global_noise', 'target_var')\ndot.edge('event_contributions', 'target_var')\ndot.edge('product_price_contribution', 'target_var')\n\ndot.edge('impressions_x2', 'target_var')\ndot.edge('impressions_x3', 'target_var')\ndot.edge('impressions_x4', 'target_var')\n\n# Render the graph\ndot\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{align}\n\\text{Target} &\\sim \\sum_{i \\in \\{2,3,4\\}} f_i(\\text{impressions}_i) + \\\\\n&\\text{event\\_contributions} + \\\\\n&\\text{product\\_price\\_contribution} + \\\\\n&\\text{trend} + \\\\\n&\\text{noise}\n\\end{align}\n\\]\nWhere \\(f_i\\) represents the forward pass function (adstock and saturation) applied to each channel’s impressions.\n\n\nCode\ntarget_var = rewrite_graph(\n    impressions_x4_forward + \n    impressions_x3_forward +\n    impressions_x2_forward +\n    pt_event_contributions +\n    pt_product_price_contribution + \n    trend + \n    global_noise\n)\n\n# Eval target_var and plot\nnp_target_var = target_var.eval({\n    \"spend_x4\": pz_spend_x4,\n    \"spend_x3\": pz_spend_x3,\n    \"spend_x2\": pz_spend_x2,\n    \"spend_x1\": pz_spend_x1,\n    \"event_signal\": event_signal[:-1],\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    \"beta_x2\": pz_beta_x2,\n    \"beta_x3\": pz_beta_x3,\n    \"beta_x4\": pz_beta_x4,\n    \"beta_x1\": pz_beta_x1,\n    \"saturation_lam_x2\": .5,\n    \"saturation_alpha_x2\": .2,\n    \"saturation_lam_x3\": .7,\n    \"saturation_alpha_x3\": .7,\n    \"saturation_lam_x4\": .2,\n    \"saturation_alpha_x4\": .1,\n    \"global_adstock_alpha\": .2,\n    \"product_price\": pz_product_price,\n    \"event_contributions\": np_event_contributions[:-1],\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value,\n    \"trend\": np_trend,\n    \"global_noise\": pz_global_noise,\n})\n\nplt.plot(np_target_var, linewidth=2)\nplt.title('Target Variable Over Time', fontsize=14)\nplt.xlabel('Time Period', fontsize=12)\nplt.ylabel('Target Value', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNow, we can imagine our dataframe in this case will be something like the following:\n\n\nCode\n# make dataset with impressions x1, x2, x3, x4 and target_var\nscaler_factor_for_all = 150\ndates = pd.date_range(start='2020-01-01', periods=n_observations, freq='D')\ndata = pd.DataFrame({\n    \"date\": dates,\n    \"target_var\": np.round(np_target_var * scaler_factor_for_all, 4),\n    \"impressions_x1\": np.round(impressions_x1.eval(x1_deps) * scaler_factor_for_all, 4),\n    \"impressions_x2\": np.round(impressions_x2.eval(x2_deps) * scaler_factor_for_all, 4),\n    \"impressions_x3\": np.round(impressions_x3.eval(x3_deps) * scaler_factor_for_all, 4),\n    \"impressions_x4\": np.round(impressions_x4.eval(x4_deps) * scaler_factor_for_all, 4),\n    \"event_2020_09\": np.round(signals_independent[0][:-1], 4),\n    \"event_2020_12\": np.round(signals_independent[1][:-1], 4),\n    \"event_2021_09\": np.round(signals_independent[2][:-1], 4),\n    \"event_2021_12\": np.round(signals_independent[3][:-1], 4),\n    \"event_2022_09\": np.round(signals_independent[4][:-1], 4),\n})\ndata[\"trend\"] = data.index\ndata.head()\n\n\n\n\n\n\n\n\n\ndate\ntarget_var\nimpressions_x1\nimpressions_x2\nimpressions_x3\nimpressions_x4\nevent_2020_09\nevent_2020_12\nevent_2021_09\nevent_2021_12\nevent_2022_09\ntrend\n\n\n\n\n0\n2020-01-01\n128.7894\n112.9178\n30.9076\n34.3534\n15.0851\n0.0\n0.0\n0.0\n0.0\n0.0\n0\n\n\n1\n2020-01-02\n123.5265\n74.9429\n4.3523\n27.7279\n14.7826\n0.0\n0.0\n0.0\n0.0\n0.0\n1\n\n\n2\n2020-01-03\n98.5682\n0.0000\n0.0000\n0.0000\n0.0000\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n3\n2020-01-04\n107.3861\n5.3253\n0.0001\n12.7077\n13.7833\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n\n\n4\n2020-01-05\n93.9367\n0.0000\n0.0000\n0.0001\n5.6283\n0.0\n0.0\n0.0\n0.0\n0.0\n4\n\n\n\n\n\n\n\nIf we don’t think in a causal way, we will probably just say, “lets add all to the blender”.\n\n\nCode\n# Building priors for adstock and saturation\nadstock_priors = {\n    \"alpha\": Prior(\"Beta\", alpha=1, beta=1, dims=\"channel\"),\n}\n\nadstock = GeometricAdstock(l_max=28, priors=adstock_priors)\n\nsaturation_priors = {\n    \"lam\": Prior(\n        \"Gamma\",\n        mu=2,\n        sigma=1,\n        dims=\"channel\",\n    ),\n    \"alpha\": Prior(\n        \"Gamma\",\n        mu=.5,\n        sigma=.5,\n        dims=\"channel\",\n    ),\n}\n\nsaturation = MichaelisMentenSaturation(priors=saturation_priors)\n\n# Split data into train and test sets\ntrain_idx = 879\n\nX_train = data.iloc[:train_idx].drop(columns=[\"target_var\"])\nX_test = data.iloc[train_idx:].drop(columns=[\"target_var\"])\ny_train = data.iloc[:train_idx][\"target_var\"]\ny_test = data.iloc[train_idx:][\"target_var\"]\n\ncontrol_columns = [\n    \"event_2020_09\", \"event_2020_12\", \n    \"event_2021_09\", \"event_2021_12\", \n    \"event_2022_09\",\n    \"trend\"\n]\nchannel_columns = [\n    col for col in X_train.columns if col not in control_columns and col != \"date\"\n]\n\n# Model config\nmodel_config = {\n    \"likelihood\": Prior(\n        \"TruncatedNormal\",\n        lower=0,\n        sigma=Prior(\"HalfNormal\", sigma=1),\n        dims=\"date\",\n    ),\n}\n\n# sampling options for PyMC\nsample_kwargs = {\n    \"tune\": 1000,\n    \"draws\": 500,\n    \"chains\": 4,\n    \"random_seed\": 42,\n    \"target_accept\": 0.94,\n    \"nuts_sampler\": \"nutpie\",\n}\n\nnon_causal_mmm = MMM(\n    date_column=\"date\",\n    channel_columns=channel_columns,\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\nnon_causal_mmm.build_model(X_train, y_train)\n\n\n\n\n\n\n\n\nBuilding the model\n\n\n\nAll PyMC models are structural causal models, which means they represent the causal generative process of the data. We can visualize this process through a Directed Acyclic Graph (DAG) that shows how variables influence each other in the model.\n\n\nCode\nnon_causal_mmm.model.to_graphviz()\n\n\n\n\n\n\n\n\n\n\n\nOnce the model is build, we can train it.\n\n\nCode\nnon_causal_mmm.fit(X_train, y_train,)\nnon_causal_mmm.sample_posterior_predictive(X_train, extend_idata=True, combined=True)\n\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for 3 minutes\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.19\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.20\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.19\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.20\n                    15\n                \n            \n            \n        \n    \n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 14MB\nDimensions:  (sample: 2000, date: 879)\nCoordinates:\n  * date     (date) datetime64[ns] 7kB 2020-01-01 2020-01-02 ... 2022-05-28\n  * sample   (sample) object 16kB MultiIndex\n  * chain    (sample) int64 16kB 0 0 0 0 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3 3 3 3\n  * draw     (sample) int64 16kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    y        (date, sample) float64 14MB 105.2 112.7 110.4 ... 173.6 138.0 170.2\nAttributes:\n    created_at:                 2025-05-02T09:20:59.422069+00:00\n    arviz_version:              0.21.0\n    inference_library:          pymc\n    inference_library_version:  5.22.0xarray.DatasetDimensions:sample: 2000date: 879Coordinates: (4)date(date)datetime64[ns]2020-01-01 ... 2022-05-28array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2022-05-26T00:00:00.000000000',\n       '2022-05-27T00:00:00.000000000', '2022-05-28T00:00:00.000000000'],\n      dtype='datetime64[ns]')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(date, sample)float64105.2 112.7 110.4 ... 138.0 170.2array([[105.24899531, 112.70472111, 110.35957187, ..., 136.554518  ,\n        119.67973635,  83.33534944],\n       [112.89605536, 105.37323187, 129.24776643, ..., 110.99179285,\n        120.80744187, 102.97246333],\n       [129.7807535 , 108.39683985, 133.47828408, ..., 116.6551268 ,\n        117.00608376, 118.91714801],\n       ...,\n       [141.27679806, 165.07206169, 177.15525813, ..., 184.50697353,\n        150.78626141, 141.1849434 ],\n       [144.81503318, 141.44717   , 157.53667586, ..., 166.16366307,\n        156.20077728, 154.7101765 ],\n       [169.86154747, 163.74857913, 130.7232915 , ..., 173.55629339,\n        138.02364029, 170.17288333]])Indexes: (2)datePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2022-05-19', '2022-05-20', '2022-05-21', '2022-05-22',\n               '2022-05-23', '2022-05-24', '2022-05-25', '2022-05-26',\n               '2022-05-27', '2022-05-28'],\n              dtype='datetime64[ns]', name='date', length=879, freq=None))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (4)created_at :2025-05-02T09:20:59.422069+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.22.0\n\n\nWe are happy with our model, we don’t get any divergencies, and the sampling looks good.\n\n\nCode\n# Number of diverging samples\nprint(\n    f\"Total divergencies: {non_causal_mmm.idata['sample_stats']['diverging'].sum().item()}\"\n)\n\naz.summary(\n    data=non_causal_mmm.fit_result,\n    var_names=[\n        \"intercept\",\n        \"y_sigma\",\n        \"saturation_alpha\",\n        \"saturation_lam\",\n        \"adstock_alpha\",\n    ],\n)\n\n\nTotal divergencies: 0\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n0.569\n0.006\n0.558\n0.581\n0.000\n0.000\n1246.0\n1153.0\n1.00\n\n\ny_sigma\n0.072\n0.002\n0.069\n0.076\n0.000\n0.000\n2519.0\n1281.0\n1.00\n\n\nsaturation_alpha[impressions_x1]\n0.015\n0.017\n0.000\n0.043\n0.000\n0.001\n1451.0\n1143.0\n1.00\n\n\nsaturation_alpha[impressions_x2]\n0.014\n0.015\n0.000\n0.041\n0.000\n0.001\n1584.0\n913.0\n1.00\n\n\nsaturation_alpha[impressions_x3]\n0.037\n0.040\n0.000\n0.105\n0.001\n0.002\n868.0\n819.0\n1.01\n\n\nsaturation_alpha[impressions_x4]\n0.075\n0.054\n0.000\n0.170\n0.002\n0.002\n615.0\n362.0\n1.00\n\n\nsaturation_lam[impressions_x1]\n2.362\n1.083\n0.582\n4.373\n0.021\n0.024\n2467.0\n1331.0\n1.00\n\n\nsaturation_lam[impressions_x2]\n2.339\n1.126\n0.391\n4.333\n0.022\n0.028\n2251.0\n1148.0\n1.00\n\n\nsaturation_lam[impressions_x3]\n2.346\n1.094\n0.495\n4.276\n0.024\n0.025\n1806.0\n1086.0\n1.00\n\n\nsaturation_lam[impressions_x4]\n2.328\n1.076\n0.623\n4.379\n0.026\n0.029\n1604.0\n1258.0\n1.00\n\n\nadstock_alpha[impressions_x1]\n0.501\n0.307\n0.039\n0.988\n0.005\n0.004\n2895.0\n1398.0\n1.01\n\n\nadstock_alpha[impressions_x2]\n0.518\n0.281\n0.068\n0.993\n0.005\n0.005\n3152.0\n1254.0\n1.01\n\n\nadstock_alpha[impressions_x3]\n0.668\n0.275\n0.132\n1.000\n0.007\n0.005\n2122.0\n1312.0\n1.00\n\n\nadstock_alpha[impressions_x4]\n0.506\n0.231\n0.067\n0.898\n0.005\n0.004\n2302.0\n1239.0\n1.00\n\n\n\n\n\n\n\nIf our model has a correct understanding of causality, we can use it to perform a do-calculus to estimate the effect of our channel, using out of sample (sampling from the posterior). Mathematically, we want to compute the causal effect as the difference between two interventions: \\[P(Y|do(X=x)) - P(Y|do(X=0))\\]\nThis should allows us to isolate the causal impact of our marketing channels on the outcome variable.\n\n\nCode\nX_test_x2_zero = X_test.copy()\nX_test_x2_zero[\"impressions_x2\"].iloc[:100] = 0\n\ny_do_x2_zero = non_causal_mmm.sample_posterior_predictive(\n    X_test_x2_zero, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\ny_do_x2 = non_causal_mmm.sample_posterior_predictive(\n    X_test, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\nNow that we have both posteriors, we can compute the difference between the period with the index 880-890 and plot the causal effect and the cumulative causal effect.\n\n\nCode\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect = (y_do_x2_zero - y_do_x2).y\n# Get dates from the coordinates for x-axis\ndates = x2_causal_effect.coords['date'].values[:100]  # Take only first 100 days\n\n# Plot the causal effect\nplt.subplot(1, 2, 1)\n# Calculate mean and quantiles\nmean_effect = x2_causal_effect.mean(dim=\"sample\")[:100]\nplt.plot(dates, mean_effect)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\n# For cumulative effect, compute quantiles directly from cumulative sums\ncum_effect = x2_causal_effect.cumsum(dim=\"date\")\ncum_mean = cum_effect.mean(dim=\"sample\")[:100]\nplt.plot(dates, cum_mean)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nIn reality, in order to validate the following estimated effect, we’ll need to run an actual experiment. Because we did the data generation process we can run this actual experiment to compare.\n\n\nCode\n# Create an intervened spend_x2 with zeros between index 880 and 980\nintervened_spend_x2 = pz_spend_x2.copy()\nintervened_spend_x2[880:980] = 0\n\n# Evaluate target variable with the intervention\nnp_target_var_x2_zero = target_var.eval({\n    \"spend_x4\": pz_spend_x4,\n    \"spend_x3\": pz_spend_x3,\n    \"spend_x2\": intervened_spend_x2,\n    \"spend_x1\": pz_spend_x1,\n    \"event_signal\": event_signal[:-1],\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    \"beta_x2\": pz_beta_x2,\n    \"beta_x3\": pz_beta_x3,\n    \"beta_x4\": pz_beta_x4,\n    \"beta_x1\": pz_beta_x1,\n    \"saturation_lam_x2\": .5,\n    \"saturation_alpha_x2\": .2,\n    \"saturation_lam_x3\": .7,\n    \"saturation_alpha_x3\": .7,\n    \"saturation_lam_x4\": .2,\n    \"saturation_alpha_x4\": .1,\n    \"global_adstock_alpha\": .2,\n    \"product_price\": pz_product_price,\n    \"event_contributions\": np_event_contributions[:-1],\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value,\n    \"trend\": np_trend,\n    \"global_noise\": pz_global_noise,\n})\n\n# x2 total effect y | do(x2=&gt;1) - y | do(x2=0)\nx2_intervention_real_effect = np_target_var_x2_zero - np_target_var\nx2_intervention_real_cumulative_effect = np.cumsum(x2_intervention_real_effect)\n\n# Plot both the intervention effect and cumulative effect\nplt.subplot(1, 2, 1)\n# Plot the daily effect\ndaily_effect = x2_intervention_real_effect[880:980] * scaler_factor_for_all\nplt.plot(dates, daily_effect)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\ncumulative_effect = x2_intervention_real_cumulative_effect[880:980] * scaler_factor_for_all\nplt.plot(dates, cumulative_effect)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHow does compare to the recovered effect? Let’s observe! 👀\n\n\nCode\n# Create a figure to compare real effects with estimated effects\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe initial model have been under estimating the effect of \\(X2\\). We can see the model was thinking we’ll loosing almost none users when in reality wi’ll loose around 600 in total. Maybe we did something wrong? Are we maybe the wrong causal question?\nThat doesn’t matter, we have calibration! 🤪\nLets compute the observable delta in Y and observable delta in X and use it for calibration.\n\n\nCode\nintervened_channel = \"impressions_x2\"\ntotal_observed_effect = cumulative_effect[-1] # delta Y\ntotal_previous_imp_before_intervention = X_train[intervened_channel].iloc[-100:].sum()\ntotal_change_imp_during_intervention = -X_train[intervened_channel].iloc[-100:].sum()\nsigma = 0.3 # confidence in the experiment.\n\ndf_lift_test = pd.DataFrame(\n    [{\n        \"channel\": intervened_channel,\n        \"x\": total_previous_imp_before_intervention,\n        \"delta_x\": total_change_imp_during_intervention,\n        \"delta_y\": total_observed_effect,\n        \"sigma\": sigma,\n    }]\n)\n\nintervened_data = data.copy()\nintervened_data.loc[880:980, \"impressions_x2\"] = 0\n\nnon_causal_mmm2 = MMM(\n    date_column=\"date\",\n    channel_columns=channel_columns,\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\nnon_causal_mmm2.build_model(\n    intervened_data.drop(columns=[\"target_var\"]), \n    intervened_data[\"target_var\"]\n)\n\nnon_causal_mmm2.add_lift_test_measurements(df_lift_test)\nnon_causal_mmm2.model.to_graphviz()\n\n\n\n\n\n\n\n\n\nAs we can see a new observational point have been added to our data. This new point must be satisfied as the rest of our data, pooling parameter into a new direction.\n\n\n\n\n\n\nNote\n\n\n\nIn a Bayesian model, each observation—whether it is a daily data point \\(y_t\\) or a lift measurement \\(\\Delta y\\)—contributes a term to the likelihood. The posterior arises from the product of all these likelihood terms and the prior(s). In other words, theres no actual difference between priors and data, they both carry the same weight and multiply in the numerator of Bayes theorem. There’s no discrete “decision” about which part of the data (or which prior) to weight more; it all goes into the same log‐posterior function. The sampling or optimization algorithm (MCMC, variational inference, etc.) explores the parameter space in proportion to the posterior probability (which is prior × likelihood). Whichever parameters jointly give higher posterior density get visited more often by the sampler.\n\n\n\n\nCode\nnon_causal_mmm2.fit(\n    intervened_data.drop(columns=[\"target_var\"]), \n    intervened_data[\"target_var\"],\n)\nnon_causal_mmm2.sample_posterior_predictive(\n    intervened_data.drop(columns=[\"target_var\"]), \n    extend_idata=True, \n    combined=True\n)\n\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for 2 minutes\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.26\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.25\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.23\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.25\n                    15\n                \n            \n            \n        \n    \n\n\n\nSampling: [lift_measurements, y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 17MB\nDimensions:                  (lift_measurements_dim_0: 1, sample: 2000,\n                              date: 1050)\nCoordinates:\n  * lift_measurements_dim_0  (lift_measurements_dim_0) int64 8B 0\n  * date                     (date) datetime64[ns] 8kB 2020-01-01 ... 2022-11-15\n  * sample                   (sample) object 16kB MultiIndex\n  * chain                    (sample) int64 16kB 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3\n  * draw                     (sample) int64 16kB 0 1 2 3 4 ... 496 497 498 499\nData variables:\n    lift_measurements        (lift_measurements_dim_0, sample) float64 16kB 0...\n    y                        (sample, date) float64 17MB 154.1 136.0 ... 176.0\nAttributes:\n    created_at:                 2025-05-02T09:23:58.826836+00:00\n    arviz_version:              0.21.0\n    inference_library:          pymc\n    inference_library_version:  5.22.0xarray.DatasetDimensions:lift_measurements_dim_0: 1sample: 2000date: 1050Coordinates: (5)lift_measurements_dim_0(lift_measurements_dim_0)int640array([0])date(date)datetime64[ns]2020-01-01 ... 2022-11-15array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2022-11-13T00:00:00.000000000',\n       '2022-11-14T00:00:00.000000000', '2022-11-15T00:00:00.000000000'],\n      dtype='datetime64[ns]')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (2)lift_measurements(lift_measurements_dim_0, sample)float640.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0array([[0., 0., 0., ..., 0., 0., 0.]])y(sample, date)float64154.1 136.0 122.8 ... 185.2 176.0array([[154.10178015, 136.04633333, 122.8062001 , ..., 161.40825944,\n        184.65368663, 173.77517009],\n       [115.71845411, 120.21068044,  98.1140684 , ..., 165.41906178,\n        186.15679515, 142.88529355],\n       [151.65445397, 107.41969331, 103.72560982, ..., 170.70583306,\n        179.673357  , 150.72316649],\n       ...,\n       [118.72439986, 140.22472176, 127.09869182, ..., 172.16606115,\n        143.10723437, 160.18956132],\n       [115.77755841, 137.74388718, 139.96448853, ..., 154.49453625,\n        161.87379289, 179.69172875],\n       [112.85046219, 115.19236861, 123.596831  , ..., 175.94217593,\n        185.18094823, 176.01012612]])Indexes: (3)lift_measurements_dim_0PandasIndexPandasIndex(Index([0], dtype='int64', name='lift_measurements_dim_0'))datePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2022-11-06', '2022-11-07', '2022-11-08', '2022-11-09',\n               '2022-11-10', '2022-11-11', '2022-11-12', '2022-11-13',\n               '2022-11-14', '2022-11-15'],\n              dtype='datetime64[ns]', name='date', length=1050, freq=None))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (4)created_at :2025-05-02T09:23:58.826836+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.22.0\n\n\nNow that our model is ready, we can check the new estimated effect.\n\n\nCode\ny_do_x2_zero_second_model = non_causal_mmm2.idata.posterior_predictive.copy()\ny_do_x2_second_model = non_causal_mmm2.sample_posterior_predictive(\n    data.drop(columns=[\"target_var\"]), \n    extend_idata=False, \n    include_last_observations=False, \n    combined=False,\n    random_seed=42\n)\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect_second_model = (y_do_x2_zero_second_model.y - y_do_x2_second_model.y).isel(date=slice(880, 980))\n\n# Plot the causal effect\nplt.subplot(1, 2, 1)\n# Calculate mean and quantiles\nmean_effect_second_model = x2_causal_effect_second_model.mean(dim=[\"chain\",\"draw\"])\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, mean_effect_second_model)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\n# For cumulative effect, compute quantiles directly from cumulative sums\ncum_effect_second_model = x2_causal_effect_second_model.cumsum(dim=\"date\")\ncum_mean_second_model = cum_effect_second_model.mean(dim=[\"chain\",\"draw\"])\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, cum_mean_second_model)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\n\n\nSampling: [lift_measurements, y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see the effect looks fully different. The size is 1000X higher than before. Let’s compare!\n\n\nCode\n# Create a figure to compare real effects with estimated effects\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, mean_effect_second_model, label='Estimated Effect (2)', color='orange', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, cum_mean_second_model, \n         label='Estimated Cumulative Effect (2)', color='orange', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs expected the new observation makes the model add more credit to X2 but this came with the price of an overestimation of the true impact. Meanwhile, it was true that X2 impact was bigger than the original one, the second model absorbe all the variability possibly explain by other variables such as X1, X3 and bring a 1000X more extra impact, with a very tight posterior.\n\n\nCode\n# plot the recovered mean daily contribution as distribution.\nchannels_contribution_original_scale_model1 = non_causal_mmm.compute_channel_contribution_original_scale()\n\nchannels_contribution_original_scale_model2 = non_causal_mmm2.compute_channel_contribution_original_scale()\n\n_dist1 = channels_contribution_original_scale_model1.isel(date=slice(0, 800)).mean(\n    dim=[\"date\"]\n).sel(channel=\"impressions_x2\").values.flatten()\n\n_dist2 = channels_contribution_original_scale_model2.isel(date=slice(0, 800)).mean(\n    dim=[\"date\"]\n).sel(channel=\"impressions_x2\").values.flatten()\n\n\n# First subplot for Model 1\nplt.subplot(1, 2, 1)\nsns.kdeplot(_dist1, shade=True, label=\"Model 1\", bw_adjust=4.5)\nplt.title(\"Distribution of Channel X2 Contribution - Model 1\", fontsize=12)\nplt.xlabel(\"Contribution Value\", fontsize=10)\nplt.ylabel(\"Density\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=9)\n\n# Second subplot for Model 2\nplt.subplot(1, 2, 2)\nsns.kdeplot(_dist2, shade=True, label=\"Model 2\", bw_adjust=4.5, color=\"orange\")\nplt.title(\"Distribution of Channel X2 Contribution - Model 2\", fontsize=12)\nplt.xlabel(\"Contribution Value\", fontsize=10)\nplt.ylabel(\"Density\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Danger of Tight Posteriors\n\n\n\nIt’s important to note that a tight posterior distribution (like we see in Model 2) should never be understood as the model being more correct or certain about the true causal effect. This is a common misconception in Bayesian analysis.\nA tight posterior simply means the model is very confident in its estimates given the data and prior assumptions it has, but says nothing about whether those assumptions are correct. In this case, the addition of the lift test measurement has created a model that is very confident in an incorrect answer.\nThis illustrates an important principle in causal inference and Bayesian modeling: precision is not the same as accuracy. A model can be precisely wrong - having a narrow posterior around an incorrect value. This often happens when:\n\nThe model structure doesn’t match the true causal process\nImportant confounders are omitted\nThe priors or likelihood are misspecified\n\n\n\nWhy all the following happened? lets take a look to the graph.\n\n\nCode\ndot\n\n\n\n\n\n\n\n\n\nThis DAG shows:\n\nDirect Spend-to-Impression Relationships: Each spend variable (X1-X4) directly influences its corresponding impression variable.\nCross-Channel Effects:\n\nImpressions from X1 influence impressions from X3\nImpressions from X2 influence both X3 and X4 impressions\nEvents influence impressions for X2 and X3\n\n\nIf we were to build a naive regression model including all variables (X1, X2, X3, X4), we would encounter significant estimation problems, particularly for X2. According to Pearl’s causal theory.\n\n1. Collider Bias\nIn our graph, X2 influences X3 and X4, which both influence the target variable. This creates a collider structure where conditioning on x1 variable because induces a spurious correlation between X2, X3. This violates the independence assumptions of standard regression.\n\n\n2. Mediator Effects\nX2 has both direct effects on the target variable and indirect effects through X3 and X4. A naive regression would conflate these paths, leading to inconsistent estimates of X2’s true total causal effect.\n\n\n3. Confounding from Events\nEvents influence both X2 impressions and the target variable directly. Without properly accounting for this common cause, the estimate for X2 will capture some of the effect that actually comes from events.\nAll the above means, in order to estimate the effect of X2 we need to address the primal causal questions.\n\n\n4. Minimal Adjustment Set for X2\nTo estimate the total causal effect of X2 on the target variable, we need to identify the minimal adjustment set that blocks all non-causal paths while preserving the causal paths. According to Pearl’s backdoor criterion, we must control for any confounders (common causes) while avoiding adjusting for colliders or mediators. In our DAG, the minimal adjustment set for estimating X2’s total effect would include Events (as it’s a confounder affecting both X2 and the target) and Spend X1 (as it influences the target through X3, creating a backdoor path). We should not adjust for impressions_x3 or impressions_x4, as these are mediators through which X2 partially exerts its effect on the target variable. Nevertheless, events are a cofounder of X2, meaning, we need to control for them if we want to get the estimates right on spot.\nThe proper identification of this minimal adjustment set is crucial for unbiased estimation. If we control for too few variables, confounding bias remains. If we control for mediators, we block part of the causal effect we’re trying to measure. This highlights why structural causal models are superior to naive regression approaches - they allow us to explicitly model the causal pathways and make appropriate adjustments based on causal reasoning rather than statistical correlation. By conditioning only on the minimal adjustment set, we can obtain a consistent estimate of X2’s total causal effect, including both its direct impact and indirect effects through other channels.\nSo, let’s see what happen if we apply causal theory 😃\n\n\nCode\n# Lets rebuild our media mix model\ncausal_mmm = MMM(\n    date_column=\"date\",\n    channel_columns=[\"impressions_x2\"],\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\ncausal_mmm.fit(X_train, y_train,)\ncausal_mmm.sample_posterior_predictive(X_train, extend_idata=True, combined=True)\n\n\n\n\n\n\n\n\n    Sampler Progress\n    Total Chains: 4\n    Active Chains: 0\n    \n        Finished Chains:\n        4\n    \n    Sampling for a minute\n    \n        Estimated Time to Completion:\n        now\n    \n\n    \n    \n    \n        \n            \n                Progress\n                Draws\n                Divergences\n                Step Size\n                Gradients/Draw\n            \n        \n        \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.21\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.19\n                    31\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.19\n                    15\n                \n            \n                \n                    \n                        \n                        \n                    \n                    1500\n                    0\n                    0.22\n                    15\n                \n            \n            \n        \n    \n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 14MB\nDimensions:  (sample: 2000, date: 879)\nCoordinates:\n  * date     (date) datetime64[ns] 7kB 2020-01-01 2020-01-02 ... 2022-05-28\n  * sample   (sample) object 16kB MultiIndex\n  * chain    (sample) int64 16kB 0 0 0 0 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3 3 3 3\n  * draw     (sample) int64 16kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    y        (date, sample) float64 14MB 125.6 123.1 93.13 ... 154.9 155.2 166.0\nAttributes:\n    created_at:                 2025-05-02T09:25:05.478915+00:00\n    arviz_version:              0.21.0\n    inference_library:          pymc\n    inference_library_version:  5.22.0xarray.DatasetDimensions:sample: 2000date: 879Coordinates: (4)date(date)datetime64[ns]2020-01-01 ... 2022-05-28array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2022-05-26T00:00:00.000000000',\n       '2022-05-27T00:00:00.000000000', '2022-05-28T00:00:00.000000000'],\n      dtype='datetime64[ns]')sample(sample)objectMultiIndexarray([(0, 0), (0, 1), (0, 2), ..., (3, 497), (3, 498), (3, 499)], dtype=object)chain(sample)int640 0 0 0 0 0 0 0 ... 3 3 3 3 3 3 3 3array([0, 0, 0, ..., 3, 3, 3])draw(sample)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(date, sample)float64125.6 123.1 93.13 ... 155.2 166.0array([[125.6289921 , 123.12912833,  93.1308625 , ..., 123.10773868,\n        118.21674564, 127.22796179],\n       [116.9573604 , 106.16434355, 149.65025282, ...,  97.2714402 ,\n        122.40256994, 107.06750596],\n       [112.97735529, 103.41318743, 106.80332019, ..., 116.32189034,\n         99.09141556, 101.27925967],\n       ...,\n       [162.85329414, 152.58125226, 154.75744827, ..., 158.27544073,\n        162.46096276, 171.44496086],\n       [161.87308558, 146.70198717, 137.49020491, ..., 158.11672742,\n        163.9178362 , 149.69364311],\n       [153.83869778, 146.64408655, 139.66232214, ..., 154.93617656,\n        155.2024514 , 165.98772733]])Indexes: (2)datePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2022-05-19', '2022-05-20', '2022-05-21', '2022-05-22',\n               '2022-05-23', '2022-05-24', '2022-05-25', '2022-05-26',\n               '2022-05-27', '2022-05-28'],\n              dtype='datetime64[ns]', name='date', length=879, freq=None))samplechaindrawPandasMultiIndexPandasIndex(MultiIndex([(0,   0),\n            (0,   1),\n            (0,   2),\n            (0,   3),\n            (0,   4),\n            (0,   5),\n            (0,   6),\n            (0,   7),\n            (0,   8),\n            (0,   9),\n            ...\n            (3, 490),\n            (3, 491),\n            (3, 492),\n            (3, 493),\n            (3, 494),\n            (3, 495),\n            (3, 496),\n            (3, 497),\n            (3, 498),\n            (3, 499)],\n           name='sample', length=2000))Attributes: (4)created_at :2025-05-02T09:25:05.478915+00:00arviz_version :0.21.0inference_library :pymcinference_library_version :5.22.0\n\n\nNow, lets repeat again the estimation of the effect when X2 is zero.\n\n\nCode\nX_test_x2_zero = X_test.copy()\nX_test_x2_zero[\"impressions_x2\"].iloc[:100] = 0\n\ny_do_x2_zero_causal = causal_mmm.sample_posterior_predictive(\n    X_test_x2_zero, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\ny_do_x2_causal = causal_mmm.sample_posterior_predictive(\n    X_test, extend_idata=False, include_last_observations=True, random_seed=42\n)\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect_causal = (y_do_x2_zero_causal - y_do_x2_causal).y\n# Get dates from the coordinates for x-axis\ndates = x2_causal_effect_causal.coords['date'].values[:100]  # Take only first 100 days\n\n# Calculate mean and quantiles\nmean_effect = x2_causal_effect_causal.mean(dim=\"sample\")[:100]\ncum_effect = x2_causal_effect_causal.cumsum(dim=\"date\")\ncum_mean = cum_effect.mean(dim=\"sample\")[:100]\n\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreat, as expected the true causal effect for X2 was recovered, and its possible to prove with an experiment. This just prove that maths are not magic, and that if we want to create models that explain the dynamics of the world, we need to use causal reasoning to it 🔥🙌🏻\n\n\n\nConclusion\nThe evidence is clear: calibration cannot rescue a misspecified causal model. We’ve seen that:\n\nCausal misspecification persists despite calibration. Our Model 2 became confidently wrong after calibration—tight posteriors around incorrect values.\nColliders and mediators matter. Standard MMMs ignore that marketing channels influence each other, creating spurious correlations that no amount of experimental data can fix.\nAdjustment sets are crucial. Simply including every variable yields biased estimates; we must control only for confounders while preserving causal pathways.\n\nWhen we finally built a causally-aware MMM—controlling for events as confounders but avoiding adjustment for mediators—our estimates matched the ground truth. The same experimental evidence that couldn’t rescue our misspecified model perfectly aligned with our correctly specified one.\nThe message: invest in causal discovery before calibration. Draw your DAGs. Identify your minimal adjustment sets. No amount of experimental evidence will save a model asking the wrong causal question.\nAs Pearl might say: statistics tells us what the data says; causality tells us what to do with it.\nCalibration without causation is just computation without comprehension!\n\n\nCode\n%load_ext watermark\n%watermark -n -u -v -iv -w -p pymc_marketing,pytensor\n\n\nLast updated: Fri May 02 2025\n\nPython implementation: CPython\nPython version       : 3.11.8\nIPython version      : 8.30.0\n\npymc_marketing: 0.13.0\npytensor      : 2.30.3\n\npymc_marketing: 0.13.0\narviz         : 0.21.0\npandas        : 2.2.3\npymc          : 5.22.0\npreliz        : 0.16.0\npytensor      : 2.30.3\nIPython       : 8.30.0\nnumpy         : 2.1.3\ngraphviz      : 0.20.3\nmatplotlib    : 3.10.1\nseaborn       : 0.13.2\n\nWatermark: 2.5.0"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Welcome to my collection of articles. Here you’ll find my thoughts, tutorials, and research on various topics."
  },
  {
    "objectID": "articles.html#featured-articles",
    "href": "articles.html#featured-articles",
    "title": "Articles",
    "section": "Featured Articles",
    "text": "Featured Articles\n\n\n\nNo More Experiments Without Causality\n\n\nAn article discussing the importance of causality in experiments. Talk given in PyData DE Darmstadt 2025.\n\nRead More\n\n\n\n\n\n\n\n\nBaby Steps for Causal Discovery\n\n\nAn article discussing the importance of causality in experiments. Talk given in PyData Tallinn 2025.\n\nRead More"
  },
  {
    "objectID": "articles.html#all-articles",
    "href": "articles.html#all-articles",
    "title": "Articles",
    "section": "All Articles",
    "text": "All Articles\n\nNo More Experiments Without Causality - April 2025\nBaby Steps for Causal Discovery - February 2025"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a globe-trotting data professional with a passion for statistics and mathematics. From studying in Venezuela to collaborating with teams across continents, my journey has been a whirlwind of diverse experiences. I’m all about using data to transform businesses and solve complex problems.\nInterested in learning more about how to navigate the digital landscape and harness the power of data? Check out my articles below and let’s connect for some data-driven discussions and creative problem-solving. Looking forward to hearing from you! 👇🏻"
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About Me",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n2024 - Present\n\n\nMarketing Scientist\n\n\nWise\n\n\nTallinn, Estonia\n\n\nData scientist for the Marketing team, dedicated to measuring and improving the region’s global strategies through the use of data.\n\n\nCareer Progression at Wise\n\nLead Data Scientist (March 2025 - Present)\n\n[Role responsibilities to be added]\n\nSenior Marketing Scientist (January 2024 - March 2025)\n\n[Role responsibilities to be added]\n\n\n\n\nRead more\n\n\n\n\n2024 - Present\n\n\nPrincipal Data Scientist\n\n\nPyMC Labs\n\n\nRemote\n\n\nSolving complex business problems through advanced Bayesian statistical methods.\n\n\n\nSolving business problems through Bayesian Statistics\n[Additional responsibilities to be added]\n\n\n\nRead more\n\n\n\n\n2022 - 2024\n\n\nMarketing ads, Functional Analyst\n\n\nBolt\n\n\nTallinn, Estonia\n\n\nI relocated from Chile to Estonia and am currently applying data analytics to determine the most effective strategies for optimizing acquisition marketing efforts based on profitability.\n\n\n\nApplied data analytics to determine best strategies to optimize acquisition marketing efforts based on profitability across Europe and South Africa\nPerformed daily analysis using Python or R to run marketing mix, causal impact and budget allocation models\nEstimated marketing effects around the world to achieve company goals\nRelocated from Chile to Estonia\n\n\n\nRead more\n\n\n\n\n2020 - 2022\n\n\nHead of Data Analytics\n\n\nOmnicom Media Group\n\n\nSantiago, Chile\n\n\nLed the data department of the Marketing Science LATAM group, developing value solutions and creating analytical services around the region.\n\n\nCareer Progression at Omnicom Media Group\n\nHead of Data Analytics (October 2021 - January 2022)\n\nLed the data department of the Marketing Science LATAM group\nDeveloped value solutions and created analytical services around the region\nUtilized cloud technologies (AWS/GCP) to develop sustainable data structures\nMaintained direct communication with Google and Facebook teams to develop use cases and service optimization\nCreated innovation proposals and fostered data culture to optimize operational processes\n\nData Analytics Manager (July 2020 - October 2021)\n\nManaged analyst team and ensured correct delivery of dashboards on platforms such as Tableau, Power BI, and Data Studio\nDeveloped architectures in GCP and managed databases in AWS\nMaintained direct contact with OMG teams in Miami for data centralization and optimization of information flows\nUtilized visualization tools, Python/R, SQL, and NoSQL for data pre-processing\n\n\n\n\nRead more\n\n\n\n\n2019 - 2020\n\n\nData Analyst\n\n\nRaya\n\n\nSantiago, Chile\n\n\nManaged analysis and reporting for different agency brands including Huawei, MercadoLibre, Concha y Toro, and Walmart Chile.\n\n\n\nResponsible for interpretation, analysis, and reporting for different agency brands\nManaged Brandwatch, Google Analytics, Tag Manager and similar tools to determine guidelines for paid or organic campaigns\nDeveloped categorization and prediction models in Python for process optimization and advanced variable analysis\nWorked with brands including Huawei, MercadoLibre, Concha y Toro, and Walmart Chile\n\n\n\nRead more\n\n\n\n\n2018 - 2019\n\n\nPerformance Manager\n\n\nOYM Agencia\n\n\nSantiago, Chile\n\n\nManaged digital marketing strategies for multiple brands, including analysis, optimization and investment across major platforms.\n\n\n\nManaged analysis, optimization and investment in digital platforms including Facebook, Google Ads, and LinkedIn\nDeveloped dashboards in Cyfe and DataStudio\nWorked with brands including Jansport, BodyAndSoul, Ursus Trotter, and over 15 Chilean real estate companies\n\n\n\nRead more"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\nData Scientist\nAcámica - 2020\n\nSpecialized in Data Science\n\n\n\nBachelor’s in Market Studies\nUniversidad José Antonio Páez - 2015-2019\n\nSpecialized in Market Research"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills\n\n\n\n🧑🏻‍💻\n\n\nProfessional Skills\n\n\n\n\nSEO/SEM Marketing\n\n\nProject Management\n\n\nPaid Social Media\n\n\nStatistical Analysis\n\n\nCausal Analysis\n\n\nLifetime Value Analysis\n\n\n\n\n📊\n\n\nLanguages\n\n\n\n\nHTML\n\n\nNon-SQL\n\n\nSQL\n\n\nPython\n\n\nJulia\n\n\nR\n\n\n\n\n⚙️\n\n\nTechnologies\n\n\n\n\nGoogle Cloud Platform\n\n\nAWS\n\n\nDocker\n\n\nGit/GitHub\n\n\nJupyter\n\n\nRStudio"
  },
  {
    "objectID": "about.html#download-cv",
    "href": "about.html#download-cv",
    "title": "About Me",
    "section": "Download CV",
    "text": "Download CV\nDownload CV (PDF)"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nHave a project in mind or just want to chat about data? There are two easy ways to get in touch:\n\n\nContact Me\n\n\nBook 30 Minutes"
  },
  {
    "objectID": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html",
    "href": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html",
    "title": "Baby Steps for Causal Discovery",
    "section": "",
    "text": "In this notebook, we’ll dive into how to uncover causal relationships in marketing data, a crucial step for understanding the true impact of various channels on business outcomes. We’ll start by generating synthetic data that mimics real-world marketing scenarios, complete with confounding variables and complex causal structures.\nNext, we’ll fit a Bayesian marketing mix model using PyMC-Marketing, check causal directions between variables, and perform mediation analysis to explore indirect effects. Finally, we’ll use structure discovery techniques to infer potential causal graphs. By the end, you’ll have a solid grasp of how to apply these techniques to reveal hidden causal insights in your marketing data.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pymc_marketing.mmm.transformers import geometric_adstock, michaelis_menten\n\nfrom pymc_marketing.mmm import MMM, GeometricAdstock, MichaelisMentenSaturation\nfrom pymc_marketing.prior import Prior\n\nimport networkx as nx\nfrom graphviz import Digraph\nimport pydot\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nfrom IPython.display import SVG, display\n\nimport numpy as np\nimport pandas as pd\n\nimport preliz as pz\nimport pymc as pm\n\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom causallearn.graph.Endpoint import Endpoint\nfrom causallearn.utils.GraphUtils import GraphUtils\nfrom causallearn.search.ScoreBased.GES import ges\nfrom causallearn.search.ConstraintBased.PC import pc\n\naz.style.use(\"arviz-darkgrid\")\nplt.rcParams[\"figure.figsize\"] = [8, 4]\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"axes.labelsize\"] = 6\nplt.rcParams[\"xtick.labelsize\"] = 6\nplt.rcParams[\"ytick.labelsize\"] = 6\nplt.rcParams.update({\"figure.constrained_layout.use\": True})\n\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = \"retina\"\n\nseed = sum(map(ord, \"Estimating effects despite having Confounding Variables\"))\nrng = np.random.default_rng(seed)\n\nprint(seed)\nprint(rng)\n\n\n5395\nGenerator(PCG64)"
  },
  {
    "objectID": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#data-generation",
    "href": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#data-generation",
    "title": "Baby Steps for Causal Discovery",
    "section": "Data generation",
    "text": "Data generation\nBased on the provided DAG, we can create some synthetic data to test how our model performs when we have a complex causal structures. Using the same data, we can test different model compositions and see how could we improve our model to uncover the true causal impact of each channel on the target variable.\nWe’ll start by setting the date range. Here we’ll use a date range from 2022-01-01 to 2024-11-06, meaning we have almost 3 years of data (1041 days).\n\n\nCode\n# date range\nmin_date = pd.to_datetime(\"2022-01-01\")\nmax_date = pd.to_datetime(\"2024-11-06\")\ndate_range = pd.date_range(start=min_date, end=max_date, freq=\"D\")\n\ndf = pd.DataFrame(data={\"date_week\": date_range}).assign(\n    year=lambda x: x[\"date_week\"].dt.year,\n    month=lambda x: x[\"date_week\"].dt.month,\n    dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n)\n\nn = df.shape[0]\nprint(f\"Number of observations: {n}\")\n\n\nNumber of observations: 1041\n\n\n\nHoliday signal\nCertain holidays, like Christmas, can have a significant impact on consumer behavior before and after the specific date, leading to seasonal spikes in sales. To capture these effects, we introduce a holiday signal based on Gaussian (normal) distributions centered around specific holiday dates.\nThe function used to model the holiday effect is defined as follows:\n\\[\nH_{t} = \\exp\\left(-0.5 \\left(\\frac{\\Delta t}{\\sigma}\\right)^2\\right)\n\\]\nWhere: - \\(\\Delta t\\) is the time difference (in days) between the current date and the holiday date. - \\(\\sigma\\) is the standard deviation that controls the spread of the effect around the holiday date.\nFor each holiday, we calculate the holiday signal across the date range and add a holiday contribution by scaling the signal with a holiday-specific coefficient. This approach models seasonal holiday spikes using Gaussian functions, which capture the transient increase in market activity around holidays, and their respective decay over time.\n\nNote: Here we assume a normally distributed signal, nevertheless the signal could be skew or not normal distributed.\n\n\n\nCode\nholiday_dates = [\"24-12\", \"31-12\", \"08-06\", \"07-09\"]  # List of holidays as month-day strings\nstd_devs = [5, 5, 3, 3]  # List of standard deviations for each holiday\nholidays_coefficients = [2, 3, 4, 6]\n\n# Initialize the holiday effect array\nholiday_signal = np.zeros(len(date_range))\nholiday_contributions = np.zeros(len(date_range))\n\n# Generate holiday signals\nfor holiday, std_dev, holiday_coef in zip(\n    holiday_dates, std_devs, holidays_coefficients, strict=False\n):\n    # Find all occurrences of the holiday in the date range\n    holiday_occurrences = date_range[date_range.strftime(\"%d-%m\") == holiday]\n\n    for occurrence in holiday_occurrences:\n        # Calculate the time difference in days\n        time_diff = (date_range - occurrence).days\n\n        # Generate the Gaussian basis for the holiday\n        _holiday_signal = np.exp(-0.5 * (time_diff / std_dev) ** 2)\n\n        # Add the holiday signal to the holiday effect\n        holiday_signal += _holiday_signal\n\n        holiday_contributions += _holiday_signal * holiday_coef\n\ndf[\"holiday_signal\"] = holiday_signal\ndf[\"holiday_contributions\"] = holiday_contributions\n\n# Plot the holiday effect\nfig, ax = plt.subplots()\nsns.lineplot(x=date_range, y=holiday_signal, ax=ax)\nax.set(title=\"Holiday Effect Signal\", xlabel=\"Date\", ylabel=\"Signal Intensity\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGenerating inflation\nNext, we generate the data for Inflation. We assume the inflation follows a power-law trend, meaning that growth accelerates over time rather than remaining constant. This can be mathematically defined as:\n\\[\nIN_{t} = (t + \\text{baseline})^{\\text{exponent}} - 1\n\\]\nWhere: - \\(t\\): The time index, representing days since the start of the date range. - \\(baseline\\): A constant added to \\(t\\) to shift the starting point of the trend. This value affects the initial level of market growth. The starting value of the function will be \\((baseline)^{exponent} - 1\\), not 0. - \\(exponent\\): The power to which the time index is raised, determining the rate at which the trend accelerates over time.\n\n\nCode\ndf[\"inflation\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (2 / 4) - 1\n\nfig, ax = plt.subplots()\nsns.lineplot(\n    x=\"date_week\", y=\"inflation\", color=\"C2\", label=\"trend\", data=df, ax=ax\n)\nax.legend(loc=\"upper left\")\nax.set(title=\"Inflation Components\", xlabel=\"date\", ylabel=None);\n\n\n\n\n\n\n\n\n\n\n\nModeling Marketing Channels\nIn this section, we simulate three marketing channels, \\(x1\\), \\(x2\\), and \\(x3\\) which represent different advertising channels (e.g., Internal Marketing, Social Marketing, Offline Marketing). Each channel’s behavior is influenced by random variability and confounding effects from seasonal holidays. Here’s how we model each channel mathematically:\nChannel \\(x1\\): As mentioned before, we generate \\(x1\\) which is affected by the holiday signal, we could define it as:\n\\[\nI_{x1_t} = S_{x1_t} + e_{x1}\n\\]\nChannel \\(x2\\): On the other hand, we generate \\(x2\\) which is affected by the holiday signal, and the influence of \\(x1\\). We could define it as:\n\\[\nI_{x2_t} = S_{x2_t} + H_{t} \\times \\alpha_{x2} + (I_{x1_t} \\times \\alpha_{x1_x2}) + e_{x2}\n\\]\nChannel \\(x3\\): For the last variable, we generate \\(x3\\) which is affected by \\(x1\\) only.\n\\[\nI_{x3_t} = S_{x3_t} + (I_{x1_t} \\times \\alpha_{x1_x3}) + e_{x3}\n\\]\nThese equations allow us to capture the complex dynamics influencing each marketing channel: - Holiday Effects increase channel activity around specific dates, simulating seasonal spikes. - Cross-channel Influences introduce interdependencies, modeling how one channel’s success can amplify another’s.\n\nNote: Here we are assuming an additive impact for the channel interactions.\n\n\n\nCode\nx1 = pz.Gamma(mu=1, sigma=3).rvs(n, random_state=rng)\ncofounder_effect_holiday_x1 = 2.5\nx1_conv = np.convolve(x1, np.ones(14) / 14, mode=\"same\")\nnoise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\nx1_conv[:14] = x1_conv.mean() + noise[:14]\nx1_conv[-14:] = x1_conv.mean() + noise[14:]\ndf[\"x1\"] = x1_conv\n\nx2 = pz.Gamma(mu=2, sigma=2).rvs(n, random_state=rng)\ncofounder_effect_holiday_x2 = 2.2\ncofounder_effect_x1_x2 = 1.3\nx2_conv = np.convolve(x2, np.ones(18) / 12, mode=\"same\")\nnoise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\nx2_conv[:14] = x2_conv.mean() + noise[:14]\nx2_conv[-14:] = x2_conv.mean() + noise[14:]\ndf[\"x2\"] = (\n    x2_conv\n    + (holiday_signal * cofounder_effect_holiday_x2)\n    + (df[\"x1\"] * cofounder_effect_x1_x2)\n) # digital ads\n\nx3 = pz.Gamma(mu=5, sigma=1).rvs(n, random_state=rng)\ncofounder_effect_x1_x3 = 1.5\nx3_conv = np.convolve(x3, np.ones(16) / 10, mode=\"same\")\nnoise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\nx3_conv[:14] = x3_conv.mean() + noise[:14]\nx3_conv[-14:] = x3_conv.mean() + noise[14:]\ndf[\"x3\"] = (\n    x3_conv\n    + (df[\"x1\"] * cofounder_effect_x1_x3)\n) # internal marketing\n\n\nWe’ll assume all of marketing activities suffer the same transformations Adstock and Saturation. This means, each channel will have individual parameters for the selected transformations, in this case Geometrick adstock and michaelis menten.\n\n\nCode\n# apply geometric adstock transformation\nalpha2: float = 0.4\nalpha3: float = 0.3\n\ndf[\"x2_adstock\"] = (\n    geometric_adstock(x=df[\"x2\"].to_numpy(), alpha=alpha2, l_max=24, normalize=True)\n    .eval()\n    .flatten()\n)\n\ndf[\"x3_adstock\"] = (\n    geometric_adstock(x=df[\"x3\"].to_numpy(), alpha=alpha3, l_max=24, normalize=True)\n    .eval()\n    .flatten()\n)\n\n\n# apply saturation transformation\nlam2: float = 6.0\nlam3: float = 12.0\n\nalpha_mm2: float = 12\nalpha_mm3: float = 18\n\ndf[\"x2_adstock_saturated\"] = michaelis_menten(\n    x=df[\"x2_adstock\"].to_numpy(), lam=lam2, alpha=alpha_mm2\n)\n\ndf[\"x3_adstock_saturated\"] = michaelis_menten(\n    x=df[\"x3_adstock\"].to_numpy(), lam=lam3, alpha=alpha_mm3\n)\n\nfig, ax = plt.subplots(\n    nrows=3, ncols=2, sharex=True, sharey=False, layout=\"constrained\"\n)\nsns.lineplot(x=\"date_week\", y=\"x2\", data=df, color=\"C1\", ax=ax[0, 0])\nsns.lineplot(x=\"date_week\", y=\"x3\", data=df, color=\"C2\", ax=ax[0, 1])\n\nsns.lineplot(x=\"date_week\", y=\"x2_adstock\", data=df, color=\"C1\", ax=ax[1, 0])\nsns.lineplot(x=\"date_week\", y=\"x3_adstock\", data=df, color=\"C2\", ax=ax[1, 1])\n\nsns.lineplot(x=\"date_week\", y=\"x2_adstock_saturated\", data=df, color=\"C1\", ax=ax[2, 0])\nsns.lineplot(x=\"date_week\", y=\"x3_adstock_saturated\", data=df, color=\"C2\", ax=ax[2, 1])\n\nfig.suptitle(\"Media Costs Data - Transformed\", fontsize=16)\n# adjust size of X axis\nax[2, 0].tick_params(axis=\"x\", labelsize=8)\nax[2, 1].tick_params(axis=\"x\", labelsize=8)\n\n# adjust size of x axis labels\nfor ax in ax.flat:\n    ax.tick_params(axis=\"x\", labelsize=6)\n\n\n\n\n\n\n\n\n\nThe previous plot shows how the transformations affect each variable, and what would be the true contribution after each transformation.\n\n\nTarget variable\nThe target variable is a combination of all variables before. The mathematical formula can be expressed as:\n\\[\ny_{t} = Intercept - f(IN_{t}) + f(H_{t}) + m(I_{x3_t}) + m(I_{x2_t}) + \\epsilon\n\\]\nWhere: - Intercept: A baseline level of sales, set to 1.5, representing the base sales level in the absence of other effects. - Inflation: Represents the underlying market inflation, with an implicit negative coefficient of 1, adding a steady downward influence. - Holiday Contributions: Adds sales spikes around holiday periods, capturing the seasonal increase in consumer demand. - \\(m(Impressions_{x3_t})\\) and \\(m(Impressions_{x2_t})\\): Represent the saturated adstock values for the marketing channels \\(x3\\) and \\(x2\\). - Noise \\(\\epsilon\\): A small random error term, drawn from a normal distribution with mean 0 and standard deviation 0.08, to account for unexplained variability in sales.\n\n\nCode\ndf[\"intercept\"] = 1.5\ndf[\"epsilon\"] = rng.normal(loc=0.0, scale=0.08, size=n)\n\ndf[\"y\"] = (\n    df[\"intercept\"]\n    + df[\"holiday_contributions\"]\n    + df[\"x2_adstock_saturated\"]\n    + df[\"x3_adstock_saturated\"]\n    + df[\"epsilon\"]  # Noise\n) - df[\"inflation\"]\n\nfig, ax = plt.subplots()\nsns.lineplot(x=\"date_week\", y=\"y\", color=\"black\", data=df, ax=ax)\nax.set(title=\"Sales (Target Variable)\", xlabel=\"date\", ylabel=\"y (thousands)\");\n\n\n\n\n\n\n\n\n\nWe can scale the full dataset and we’ll have finally something very similar to reality.\n\n\nCode\n# scale df by abs max per column\ndf[\"date\"] = pd.to_datetime(df[\"date_week\"])\nscaled_df = df.copy()\nfor col in scaled_df.columns:\n    if col != 'date' and col != 'date_week':\n        scaled_df[col] = scaled_df[col] / scaled_df[col].abs().max()\n\nscaled_df[[\"date\", \"x1\", \"x2\", \"x3\", \"y\"]].head()\n\n\n\n\n\n\n\n\n\ndate\nx1\nx2\nx3\ny\n\n\n\n\n0\n2022-01-01\n0.311103\n0.416608\n0.709209\n0.523628\n\n\n1\n2022-01-02\n0.346200\n0.417910\n0.718519\n0.641921\n\n\n2\n2022-01-03\n0.310798\n0.404066\n0.690251\n0.664626\n\n\n3\n2022-01-04\n0.301643\n0.395861\n0.693495\n0.672532\n\n\n4\n2022-01-05\n0.248688\n0.379114\n0.679055\n0.668596"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marketing Science Blog",
    "section": "",
    "text": "Discovering ways to create impact with science\n\nContact Me Book 30 Minutes"
  },
  {
    "objectID": "index.html#marketing-science-enthusiast",
    "href": "index.html#marketing-science-enthusiast",
    "title": "Marketing Science Blog",
    "section": "",
    "text": "Discovering ways to create impact with science\n\nContact Me Book 30 Minutes"
  },
  {
    "objectID": "index.html#recent-articles",
    "href": "index.html#recent-articles",
    "title": "Marketing Science Blog",
    "section": "Recent Articles",
    "text": "Recent Articles\n\n\nBaby Steps for Causal Discovery\nLearn how to use bayesian regression models to identify the causal structure of a dataset.\n\n\nNo More Experiments Without Causality\nLearn why you should not blindly trust on experiments to calibrate your media mix model."
  },
  {
    "objectID": "index.html#upcoming-talks",
    "href": "index.html#upcoming-talks",
    "title": "Marketing Science Blog",
    "section": "Upcoming Talks",
    "text": "Upcoming Talks\nOSDC: Bayesian causal thinking can revolutionize your marketing - Virtual Community Event - 2025\nCheck out my Talks page for more videos and presentations."
  },
  {
    "objectID": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#causal-discovery-algorithms",
    "href": "articles/baby_steps_for_causal_discovery/baby_steps_for_causal_discovery.html#causal-discovery-algorithms",
    "title": "Baby Steps for Causal Discovery",
    "section": "Causal Discovery Algorithms",
    "text": "Causal Discovery Algorithms\nThe Peter-Clark algorithm is a constraint-based method that infers causal structures from observational data using conditional independence tests. It starts with a fully connected undirected graph where every variable is initially connected to every other variable. The algorithm systematically tests conditional independence between pairs of variables, conditioning on increasingly larger subsets of other variables. When a conditional independence is detected, the corresponding edge is removed from the graph.\nOn the other hand, Greedy Search is a score-based method that iteratively improves a candidate causal model by locally modifying its structure. It begins with an initial directed acyclic graph and evaluates a scoring metric that balances goodness-of-fit with model complexity. The algorithm explores modifications such as adding, deleting, or reversing edges to find local improvements in the score. At each iteration, it selects the change that produces the highest increase in the score, following a step-by-step improvement strategy. The search continues until no single modification can further enhance the model’s score. This method efficiently navigates the combinatorial search space of possible graphs by making locally optimal choices."
  }
]