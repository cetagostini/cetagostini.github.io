{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Baby Steps for Causal Discovery\"\n",
        "date: \"2025-02-01\"\n",
        "categories: [python, experimentation, media mix modeling, mmm, bayesian, pymc, causal discovery, causal learning, pydata, tallinn, estonia]\n",
        "image: \"../images/baby_steps_for_causal_discovery.png\"\n",
        "jupyter: cetagostini_web\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-tools: true\n",
        "    code-overflow: wrap\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Introduction to Causal Discovery\n",
        "\n",
        "In this notebook, we'll dive into how to uncover causal relationships in marketing data, a crucial step for understanding the true impact of various channels on business outcomes. We'll start by generating synthetic data that mimics real-world marketing scenarios, complete with confounding variables and complex causal structures.\n",
        "\n",
        "Next, we'll fit a Bayesian marketing mix model using PyMC-Marketing, check causal directions between variables, and perform mediation analysis to explore indirect effects. Finally, we'll use structure discovery techniques to infer potential causal graphs. By the end, you'll have a solid grasp of how to apply these techniques to reveal hidden causal insights in your marketing data.\n"
      ],
      "id": "977d230a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from pymc_marketing.mmm.transformers import geometric_adstock, michaelis_menten\n",
        "\n",
        "from pymc_marketing.mmm import MMM, GeometricAdstock, MichaelisMentenSaturation\n",
        "from pymc_marketing.prior import Prior\n",
        "\n",
        "import networkx as nx\n",
        "from graphviz import Digraph\n",
        "import pydot\n",
        "\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import preliz as pz\n",
        "import pymc as pm\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "from causallearn.graph.Endpoint import Endpoint\n",
        "from causallearn.utils.GraphUtils import GraphUtils\n",
        "from causallearn.search.ScoreBased.GES import ges\n",
        "from causallearn.search.ConstraintBased.PC import pc\n",
        "\n",
        "az.style.use(\"arviz-darkgrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = [8, 4]\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "plt.rcParams[\"axes.labelsize\"] = 6\n",
        "plt.rcParams[\"xtick.labelsize\"] = 6\n",
        "plt.rcParams[\"ytick.labelsize\"] = 6\n",
        "plt.rcParams.update({\"figure.constrained_layout.use\": True})\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%config InlineBackend.figure_format = \"retina\"\n",
        "\n",
        "seed = sum(map(ord, \"Estimating effects despite having Confounding Variables\"))\n",
        "rng = np.random.default_rng(seed)\n",
        "\n",
        "print(seed)\n",
        "print(rng)"
      ],
      "id": "f6ef609f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Business case\n",
        "\n",
        "As a company we probably invest in different channels to acquire new customers. Some actions are more direct, like paid ads, and others are more indirect, like offline media marketing, as marketers and scientist, we want to understand the impact of each channel on the target variable (number of new customers).\n",
        "\n",
        "\n",
        "The following DAG shows a possible causal structure of the problem. Let's say we have the following variables:\n",
        "\n",
        "- $x1$: offline ads, e.g. TV, radio, print, etc.\n",
        "- $x2$: digital ads, e.g. SEM, SEO, social media, etc.\n",
        "- $x3$: internal marketing, e.g. product marketing, internal communication, etc.\n",
        "- $y$: new users\n",
        "\n",
        "Its probable that our offline ads are not directly affecting our new users, but they are affecting our digital ads and internal marketing, users observe a banner of our product and then they search for it online or they are referred by a friend which finally converts using their phone. Independently, some digital ads can impact users which haven't heard of us before, and those could convert directly.\n",
        "\n",
        "On top of that, we have some external factors that could affect our new users, e.g. holidays, economic conditions, etc. Things like holidays could affect even more our digital ads, e.g. more users are online shopping, and we are investing more during those days as well.\n",
        "\n",
        "This creates a complex causal structure, where variables are not fully independent and it is not easy to infer the causal impact of each channel on the target variable.\n"
      ],
      "id": "e58bff68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "new_real_dag = Digraph(comment='DAG')\n",
        "\n",
        "new_real_dag.node('z', 'holiday', color='grey', style='dashed')\n",
        "new_real_dag.node('m', 'inflation', color='grey', style='dashed')\n",
        "new_real_dag.node('x1', 'offline ads')\n",
        "new_real_dag.node('x2', 'digital ads')\n",
        "new_real_dag.node('x3', 'internal marketing')\n",
        "new_real_dag.node('y', 'new users')\n",
        "\n",
        "new_real_dag.edge('z', 'x2', style='dashed')\n",
        "\n",
        "new_real_dag.edge('x1', 'x2')\n",
        "new_real_dag.edge('x1', 'x3')\n",
        "\n",
        "new_real_dag.edge('z', 'y', style='dashed')\n",
        "new_real_dag.edge('x2', 'y')\n",
        "new_real_dag.edge('x3', 'y')\n",
        "new_real_dag.edge('m', 'y', style='dashed')\n",
        "\n",
        "new_real_dag"
      ],
      "id": "e26c2bc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data generation\n",
        "\n",
        "Based on the provided DAG, we can create some synthetic data to test how our model performs when we have a complex causal structures. Using the same data, we can test different model compositions and see how could we improve our model to uncover the true causal impact of each channel on the target variable.\n",
        "\n",
        "We'll start by setting the date range. Here we'll use a date range from 2022-01-01 to 2024-11-06, meaning we have almost 3 years of data (1041 days).\n"
      ],
      "id": "3dab5349"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# date range\n",
        "min_date = pd.to_datetime(\"2022-01-01\")\n",
        "max_date = pd.to_datetime(\"2024-11-06\")\n",
        "date_range = pd.date_range(start=min_date, end=max_date, freq=\"D\")\n",
        "\n",
        "df = pd.DataFrame(data={\"date_week\": date_range}).assign(\n",
        "    year=lambda x: x[\"date_week\"].dt.year,\n",
        "    month=lambda x: x[\"date_week\"].dt.month,\n",
        "    dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n",
        ")\n",
        "\n",
        "n = df.shape[0]\n",
        "print(f\"Number of observations: {n}\")"
      ],
      "id": "dafb86a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Holiday signal\n",
        "\n",
        "Certain holidays, like Christmas, can have a significant impact on consumer behavior before and after the specific date, leading to seasonal spikes in sales. To capture these effects, we introduce a holiday signal based on Gaussian (normal) distributions centered around specific holiday dates.\n",
        "\n",
        "The function used to model the holiday effect is defined as follows:\n",
        "\n",
        "$$\n",
        "H_{t} = \\exp\\left(-0.5 \\left(\\frac{\\Delta t}{\\sigma}\\right)^2\\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\Delta t$ is the time difference (in days) between the current date and the holiday date.\n",
        "- $\\sigma$ is the standard deviation that controls the spread of the effect around the holiday date.\n",
        "\n",
        "For each holiday, we calculate the holiday signal across the date range and add a **holiday contribution** by scaling the signal with a holiday-specific coefficient. This approach models seasonal holiday spikes using Gaussian functions, which capture the transient increase in market activity around holidays, and their respective decay over time.\n",
        "\n",
        "> Note: Here we assume a normally distributed signal, nevertheless the signal could be skew or not normal distributed.\n"
      ],
      "id": "39d26245"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "holiday_dates = [\"24-12\", \"31-12\", \"08-06\", \"07-09\"]  # List of holidays as month-day strings\n",
        "std_devs = [5, 5, 3, 3]  # List of standard deviations for each holiday\n",
        "holidays_coefficients = [2, 3, 4, 6]\n",
        "\n",
        "# Initialize the holiday effect array\n",
        "holiday_signal = np.zeros(len(date_range))\n",
        "holiday_contributions = np.zeros(len(date_range))\n",
        "\n",
        "# Generate holiday signals\n",
        "for holiday, std_dev, holiday_coef in zip(\n",
        "    holiday_dates, std_devs, holidays_coefficients, strict=False\n",
        "):\n",
        "    # Find all occurrences of the holiday in the date range\n",
        "    holiday_occurrences = date_range[date_range.strftime(\"%d-%m\") == holiday]\n",
        "\n",
        "    for occurrence in holiday_occurrences:\n",
        "        # Calculate the time difference in days\n",
        "        time_diff = (date_range - occurrence).days\n",
        "\n",
        "        # Generate the Gaussian basis for the holiday\n",
        "        _holiday_signal = np.exp(-0.5 * (time_diff / std_dev) ** 2)\n",
        "\n",
        "        # Add the holiday signal to the holiday effect\n",
        "        holiday_signal += _holiday_signal\n",
        "\n",
        "        holiday_contributions += _holiday_signal * holiday_coef\n",
        "\n",
        "df[\"holiday_signal\"] = holiday_signal\n",
        "df[\"holiday_contributions\"] = holiday_contributions\n",
        "\n",
        "# Plot the holiday effect\n",
        "fig, ax = plt.subplots()\n",
        "sns.lineplot(x=date_range, y=holiday_signal, ax=ax)\n",
        "ax.set(title=\"Holiday Effect Signal\", xlabel=\"Date\", ylabel=\"Signal Intensity\")\n",
        "plt.show()"
      ],
      "id": "b18eaeba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating inflation\n",
        "Next, we generate the data for **Inflation**. We assume the inflation follows a power-law trend, meaning that growth accelerates over time rather than remaining constant. This can be mathematically defined as:\n",
        "\n",
        "$$\n",
        "IN_{t} = (t + \\text{baseline})^{\\text{exponent}} - 1\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $t$: The time index, representing days since the start of the date range.\n",
        "- $baseline$: A constant added to $t$ to shift the starting point of the trend. This value affects the initial level of market growth. The starting value of the function will be $(baseline)^{exponent} - 1$, not 0.\n",
        "- $exponent$: The power to which the time index is raised, determining the rate at which the trend accelerates over time.\n"
      ],
      "id": "e56d618d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"inflation\"] = (np.linspace(start=0.0, stop=50, num=n) + 10) ** (2 / 4) - 1\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.lineplot(\n",
        "    x=\"date_week\", y=\"inflation\", color=\"C2\", label=\"trend\", data=df, ax=ax\n",
        ")\n",
        "ax.legend(loc=\"upper left\")\n",
        "ax.set(title=\"Inflation Components\", xlabel=\"date\", ylabel=None);"
      ],
      "id": "7df4662d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modeling Marketing Channels\n",
        "\n",
        "In this section, we simulate three marketing channels, $x1$, $x2$, and $x3$ which represent different advertising channels (e.g., Internal Marketing, Social Marketing, Offline Marketing). Each channel's behavior is influenced by random variability and confounding effects from seasonal holidays. Here’s how we model each channel mathematically:\n",
        "\n",
        "**Channel $x1$**:\n",
        "As mentioned before, we generate $x1$ which is affected by the holiday signal, we could define it as:\n",
        "\n",
        "$$\n",
        "I_{x1_t} = S_{x1_t} + e_{x1}\n",
        "$$\n",
        "\n",
        "**Channel $x2$**:\n",
        "On the other hand, we generate $x2$ which is affected by the holiday signal, and the influence of $x1$. We could define it as:\n",
        "\n",
        "$$\n",
        "I_{x2_t} = S_{x2_t} + H_{t} \\times \\alpha_{x2} + (I_{x1_t} \\times \\alpha_{x1_x2}) + e_{x2}\n",
        "$$\n",
        "\n",
        "**Channel $x3$**:\n",
        "For the last variable, we generate $x3$ which is affected by $x1$ only.\n",
        "\n",
        "$$\n",
        "I_{x3_t} = S_{x3_t} + (I_{x1_t} \\times \\alpha_{x1_x3}) + e_{x3}\n",
        "$$\n",
        "\n",
        "These equations allow us to capture the complex dynamics influencing each marketing channel:\n",
        "- **Holiday Effects** increase channel activity around specific dates, simulating seasonal spikes.\n",
        "- **Cross-channel Influences** introduce interdependencies, modeling how one channel's success can amplify another’s.\n",
        "\n",
        "> Note: Here we are assuming an additive impact for the channel interactions.\n"
      ],
      "id": "62460284"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x1 = pz.Gamma(mu=1, sigma=3).rvs(n, random_state=rng)\n",
        "cofounder_effect_holiday_x1 = 2.5\n",
        "x1_conv = np.convolve(x1, np.ones(14) / 14, mode=\"same\")\n",
        "noise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\n",
        "x1_conv[:14] = x1_conv.mean() + noise[:14]\n",
        "x1_conv[-14:] = x1_conv.mean() + noise[14:]\n",
        "df[\"x1\"] = x1_conv\n",
        "\n",
        "x2 = pz.Gamma(mu=2, sigma=2).rvs(n, random_state=rng)\n",
        "cofounder_effect_holiday_x2 = 2.2\n",
        "cofounder_effect_x1_x2 = 1.3\n",
        "x2_conv = np.convolve(x2, np.ones(18) / 12, mode=\"same\")\n",
        "noise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\n",
        "x2_conv[:14] = x2_conv.mean() + noise[:14]\n",
        "x2_conv[-14:] = x2_conv.mean() + noise[14:]\n",
        "df[\"x2\"] = (\n",
        "    x2_conv\n",
        "    + (holiday_signal * cofounder_effect_holiday_x2)\n",
        "    + (df[\"x1\"] * cofounder_effect_x1_x2)\n",
        ") # digital ads\n",
        "\n",
        "x3 = pz.Gamma(mu=5, sigma=1).rvs(n, random_state=rng)\n",
        "cofounder_effect_x1_x3 = 1.5\n",
        "x3_conv = np.convolve(x3, np.ones(16) / 10, mode=\"same\")\n",
        "noise = pz.Normal(mu=0, sigma=0.1).rvs(28, random_state=rng)\n",
        "x3_conv[:14] = x3_conv.mean() + noise[:14]\n",
        "x3_conv[-14:] = x3_conv.mean() + noise[14:]\n",
        "df[\"x3\"] = (\n",
        "    x3_conv\n",
        "    + (df[\"x1\"] * cofounder_effect_x1_x3)\n",
        ") # internal marketing"
      ],
      "id": "f04dc842",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll assume all of marketing activities suffer the same transformations Adstock and Saturation. This means, each channel will have individual parameters for the selected transformations, in this case Geometrick adstock and michaelis menten.\n"
      ],
      "id": "6de9ae1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# apply geometric adstock transformation\n",
        "alpha2: float = 0.4\n",
        "alpha3: float = 0.3\n",
        "\n",
        "df[\"x2_adstock\"] = (\n",
        "    geometric_adstock(x=df[\"x2\"].to_numpy(), alpha=alpha2, l_max=24, normalize=True)\n",
        "    .eval()\n",
        "    .flatten()\n",
        ")\n",
        "\n",
        "df[\"x3_adstock\"] = (\n",
        "    geometric_adstock(x=df[\"x3\"].to_numpy(), alpha=alpha3, l_max=24, normalize=True)\n",
        "    .eval()\n",
        "    .flatten()\n",
        ")\n",
        "\n",
        "\n",
        "# apply saturation transformation\n",
        "lam2: float = 6.0\n",
        "lam3: float = 12.0\n",
        "\n",
        "alpha_mm2: float = 12\n",
        "alpha_mm3: float = 18\n",
        "\n",
        "df[\"x2_adstock_saturated\"] = michaelis_menten(\n",
        "    x=df[\"x2_adstock\"].to_numpy(), lam=lam2, alpha=alpha_mm2\n",
        ")\n",
        "\n",
        "df[\"x3_adstock_saturated\"] = michaelis_menten(\n",
        "    x=df[\"x3_adstock\"].to_numpy(), lam=lam3, alpha=alpha_mm3\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(\n",
        "    nrows=3, ncols=2, sharex=True, sharey=False, layout=\"constrained\"\n",
        ")\n",
        "sns.lineplot(x=\"date_week\", y=\"x2\", data=df, color=\"C1\", ax=ax[0, 0])\n",
        "sns.lineplot(x=\"date_week\", y=\"x3\", data=df, color=\"C2\", ax=ax[0, 1])\n",
        "\n",
        "sns.lineplot(x=\"date_week\", y=\"x2_adstock\", data=df, color=\"C1\", ax=ax[1, 0])\n",
        "sns.lineplot(x=\"date_week\", y=\"x3_adstock\", data=df, color=\"C2\", ax=ax[1, 1])\n",
        "\n",
        "sns.lineplot(x=\"date_week\", y=\"x2_adstock_saturated\", data=df, color=\"C1\", ax=ax[2, 0])\n",
        "sns.lineplot(x=\"date_week\", y=\"x3_adstock_saturated\", data=df, color=\"C2\", ax=ax[2, 1])\n",
        "\n",
        "fig.suptitle(\"Media Costs Data - Transformed\", fontsize=16)\n",
        "# adjust size of X axis\n",
        "ax[2, 0].tick_params(axis=\"x\", labelsize=8)\n",
        "ax[2, 1].tick_params(axis=\"x\", labelsize=8)\n",
        "\n",
        "# adjust size of x axis labels\n",
        "for ax in ax.flat:\n",
        "    ax.tick_params(axis=\"x\", labelsize=6)"
      ],
      "id": "06302c16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous plot shows how the transformations affect each variable, and what would be the true contribution after each transformation.\n",
        "\n",
        "### Target variable\n",
        "The target variable is a combination of all variables before. The mathematical formula can be expressed as:\n",
        "\n",
        "$$\n",
        "y_{t} = Intercept - f(IN_{t}) + f(H_{t}) + m(I_{x3_t}) + m(I_{x2_t}) + \\epsilon\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- **Intercept**: A baseline level of sales, set to 1.5, representing the base sales level in the absence of other effects.\n",
        "- **Inflation**: Represents the underlying market inflation, with an implicit negative coefficient of 1, adding a steady downward influence.\n",
        "- **Holiday Contributions**: Adds sales spikes around holiday periods, capturing the seasonal increase in consumer demand.\n",
        "- **$m(Impressions_{x3_t})$ and $m(Impressions_{x2_t})$**: Represent the **saturated adstock** values for the marketing channels $x3$ and $x2$.\n",
        "- **Noise $\\epsilon$**: A small random error term, drawn from a normal distribution with mean 0 and standard deviation 0.08, to account for unexplained variability in sales.\n"
      ],
      "id": "bf7d9db5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"intercept\"] = 1.5\n",
        "df[\"epsilon\"] = rng.normal(loc=0.0, scale=0.08, size=n)\n",
        "\n",
        "df[\"y\"] = (\n",
        "    df[\"intercept\"]\n",
        "    + df[\"holiday_contributions\"]\n",
        "    + df[\"x2_adstock_saturated\"]\n",
        "    + df[\"x3_adstock_saturated\"]\n",
        "    + df[\"epsilon\"]  # Noise\n",
        ") - df[\"inflation\"]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.lineplot(x=\"date_week\", y=\"y\", color=\"black\", data=df, ax=ax)\n",
        "ax.set(title=\"Sales (Target Variable)\", xlabel=\"date\", ylabel=\"y (thousands)\");"
      ],
      "id": "1de4e6b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can scale the full dataset and we'll have finally something very similar to reality.\n"
      ],
      "id": "25929612"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# scale df by abs max per column\n",
        "df[\"date\"] = pd.to_datetime(df[\"date_week\"])\n",
        "scaled_df = df.copy()\n",
        "for col in scaled_df.columns:\n",
        "    if col != 'date' and col != 'date_week':\n",
        "        scaled_df[col] = scaled_df[col] / scaled_df[col].abs().max()\n",
        "\n",
        "scaled_df[[\"date\", \"x1\", \"x2\", \"x3\", \"y\"]].head()"
      ],
      "id": "0f731264",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intial approach\n",
        "\n",
        "If we have a dataset like the one we just created, we can try to fit a model with the following to find the causal impact of each channel on the target variable. For the example, we'll use a simple model from Pymc-Marketing for this purpose.\n",
        "\n",
        "Let's see what happens if we fit a model with all what we have without any knowledge of the causal structure.\n"
      ],
      "id": "ed9285ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scaled_df[[\"date\", \"x1\", \"x2\", \"x3\", \"y\"]].head()\n",
        "\n",
        "model_config = {\n",
        "    \"intercept\": Prior(\"Gamma\", mu=1, sigma=1),\n",
        "    \"likelihood\": Prior(\"Normal\", sigma=Prior(\"Normal\", mu=0, sigma=.5)),\n",
        "}\n",
        "\n",
        "fit_kwargs = dict(nuts_sampler=\"numpyro\", random_seed=rng,)\n",
        "\n",
        "X = df.drop(columns=[\"y\"])\n",
        "y = df[\"y\"]\n",
        "\n",
        "mmm = MMM(\n",
        "    date_column=\"date\",\n",
        "    channel_columns=[\n",
        "        \"x1\",\n",
        "        \"x2\",\n",
        "        \"x3\"\n",
        "    ],\n",
        "    control_columns=[\n",
        "        \"holiday_signal\",\n",
        "        \"inflation\"\n",
        "    ],\n",
        "    adstock=GeometricAdstock(l_max=24),\n",
        "    saturation=MichaelisMentenSaturation(),\n",
        ")\n",
        "mmm.fit(X, y, **fit_kwargs)\n",
        "mmm.sample_posterior_predictive(\n",
        "    X=X,\n",
        "    extend_idata=True,\n",
        "    combined=True,\n",
        "    random_seed=rng,\n",
        ")"
      ],
      "id": "5d9fc271",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How the recover contributions look like, If we compare to the real contributions?\n"
      ],
      "id": "70f960df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "initial_model_recover_effect = (\n",
        "    az.hdi(mmm.fit_result[\"channel_contributions\"], hdi_prob=0.95)\n",
        "    * mmm.target_transformer[\"scaler\"].scale_.item()\n",
        ")\n",
        "initial_model_mean_effect = (\n",
        "    mmm.fit_result.channel_contributions.mean(dim=[\"chain\", \"draw\"])\n",
        "    * mmm.target_transformer[\"scaler\"].scale_.item()\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, sharex=True)\n",
        "# x2 -> online\n",
        "ax[0].plot(\n",
        "    date_range,\n",
        "    initial_model_mean_effect.sel(channel=\"x2\"),\n",
        "    label=\"Mean Recover x2 Effect\",\n",
        "    linestyle=\"--\",\n",
        "    color=\"orange\",\n",
        ")\n",
        "ax[0].fill_between(\n",
        "    date_range,\n",
        "    initial_model_recover_effect.channel_contributions.isel(hdi=0).sel(channel=\"x2\"),\n",
        "    initial_model_recover_effect.channel_contributions.isel(hdi=1).sel(channel=\"x2\"),\n",
        "    alpha=0.2,\n",
        "    label=\"95% Credible Interval\",\n",
        "    color=\"orange\",\n",
        ")\n",
        "ax[0].plot(\n",
        "    date_range, df[\"x2_adstock_saturated\"], label=\"Real x2 Effect\", color=\"black\"\n",
        ")\n",
        "\n",
        "# x3 -> internal\n",
        "ax[1].plot(\n",
        "    date_range,\n",
        "    initial_model_mean_effect.sel(channel=\"x3\"),\n",
        "    label=\"Mean Recover x3 Effect\",\n",
        "    linestyle=\"--\",\n",
        "    color=\"green\",\n",
        ")\n",
        "ax[1].fill_between(\n",
        "    date_range,\n",
        "    initial_model_recover_effect.channel_contributions.isel(hdi=0).sel(channel=\"x3\"),\n",
        "    initial_model_recover_effect.channel_contributions.isel(hdi=1).sel(channel=\"x3\"),\n",
        "    alpha=0.2,\n",
        "    label=\"95% Credible Interval\",\n",
        "    color=\"green\",\n",
        ")\n",
        "ax[1].plot(\n",
        "    date_range, df[\"x3_adstock_saturated\"], label=\"Real x3 Effect\", color=\"black\"\n",
        ")\n",
        "\n",
        "# formatting\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "\n",
        "plt.grid(True)\n",
        "ax[1].set(xlabel=\"date\")\n",
        "fig.suptitle(\"Media Contribution Recovery\", fontsize=16)\n",
        "plt.show()"
      ],
      "id": "058a4fc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As anticipated, the model fails to accurately reflect the true contributions, resulting in estimates that significantly deviate from the actual values. How can this occur, and why is this happening?\n",
        "\n",
        "The explanation is straightforward: by neglecting any causal structure, we inadvertently impose one onto the data. The issue lies in our assumption of the simplest causal framework, which rarely aligns with real-world complexity.\n",
        "\n",
        "What kind of causal structure are we implicitly assuming when we fit the model?\n"
      ],
      "id": "79e3848a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize a directed graph\n",
        "naive_causal_mmm_graph = Digraph()\n",
        "\n",
        "# Add nodes\n",
        "naive_causal_mmm_graph.node(\"X1\", \"Offline\")\n",
        "naive_causal_mmm_graph.node(\"X2\", \"Online\")\n",
        "naive_causal_mmm_graph.node(\"X3\", \"Internal\")\n",
        "naive_causal_mmm_graph.node(\"E\", \"Exogenous variables\", style=\"dashed\")\n",
        "naive_causal_mmm_graph.node(\"T\", \"Target\")\n",
        "\n",
        "naive_causal_mmm_graph.edge(\"E\", \"T\", style=\"dashed\")\n",
        "naive_causal_mmm_graph.edge(\"X1\", \"T\")\n",
        "naive_causal_mmm_graph.edge(\"X2\", \"T\")\n",
        "naive_causal_mmm_graph.edge(\"X3\", \"T\")\n",
        "\n",
        "# Render the graph to SVG and display it inline\n",
        "svg_str = naive_causal_mmm_graph.pipe(format=\"svg\")\n",
        "display(SVG(svg_str))"
      ],
      "id": "c56572c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The DAG above represents the causal structure that we are implicitly assuming when we fit the model. Here all variables are independent of each other, and those impact directly the target variable.\n",
        "\n",
        "During the model development, we established a specific structure and flow for our data. We concluded that the impacts of our channels operate independently of each other. Moreover, we determined that if any component of our ecosystem is missing, its influence will be accounted for by the baseline term due to this equation. As you can see, even by adopting this basic model, we are making significant assumptions.\n",
        "\n",
        "On one hand, you are assuming that the impact is not linear by applying these transformations, and you are suggesting that the impact is positive and that there may be a maximum delay of a certain number of days.\n",
        "\n",
        "You've even defined the direction of your relationships. By defining these relationships and assuming no direct causal connections between our variables, we can conclude that, if the nature of their relationship is accurately represented by the provided equation, then by controlling the relevant channels, we could uncover their true effects.\n",
        "\n",
        "This leads us to which causal DAG we assume is correct, based on our previous assumptions. If you recognize this process, congratulations! You've created a generative model or Structural Causal Model, with a Structural Causal Equation, using PyMC-marketing.\n",
        "\n",
        "However, this Causal DAG does not depict the true Causal DAG. Since our PyMC model is structural and causal, we must ask: *What happens if I create a model with a different causal structure than the real one?*\n",
        "\n",
        "The answer is what we observed above, the model will not be able to recover the true causal structure.\n",
        "\n",
        "# Learning about generative models\n",
        "Generative models are frameworks that describe how data could be produced in the real world. They outline a process by defining probability distributions for each component, simulating the creation of data from underlying random variables. This approach captures uncertainty and variability, providing a complete picture of the data generation mechanism.\n",
        "\n",
        "In PyMC, this concept is at the core of every model. PyMC lets you explicitly define priors, likelihoods, and the structure of your data generation process. Even simple models built in PyMC carry an inherent generative assumption, making them flexible and robust in representing how data might naturally arise.\n",
        "\n",
        "This means that each possible graph with N number of variables can be a specific model. How many models could we specify if we have 5 variables for one target?\n"
      ],
      "id": "5e2ad690"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def dag_with_exactly_s_sources(n: int, s: int) -> int:\n",
        "    \"\"\"\n",
        "    Count DAGs with exactly s source nodes.\n",
        "\n",
        "    Uses formula: S(n,s) = C(n,s) * sum[(-1)^j * C(n-s,j) * 2^((m-j)(m-j-1)/2) *\n",
        "    (2^(m-j)-1)^s] where m=n-s and j=0..m. When n=s, S(n,n)=1.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n : int\n",
        "        Total number of labeled nodes\n",
        "    s : int\n",
        "        Number of source nodes\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Number of possible DAGs\n",
        "    \"\"\"\n",
        "    if n == s:\n",
        "        return 1\n",
        "    total = 0\n",
        "    m = n - s\n",
        "    for j in range(m + 1):\n",
        "        term = (\n",
        "            (-1) ** j\n",
        "            * math.comb(m, j)\n",
        "            * 2 ** (((m - j) * (m - j - 1)) // 2)\n",
        "            * (2 ** (m - j) - 1) ** s\n",
        "        )\n",
        "        total += term\n",
        "    return math.comb(n, s) * total\n",
        "\n",
        "def count_valid_final_graphs_with_parents(num_regressors: int, num_parents: int) -> int:\n",
        "    \"\"\"Count valid final graphs with parent node restrictions.\n",
        "\n",
        "    The counting process has two main steps:\n",
        "\n",
        "    1. Build a DAG among regressors where parents have no incoming edges:\n",
        "       - For non-parents (Q = num_regressors - num_parents), count DAGs with given sinks\n",
        "       - Parents can only have edges to non-parents (Q options each)\n",
        "       - Non-sink parents must have ≥1 outgoing edge (2^Q - 1 ways)\n",
        "       - Sink parents have no edges (1 way)\n",
        "\n",
        "    2. Add edges from regressors to target:\n",
        "       - Sink regressors must connect to target\n",
        "       - Non-sink regressors optionally connect\n",
        "       - Yields factor 2^(num_regressors - total_sinks)\n",
        "\n",
        "    The total count formula is:\n",
        "    sum_{s_p=0}^P sum_{s_q=1}^Q [binom(P,s_p) * (2^Q-1)^(P-s_p) *\n",
        "    (DAGs_Q_s_q) * 2^((P+Q)-(s_p+s_q))]\n",
        "\n",
        "    where P = num_parents, Q = num_regressors - num_parents\n",
        "\n",
        "    When Q = 0 (all regressors are parents), the final graph is unique.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_regressors : int\n",
        "        Total number of regressor nodes in the graph\n",
        "    num_parents : int\n",
        "        Number of designated parent nodes that cannot have incoming edges\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Total count of valid final graph configurations\n",
        "    \"\"\"\n",
        "    P = num_parents\n",
        "    Q = num_regressors - num_parents  # non-parents\n",
        "    if Q < 0:\n",
        "        raise ValueError(\"num_parents cannot exceed num_regressors\")\n",
        "\n",
        "    # Case where all regressors are parents: no DAG edges are allowed;\n",
        "    # every node is isolated (and hence a sink),\n",
        "    # so the regressor-to-target assignment is forced.\n",
        "    if Q == 0:\n",
        "        return 1\n",
        "\n",
        "    total = 0\n",
        "    # s_p: number of parents that end up as sinks\n",
        "    # (i.e. with no outgoing edge to any non-parent)\n",
        "    for s_p in range(P + 1):\n",
        "        # For each parent:\n",
        "        #   - If not a sink: choose at least one outgoing edge\n",
        "        #     among Q non-parents: (2^Q - 1) ways.\n",
        "        #   - If a sink: only 1 way (choose no outgoing edge).\n",
        "        parent_config = math.comb(P, s_p) * ((2**Q - 1) ** (P - s_p))\n",
        "        # s_q: number of sinks among non-parents in the DAG on Q nodes.\n",
        "        # Note: Every DAG on at least one node has at least one sink.\n",
        "        for s_q in range(1, Q + 1):\n",
        "            nonparent_count = dag_with_exactly_s_sources(Q, s_q)\n",
        "            # Total sinks in the regressor\n",
        "            # DAG is s_p (from parents) plus s_q (from non-parents)\n",
        "            total_sinks = s_p + s_q\n",
        "            # For each regressor that is not a sink,\n",
        "            # the regressor-to-target edge is optional.\n",
        "            # Thus, a factor of 2^( (P+Q) - total_sinks ).\n",
        "            assignment_factor = 2 ** ((P + Q) - total_sinks)\n",
        "            total += parent_config * nonparent_count * assignment_factor\n",
        "    return total\n",
        "\n",
        "possible_dags = count_valid_final_graphs_with_parents(num_regressors=5, num_parents=2)\n",
        "print(f\"Number of possible DAGs with two parents (Graphical/Generative Model): {possible_dags:,}\")\n",
        "\n",
        "possible_dags = count_valid_final_graphs_with_parents(num_regressors=5, num_parents=1)\n",
        "print(f\"Number of possible DAGs with one parent (Graphical/Generative Model): {possible_dags:,}\")"
      ],
      "id": "8ae6e5a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of possible models we can generate with two out of five variables as parents is around 12,000, while having just one parent increases that number to approximately 52,000. Interestingly, removing a single parent node triples the potential models we can create, effectively multiplying the number of possible scenarios.\n",
        "\n",
        "This sheds light on the challenges posed by large models:  \n",
        "a) As the number of variables increases, the exponential growth in potential relationships becomes overwhelming, making it difficult to pinpoint our actual situation.  \n",
        "b) With more variables, the likelihood of mistakenly controlling for the wrong variables also increases.\n",
        "\n",
        "This latter point aligns with our earlier observation: if we control for inappropriate variables, the model fails to recover the true causal structure.\n",
        "\n",
        "So, why is it problematic to control for certain variables? Every variable should add more explanatory power, no?. Let's start learning about structures to understand.\n",
        "\n",
        "# Learning about causal structures\n",
        "\n",
        "**Forks**: A fork is a causal structure where a single variable acts as a common cause for two or more other variables. This common cause transmits its influence to all its direct descendants. The existence of a fork creates confounding, making the relationship between the descendant variables appear related. Controlling for the common cause can effectively block the backdoor paths created by this structure.\n",
        "\n",
        "**Chains**: A chain represents a sequential causal pathway where one variable influences another, which in turn affects a third variable. This structure highlights the mediation process through which causal effects are transmitted. The intermediate variable acts as a mediator, carrying the influence from the initial cause to the final outcome. Analyzing chains helps distinguish between direct and indirect effects in a causal system. Controlling for the mediator inappropriately may block parts of the causal effect that are of interest.\n",
        "\n",
        "**Colliders**: A collider is a variable that is the common effect of two or more causal factors. It sits at the convergence of different causal paths and can introduce spurious associations when conditioned upon. Controlling for a collider can inadvertently open up non-causal, backdoor paths, leading to biased estimates. This phenomenon, known as collider bias, distorts the true relationships among the causal variables. Avoiding conditioning on colliders is crucial for maintaining the validity of causal models.\n"
      ],
      "id": "bf813ec8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create figure with 3 subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,)\n",
        "\n",
        "# Create collider DAG\n",
        "collider = Digraph()\n",
        "collider.attr(rankdir='TB')\n",
        "collider.node('A', 'A')\n",
        "collider.node('B', 'B')\n",
        "collider.node('C', 'C')\n",
        "collider.edge('A', 'C')\n",
        "collider.edge('B', 'C')\n",
        "ax1.set_title('Collider (A→C←B)')\n",
        "ax1.imshow(Image.open(BytesIO(collider.pipe(format='png'))))\n",
        "ax1.axis('off')\n",
        "\n",
        "# Create fork DAG\n",
        "fork = Digraph()\n",
        "fork.attr(rankdir='TB')\n",
        "fork.node('A', 'A')\n",
        "fork.node('B', 'B')\n",
        "fork.node('C', 'C')\n",
        "fork.edge('A', 'B')\n",
        "fork.edge('A', 'C')\n",
        "ax2.set_title('Fork (B←A→C)')\n",
        "ax2.imshow(Image.open(BytesIO(fork.pipe(format='png'))))\n",
        "ax2.axis('off')\n",
        "\n",
        "# Create chain DAG\n",
        "chain = Digraph()\n",
        "chain.attr(rankdir='TB')\n",
        "chain.node('A', 'A')\n",
        "chain.node('B', 'B')\n",
        "chain.node('C', 'C')\n",
        "chain.edge('A', 'B')\n",
        "chain.edge('B', 'C')\n",
        "ax3.set_title('Chain (A→B→C)')\n",
        "ax3.imshow(Image.open(BytesIO(chain.pipe(format='png'))))\n",
        "ax3.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "281014ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Causal structures play a crucial role in various causal inference methods, serving as the foundation for their functionality. For instance, chain structures are key to methods like instrumental variables (IV). In IV analysis, this chain structure comes into play by introducing an instrument—a variable that impacts the exposure but does not directly affect the outcome, except through that exposure. This approach helps break the confounding path, allowing us to isolate the exogenous variation in treatment.\n",
        "\n",
        "As a result, it enables a consistent estimation of causal effects, even when we encounter endogeneity. Therefore, grasping chain structures is essential, as it not only supports the rationale behind IV methods but also underscores the significance of identifying valid instruments.\n",
        "\n",
        "If you're particularly interested in learning more about IVs, I recommend checking out a post by Anton Bugaev or heading to the fifth floor if you’re at Bolt.\n",
        "\n",
        "Ultimately, each of these causal structures exhibits different observational behaviors. This means that based on the observational data, we can deduce which structure is present, and consecuently determinate whats the right variable set to control.\n",
        "\n",
        "One thing to understand what to control is find out the parent nodes to avoid to control by mediators, we could identify this understanding the conditional dependencies.\n",
        "\n",
        "# Lets check conditional independencies\n",
        "Conditional independence is a core concept in probability theory and statistics where two variables are independent of each other once a third variable is held constant. This means that, given the value of the conditioning variable, the two variables do not provide additional information about one another. In causal discovery, conditional independencies are crucial because they reveal the underlying structure of causal relationships in a model or a directed acyclic graph (DAG). By identifying these independencies, we can determine how variables are related to each other, or not.\n",
        "\n",
        "Bayesian regression models allow us to estimate the conditional expectation of an outcome given a set of predictors, effectively uncovering the underlying conditional probabilities. In a bayesian linear regression, for example, we estimate $E(Y \\mid X) = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_kX_k$, which represents the average outcome $Y$ when the predictors $X_1, \\dots, X_k$ are held at specific values.\n",
        "\n",
        "Let's define a function to build and sample a linear model from a formula.\n"
      ],
      "id": "27247ebf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_and_sample_model(data: pd.DataFrame, formula: str):\n",
        "    \"\"\"\n",
        "    Build and sample a linear model from a formula.\n",
        "    \"\"\"\n",
        "    # Parse the formula to get target and channels\n",
        "    target, channels = formula.split('~')\n",
        "    target = target.strip()\n",
        "    channels = [ch.strip() for ch in channels.split('+') if ch.strip() != \"1\"]\n",
        "\n",
        "    # Define coordinates\n",
        "    coordinates = {\"date\": data.date.unique()}\n",
        "    if channels:  # If there are regressors, include them in coordinates\n",
        "        coordinates[\"channel\"] = channels\n",
        "\n",
        "    # Filter the dataset based on the formula\n",
        "    with pm.Model(coords=coordinates) as linear_model:\n",
        "        # Load Data in Model\n",
        "        target_data = pm.Data(\"target\", data[target].values, dims=\"date\")\n",
        "\n",
        "        # Constant or intercept\n",
        "        intercept = pm.Gamma(\"intercept\", mu=3, sigma=2)\n",
        "\n",
        "        mu_var = 0\n",
        "\n",
        "        if channels:  # If there are regressors, include them\n",
        "            regressors = pm.Data(\"regressors\", data[channels].values, dims=(\"date\", \"channel\"))\n",
        "            gamma = pm.Normal(\"gamma\", mu=3, sigma=2, dims=\"channel\")\n",
        "            mu_var += (regressors * gamma).sum(axis=-1) + intercept\n",
        "        else:\n",
        "            mu_var += intercept\n",
        "\n",
        "        # Likelihood\n",
        "        pm.Normal(\"likelihood\", mu=mu_var, sigma=pm.Gamma(\"sigma\", mu=2, sigma=3), observed=target_data, dims=\"date\")\n",
        "\n",
        "        # Sample\n",
        "        idata = pm.sample_prior_predictive(random_seed=42)\n",
        "        idata.extend(\n",
        "            pm.sample(tune=1000, draws=500, chains=4, random_seed=42, target_accept=0.9, nuts_sampler=\"numpyro\", progressbar=False)\n",
        "        )\n",
        "        pm.compute_log_likelihood(idata, progressbar=False)\n",
        "        idata.extend(\n",
        "            pm.sample_posterior_predictive(idata, random_seed=42)\n",
        "        )\n",
        "\n",
        "    return (idata, linear_model)"
      ],
      "id": "a4d5c5f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's build and sample the models for each variable.\n"
      ],
      "id": "1dbf435e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "idata1, model1 = build_and_sample_model(\n",
        "    scaled_df,\n",
        "    \"x1 ~ 1\"\n",
        ")\n",
        "\n",
        "idata2, model2 = build_and_sample_model(\n",
        "    scaled_df,\n",
        "    \"x1 ~ x2 + 1\"\n",
        ")\n",
        "\n",
        "idata3, model3 = build_and_sample_model(\n",
        "    scaled_df,\n",
        "    \"x1 ~ x3 + 1\"\n",
        ")\n",
        "\n",
        "idata4, model4 = build_and_sample_model(\n",
        "    scaled_df,\n",
        "    \"x1 ~ x2 + x3 + 1\"\n",
        ")\n",
        "\n",
        "_real_mean = scaled_df[\"x1\"].mean()\n",
        "_estimated_mean1 = idata1.posterior_predictive.likelihood.mean(dim=[\"date\"]).values.flatten()\n",
        "_estimated_mean2 = idata2.posterior_predictive.likelihood.mean(dim=[\"date\"]).values.flatten()\n",
        "_estimated_mean3 = idata3.posterior_predictive.likelihood.mean(dim=[\"date\"]).values.flatten()\n",
        "\n",
        "sns.kdeplot(_estimated_mean1 - _real_mean, label='Estimated Mean f(x1 ~ 1)', fill=True)\n",
        "sns.kdeplot(_estimated_mean2 - _real_mean, label='Estimated Mean f(x1 ~ x2 + 1)', fill=True)\n",
        "sns.kdeplot(_estimated_mean3 - _real_mean, label='Estimated Mean f(x1 ~ x3 + 1)', fill=True)\n",
        "plt.axvline(0, color='red', linestyle='--', label='Zero')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "4af52f50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a causal system where the true direction is $x_1$ to $x_2$, the joint distribution factorizes as\n",
        "$$\n",
        "P(x_1, x_2) = P(x_1) \\, P(x_2 \\mid x_1),\n",
        "$$\n",
        "\n",
        "where $x_1$ is exogenous and independent of any effects. This structure reflects that $x_1$'s distribution remains unchanged regardless of the downstream variable $x_2$.\n",
        "\n",
        "When regressing $x_2$ on $x_1$, the model leverages the causal direction, and the conditional distribution $P(x_2 \\mid do(x_1))$ is more concentrated than the marginal $P(x_2)$. This results in residuals that are centered around zero, indicating that most variability in $x_2$ is explained by $x_1$.\n",
        "\n",
        "In contrast, reversing the regression by modeling $x_1$ as a function of $x_2$ disrupts the causal order. The conditional distribution $P(x_1 \\mid do(x2))$ deviates from the true marginal $P(x_1)$, as it attempts to capture the cause from its effect, which is not supported by the causal structure.\n",
        "\n",
        "The bias in the reverse regression arises because conditioning on $x_2$ introduces variability from the noise inherent in $x_2$. This misattribution conflates the independent variability of $x_1$ with that induced by $x_2$, leading to residuals that systematically deviate from zero. Respect to the null model, the residuals are further from zero.\n",
        "\n",
        "This discrepancy underscores the importance of preserving the correct causal direction to avoid bias, as reversing the regression violates the causal Markov condition.\n",
        "\n",
        "Using this logic, we can identify not only independent variables but also the candidate parents for each variable based on how they deviate from the null model.\n"
      ],
      "id": "e315c6dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "idata1, model1 = build_and_sample_model(\n",
        "scaled_df,\n",
        "\"x2 ~ 1\"\n",
        ")\n",
        "\n",
        "idata2, model2 = build_and_sample_model(\n",
        "scaled_df,\n",
        "\"x2 ~ x1 + 1\"\n",
        ")\n",
        "\n",
        "idata3, model3 = build_and_sample_model(\n",
        "scaled_df,\n",
        "\"x2 ~ x3 + 1\"\n",
        ")\n",
        "\n",
        "idata4, model4 = build_and_sample_model(\n",
        "scaled_df,\n",
        "\"x2 ~ x1 + x3 + 1\"\n",
        ")\n",
        "\n",
        "_real_mean = scaled_df[\"x2\"].mean()\n",
        "_estimated_mean1 = idata1.posterior_predictive.likelihood.mean(dim=[\"date\"]).values.flatten()\n",
        "_estimated_mean2 = idata2.posterior_predictive.likelihood.mean(dim=[\"date\"]).values.flatten()\n",
        "_estimated_mean3 = idata3.posterior_predictive.likelihood.mean(dim=[\"date\"]).values.flatten()\n",
        "\n",
        "#plot distribution of means and real mean as vertical line\n",
        "sns.kdeplot(_estimated_mean1 - _real_mean, label='Estimated Mean f(x2 ~ 1)', fill=True)\n",
        "sns.kdeplot(_estimated_mean2 - _real_mean, label='Estimated Mean f(x2 ~ x1 + 1)', fill=True)\n",
        "sns.kdeplot(_estimated_mean3 - _real_mean, label='Estimated Mean f(x2 ~ x3 + 1)', fill=True)\n",
        "plt.axvline(0, color='red', linestyle='--', label='Zero')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "43a95ae8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that the residuals are centered around zero when we regress the marginal probability of $x_2$, but they are closer to zero with a probability distribution narrower than the null model when we regress $x_2$ on $x_1$. This is a good sign that $x_1$ is a parent of $x_2$.\n",
        "\n",
        "We can repeat this process for all the variables in our dataset to start to identify the parents of each variable, and thus identifying sections of the true causal graph.\n",
        "\n",
        "Let's implement this in code.\n",
        "\n",
        "# Identifying Parent Candidates\n",
        "\n",
        "To systematically identify potential parent variables in our causal graph, we'll create a class that evaluates different regression models and compares their residual distributions. This approach leverages the principle that when we correctly model the causal direction, residuals should be more tightly centered around zero compared to misspecified models.\n",
        "\n",
        "::: {.callout-warning}\n",
        "While this approach provides a good initial signal for causal relationships, it has limitations. The method assumes linear relationships, doesn't account for hidden confounders, and may struggle with complex causal structures. Results should be considered as preliminary evidence rather than definitive proof of causal relationships.\n",
        ":::\n",
        "\n",
        "The `ParentCandidateIdentifier` class below will:\n",
        "1. Run a baseline model with only an intercept\n",
        "2. Run models with each potential parent variable\n",
        "3. Compare how much probability mass is concentrated near zero in the residual distributions\n",
        "4. Identify variables that improve the model fit as potential parent candidates\n"
      ],
      "id": "589b14fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ParentCandidateIdentifier:\n",
        "    def __init__(self, data: pd.DataFrame, node: str, possible_parents: list, epsilon: float = 0.005):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            data: DataFrame containing your data.\n",
        "            node: The target variable for which to identify candidate parents.\n",
        "            possible_parents: A list of potential parent variable names.\n",
        "            epsilon: Threshold to define \"mass around zero\" (default 0.05).\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.node = node\n",
        "        self.possible_parents = possible_parents\n",
        "        self.epsilon = epsilon\n",
        "        self.runs = {}\n",
        "        self.results = None\n",
        "\n",
        "    def build_and_sample_model(self, formula: str):\n",
        "        \"\"\"Wrapper for the sampling function.\"\"\"\n",
        "        return build_and_sample_model(self.data, formula)\n",
        "\n",
        "    def compute_mass_around_zero(self, idata, real_mean):\n",
        "        \"\"\"\n",
        "        Compute the fraction of posterior predictive likelihood samples\n",
        "        (averaged over dates) within epsilon of the real mean.\n",
        "        \"\"\"\n",
        "        estimated_mean = idata.posterior_predictive.likelihood.mean(dim=[\"date\"]).values.flatten()\n",
        "        distribution = estimated_mean - real_mean\n",
        "        mass = np.mean(np.abs(distribution) < self.epsilon)\n",
        "        return mass, distribution\n",
        "\n",
        "    def run_all_models(self):\n",
        "        \"\"\"\n",
        "        Run the intercept-only model and each individual parent's model,\n",
        "        storing the sampling results, mass, and error distributions.\n",
        "        \"\"\"\n",
        "        real_mean = self.data[self.node].mean()\n",
        "        runs = {}\n",
        "\n",
        "        # Intercept-only model: P(node)\n",
        "        formula_intercept = f\"{self.node} ~ 1\"\n",
        "        idata_int, _ = self.build_and_sample_model(formula_intercept)\n",
        "        mass_int, dist_int = self.compute_mass_around_zero(idata_int, real_mean)\n",
        "        runs[\"intercept_only\"] = {\n",
        "            \"formula\": formula_intercept,\n",
        "            \"idata\": idata_int,\n",
        "            \"mass\": mass_int,\n",
        "            \"distribution\": dist_int\n",
        "        }\n",
        "\n",
        "        # Individual candidate parent models: P(node|parent)\n",
        "        for parent in self.possible_parents:\n",
        "            formula_parent = f\"{self.node} ~ {parent} + 1\"\n",
        "            idata_parent, _ = self.build_and_sample_model(formula_parent)\n",
        "            mass_parent, dist_parent = self.compute_mass_around_zero(idata_parent, real_mean)\n",
        "            runs[f\"parent_{parent}\"] = {\n",
        "                \"formula\": formula_parent,\n",
        "                \"idata\": idata_parent,\n",
        "                \"mass\": mass_parent,\n",
        "                \"distribution\": dist_parent\n",
        "            }\n",
        "\n",
        "        self.runs = runs\n",
        "        return runs\n",
        "\n",
        "    def identify_candidate_parents(self):\n",
        "        \"\"\"\n",
        "        Runs all models (if not already run), compares the mass around zero,\n",
        "        and returns a decision: if the intercept-only model is best, the target\n",
        "        is independent; otherwise, return the candidate parent with the highest mass.\n",
        "        \"\"\"\n",
        "        if not self.runs:\n",
        "            self.run_all_models()\n",
        "\n",
        "        best_key, best_info = max(self.runs.items(), key=lambda x: x[1][\"mass\"])\n",
        "\n",
        "        if best_key == \"intercept_only\":\n",
        "            decision = \"independent\"\n",
        "            candidate_parents = []\n",
        "        else:\n",
        "            decision = \"dependent\"\n",
        "            candidate_parents = [best_key.split(\"_\", 1)[1]]\n",
        "\n",
        "        self.results = {\n",
        "            \"results\": self.runs,\n",
        "            \"best_model\": {best_key: best_info},\n",
        "            \"decision\": decision,\n",
        "            \"candidate_parents\": candidate_parents\n",
        "        }\n",
        "        return self.results\n",
        "\n",
        "    def plot_distributions(self):\n",
        "        \"\"\"\n",
        "        Plot the error distributions from the stored runs using Seaborn.\n",
        "        \"\"\"\n",
        "        if not self.runs:\n",
        "            self.run_all_models()\n",
        "\n",
        "        for key, run in self.runs.items():\n",
        "            sns.kdeplot(run[\"distribution\"], label=run[\"formula\"], fill=True)\n",
        "        plt.axvline(0, color='red', linestyle='--', label='Zero Error')\n",
        "        plt.xlabel(\"Error (Estimated Mean - Real Mean)\")\n",
        "        plt.ylabel(\"Density\")\n",
        "        plt.title(\"Posterior Predictive Error Distributions\")\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "id": "cea7f5ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can identify the candidate parents for each variable..\n"
      ],
      "id": "e8b518d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "identifier = ParentCandidateIdentifier(data=scaled_df, node=\"x3\", possible_parents=[\"x1\", \"x2\"], epsilon=0.0005)\n",
        "decision_info = identifier.identify_candidate_parents()\n",
        "print(\"Possible parents: \", decision_info[\"candidate_parents\"])\n",
        "\n",
        "identifier.plot_distributions()"
      ],
      "id": "c38f3072",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the conditional independencies of the variables in our dataset allows us to identify the parents of each variable. Currently, we have identified that $x_3$ and $x_2$ are children of $x_1$, and $x_1$ is independent or truly exogenous.\n",
        "\n",
        "We can now use this information to update our causal graph.\n"
      ],
      "id": "c99b4e6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize a directed graph\n",
        "updated_naive_causal_mmm_graph = Digraph()\n",
        "\n",
        "# Add nodes\n",
        "updated_naive_causal_mmm_graph.node(\"X1\", \"Offline\")\n",
        "updated_naive_causal_mmm_graph.node(\"X2\", \"Online\")\n",
        "updated_naive_causal_mmm_graph.node(\"X3\", \"Internal\")\n",
        "updated_naive_causal_mmm_graph.node(\"E\", \"Exogenous variables\", style=\"dashed\")\n",
        "updated_naive_causal_mmm_graph.node(\"T\", \"Target\")\n",
        "\n",
        "updated_naive_causal_mmm_graph.edge(\"E\", \"T\", style=\"dashed\")\n",
        "updated_naive_causal_mmm_graph.edge(\"X1\", \"T\")\n",
        "updated_naive_causal_mmm_graph.edge(\"X1\",\"X2\")\n",
        "updated_naive_causal_mmm_graph.edge(\"X1\",\"X3\")\n",
        "updated_naive_causal_mmm_graph.edge(\"X2\", \"T\")\n",
        "updated_naive_causal_mmm_graph.edge(\"X3\", \"T\")\n",
        "\n",
        "# Create a figure with five subplots\n",
        "fig, axes = plt.subplots(1, 2,)\n",
        "\n",
        "# Set titles for each subplot\n",
        "titles = [\"Naive DAG\", \"Updated DAG\"]\n",
        "for ax, title in zip(axes, titles):\n",
        "    ax.set_title(title, fontsize=6)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Render and plot each graph\n",
        "naive_causal_mmm_graph.render(format='png', filename='images/naive_dag')\n",
        "axes[0].imshow(mpimg.imread('images/naive_dag.png'))\n",
        "\n",
        "updated_naive_causal_mmm_graph.render(format='png', filename='images/updated_dag')\n",
        "axes[1].imshow(mpimg.imread('images/updated_dag.png'))\n",
        "\n",
        "# Add main title\n",
        "plt.suptitle(\"Comparison of DAG Graphs\", fontsize=24)\n",
        "plt.tight_layout()"
      ],
      "id": "4dc2140b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, we can update our model of the world to include the causal relationships we have identified. How else can we use this information to learn more about the causal relationships in our dataset?\n",
        "\n",
        "# Mediation analysis for the causal discovery\n",
        "\n",
        "In mediation analysis, the total effect of a predictor $X1$ on a target $T$ is decomposed into direct and indirect components. The indirect effect operates through a mediator $M$, modeled as $M = \\alpha_m + a \\times X1 + \\text{error}$. Simultaneously, the outcome is modeled as $T = \\alpha_y + c' \\times X1 + b \\times M + \\text{error}$. Here, the product $a \\times b$ quantifies the indirect effect, while $c'$ represents the direct effect of $X1$ on $T$. By estimating these coefficients, we can assess whether the influence of $X1$ on $T$ is transmitted via $M$, entirely direct, or a combination of both. Statistical inference is performed using credible intervals, where intervals excluding zero indicate significant effects.\n",
        "\n",
        "If the indirect effect $a \\times b$ is significant and the direct effect $c'$ is not, we conclude that $X1$'s impact on $T$ is fully mediated by $M$. Conversely, significant values for both $a \\times b$ and $c'$ suggest that $X1$ exerts both direct and indirect influences on $T$.\n",
        "\n",
        "In simple terms, mediation analysis helps us determine whether a predictor $X1$ influences an outcome $T$ directly or mainly by first affecting a mediator $M$, which then impacts $T$. If the mediator's effect is significant while the direct effect is not, it suggests that $X1$ affects $T$ primarily through its influence on $M$.\n",
        "\n",
        "Why do this on top of the causal discovery we have already done? The reason is that we can use the mediation analysis to verify the causal relationships we have identified, becuase if a node is parent the other then some effect is mediated, if we can detect that mediation, then we can decide if the causal relationship is direct or indirect. If we fail to detect mediation, then probably our findings are not robust to the causal discovery we have done.\n"
      ],
      "id": "260f0b3e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MediationAnalysis:\n",
        "    \"\"\"\n",
        "    A class for performing Bayesian mediation analysis using a joint mediation model.\n",
        "\n",
        "    The model is specified as:\n",
        "      Mediator:    M = α_m + a * X + error\n",
        "      Outcome:     Y = α_y + c′ * X + b * M + error\n",
        "\n",
        "    Derived parameters:\n",
        "      - Indirect effect: ab = a * b\n",
        "      - Total effect:    c  = c′ + (a * b)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        DataFrame containing the predictor, mediator, and outcome variables.\n",
        "    x : str\n",
        "        Column name for the predictor (X).\n",
        "    m : str\n",
        "        Column name for the mediator (M).\n",
        "    y : str\n",
        "        Column name for the outcome (Y).\n",
        "    hdi : float, optional\n",
        "        Credible interval width for HDI (default 0.95).\n",
        "    sampler_kwargs : dict, optional\n",
        "        Additional keyword arguments for the sampler.\n",
        "        Default: {\"tune\": 1000, \"draws\": 500, \"chains\": 4,\n",
        "                  \"random_seed\": 42, \"target_accept\": 0.9,\n",
        "                  \"nuts_sampler\": \"numpyro\", \"progressbar\": False}\n",
        "    \"\"\"\n",
        "    def __init__(self, data: pd.DataFrame, x: str, m: str, y: str, hdi: float = 0.95, sampler_kwargs: dict = None):\n",
        "        self.data = data\n",
        "        self.x = x\n",
        "        self.m = m\n",
        "        self.y = y\n",
        "        self.hdi = hdi\n",
        "        self.sampler_kwargs = sampler_kwargs or {\n",
        "            \"tune\": 1000,\n",
        "            \"draws\": 500,\n",
        "            \"chains\": 4,\n",
        "            \"random_seed\": 42,\n",
        "            \"target_accept\": 0.9,\n",
        "            \"nuts_sampler\": \"numpyro\",\n",
        "            \"progressbar\": False\n",
        "        }\n",
        "        self.idata = None\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the Bayesian mediation model.\n",
        "\n",
        "        This method constructs the PyMC model and stores it in self.model.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        model : pm.Model\n",
        "            The constructed PyMC model.\n",
        "        \"\"\"\n",
        "        # Extract data arrays\n",
        "        X_data = self.data[self.x].values\n",
        "        M_data = self.data[self.m].values\n",
        "        Y_data = self.data[self.y].values\n",
        "        N = len(self.data)\n",
        "        coords = {\"obs\": range(N)}\n",
        "\n",
        "        with pm.Model(coords=coords) as model:\n",
        "            # Mediator path: M = α_m + a * X + error\n",
        "            alpha_m = pm.Normal(\"alpha_m\", mu=0.0, sigma=1.0)\n",
        "            a = pm.Normal(\"a\", mu=0.0, sigma=1.0)\n",
        "            sigma_m = pm.Exponential(\"sigma_m\", lam=1.0)\n",
        "            mu_m = alpha_m + a * X_data\n",
        "            pm.Normal(\"M_obs\", mu=mu_m, sigma=sigma_m, observed=M_data, dims=\"obs\")\n",
        "\n",
        "            # Outcome path: Y = α_y + c′ * X + b * M + error\n",
        "            alpha_y = pm.Normal(\"alpha_y\", mu=0.0, sigma=1.0)\n",
        "            c_prime = pm.Normal(\"c_prime\", mu=0.0, sigma=1.0)\n",
        "            b = pm.Normal(\"b\", mu=0.0, sigma=1.0)\n",
        "            sigma_y = pm.Exponential(\"sigma_y\", lam=1.0)\n",
        "            mu_y = alpha_y + c_prime * X_data + b * M_data\n",
        "            pm.Normal(\"Y_obs\", mu=mu_y, sigma=sigma_y, observed=Y_data, dims=\"obs\")\n",
        "\n",
        "            # Derived parameters: indirect and total effects\n",
        "            pm.Deterministic(\"ab\", a * b)\n",
        "            pm.Deterministic(\"c\", c_prime + a * b)\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Sample from the previously built mediation model.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : MediationAnalysis\n",
        "            The fitted mediation analysis object.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the model has not been built yet.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"The model has not been built. Call build_model() before fit().\")\n",
        "\n",
        "        with self.model:\n",
        "            self.idata = pm.sample(**self.sampler_kwargs)\n",
        "\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"\n",
        "        Get a numerical summary of the mediation parameters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dictionary with mean estimates and HDI bounds for each parameter.\n",
        "        \"\"\"\n",
        "        var_names = [\"alpha_m\", \"a\", \"alpha_y\", \"c_prime\", \"b\", \"ab\", \"c\"]\n",
        "        summary_df = az.summary(self.idata, var_names=var_names, hdi_prob=self.hdi)\n",
        "\n",
        "        # Compute the HDI column names based on the specified interval\n",
        "        lower_percent = (1 - self.hdi) / 2 * 100\n",
        "        upper_percent = 100 - lower_percent\n",
        "        lower_col = f\"hdi_{lower_percent:.1f}%\"\n",
        "        upper_col = f\"hdi_{upper_percent:.1f}%\"\n",
        "\n",
        "        results = {}\n",
        "        for key in var_names:\n",
        "            results[key] = {\n",
        "                \"mean\": summary_df.loc[key, \"mean\"],\n",
        "                \"hdi_lower\": summary_df.loc[key, lower_col],\n",
        "                \"hdi_upper\": summary_df.loc[key, upper_col]\n",
        "            }\n",
        "        return results\n",
        "\n",
        "    def get_report(self, x_label: str = None, m_label: str = None, y_label: str = None):\n",
        "        \"\"\"\n",
        "        Generate a plain-language report of the mediation analysis results.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_label : str, optional\n",
        "            Label for the predictor variable (default uses self.x).\n",
        "        m_label : str, optional\n",
        "            Label for the mediator variable (default uses self.m).\n",
        "        y_label : str, optional\n",
        "            Label for the outcome variable (default uses self.y).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            A human-readable summary of the mediation effects.\n",
        "        \"\"\"\n",
        "        # Use provided labels or default to variable names\n",
        "        x_label = x_label or self.x\n",
        "        m_label = m_label or self.m\n",
        "        y_label = y_label or self.y\n",
        "\n",
        "        var_names = [\"a\", \"b\", \"c_prime\", \"ab\", \"c\"]\n",
        "        summary_df = az.summary(self.idata, var_names=var_names, hdi_prob=self.hdi)\n",
        "\n",
        "        def hdi_includes_zero(row):\n",
        "            lower_percent = (1 - self.hdi) / 2 * 100\n",
        "            upper_percent = 100 - lower_percent\n",
        "            lower_col = f\"hdi_{lower_percent:.1f}%\"\n",
        "            upper_col = f\"hdi_{upper_percent:.1f}%\"\n",
        "            return row[lower_col] <= 0 <= row[upper_col]\n",
        "\n",
        "        # Extract summary statistics\n",
        "        a_stats = summary_df.loc[\"a\"]\n",
        "        b_stats = summary_df.loc[\"b\"]\n",
        "        c_prime_stats = summary_df.loc[\"c_prime\"]\n",
        "        ab_stats = summary_df.loc[\"ab\"]\n",
        "        c_stats = summary_df.loc[\"c\"]\n",
        "\n",
        "        a_mean = a_stats[\"mean\"]\n",
        "        b_mean = b_stats[\"mean\"]\n",
        "        c_prime_mean = c_prime_stats[\"mean\"]\n",
        "        ab_mean = ab_stats[\"mean\"]\n",
        "        c_mean = c_stats[\"mean\"]\n",
        "\n",
        "        a_zero = hdi_includes_zero(a_stats)\n",
        "        b_zero = hdi_includes_zero(b_stats)\n",
        "        c_prime_zero = hdi_includes_zero(c_prime_stats)\n",
        "        ab_zero = hdi_includes_zero(ab_stats)\n",
        "        c_zero = hdi_includes_zero(c_stats)\n",
        "\n",
        "        lines = []\n",
        "        lines.append(f\"**Bayesian Mediation Analysis Overview** ({int(self.hdi * 100)}% HDI)\")\n",
        "        lines.append(f\"Variables: {x_label} (predictor), {m_label} (mediator), {y_label} (outcome).\")\n",
        "\n",
        "        # Interpret each path\n",
        "        if not a_zero:\n",
        "            direction = \"positive\" if a_mean > 0 else \"negative\"\n",
        "            lines.append(f\"- Path a ({x_label} → {m_label}) is credibly {direction} (mean = {a_mean:.3f}).\")\n",
        "        else:\n",
        "            lines.append(f\"- Path a ({x_label} → {m_label}) is weak (HDI includes 0, mean = {a_mean:.3f}).\")\n",
        "\n",
        "        if not b_zero:\n",
        "            direction = \"positive\" if b_mean > 0 else \"negative\"\n",
        "            lines.append(f\"- Path b ({m_label} → {y_label}, controlling for {x_label}) is credibly {direction} (mean = {b_mean:.3f}).\")\n",
        "        else:\n",
        "            lines.append(f\"- Path b ({m_label} → {y_label}, controlling for {x_label}) is weak (HDI includes 0, mean = {b_mean:.3f}).\")\n",
        "\n",
        "        if not ab_zero:\n",
        "            direction = \"positive\" if ab_mean > 0 else \"negative\"\n",
        "            lines.append(f\"- Indirect effect (a×b) is credibly {direction} (mean = {ab_mean:.3f}).\")\n",
        "        else:\n",
        "            lines.append(f\"- Indirect effect (a×b) is uncertain (HDI includes 0, mean = {ab_mean:.3f}).\")\n",
        "\n",
        "        if not c_prime_zero:\n",
        "            direction = \"positive\" if c_prime_mean > 0 else \"negative\"\n",
        "            lines.append(f\"- Direct effect (c') is credibly {direction} (mean = {c_prime_mean:.3f}).\")\n",
        "        else:\n",
        "            lines.append(f\"- Direct effect (c') is near zero (HDI includes 0, mean = {c_prime_mean:.3f}).\")\n",
        "\n",
        "        if not c_zero:\n",
        "            direction = \"positive\" if c_mean > 0 else \"negative\"\n",
        "            lines.append(f\"- Total effect (c) is credibly {direction} (mean = {c_mean:.3f}).\")\n",
        "        else:\n",
        "            lines.append(f\"- Total effect (c) is uncertain (HDI includes 0, mean = {c_mean:.3f}).\")\n",
        "\n",
        "        lines.append(\"\")\n",
        "        if not ab_zero and c_prime_zero:\n",
        "            lines.append(f\"It appears that {m_label} fully mediates the effect of {x_label} on {y_label} (indirect effect is non-zero while direct effect is near zero).\")\n",
        "        elif not ab_zero and not c_prime_zero:\n",
        "            lines.append(f\"It appears that {m_label} partially mediates the effect of {x_label} on {y_label} (both indirect and direct effects are credibly non-zero).\")\n",
        "        else:\n",
        "            lines.append(\"Mediation is unclear or absent (the indirect effect includes zero or the total effect is not clearly different from zero).\")\n",
        "\n",
        "        return \"\\n\".join(lines)"
      ],
      "id": "9004977a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the mediation analysis for the first two variables.\n"
      ],
      "id": "a902b138"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis1 = MediationAnalysis(data=scaled_df, x=\"x1\", m=\"x2\", y=\"y\")\n",
        "analysis1.build_model()\n",
        "analysis1.fit()\n",
        "analysis1.get_summary()\n",
        "print(analysis1.get_report())\n",
        "\n",
        "analysis2 = MediationAnalysis(data=scaled_df, x=\"x1\", m=\"x3\", y=\"y\")\n",
        "analysis2.build_model()\n",
        "analysis2.fit()\n",
        "analysis2.get_summary()\n",
        "print(analysis2.get_report())"
      ],
      "id": "27242d5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great 👏🏻 Based on the following results we can conclude that $x_1$ affects $y$ through $x_2$ and $x_3$ but not directly. This conclusion is based on the indirect effect being significant and the direct effect being near zero when controlling for the mediator $x2$ and partial for $x3$.\n",
        "\n",
        "If both factor were present, the indirect effect would be stronger, given the previous results. So, for simplicity, we'll not test the mediation when both factors are present.\n",
        "\n",
        "We can again, update our causal graph to reflect the new findings.\n"
      ],
      "id": "8e2a0817"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize a directed graph\n",
        "updated_naive_causal_mmm_graph1 = Digraph()\n",
        "\n",
        "# Add nodes\n",
        "updated_naive_causal_mmm_graph1.node(\"X1\", \"Offline\")\n",
        "updated_naive_causal_mmm_graph1.node(\"X2\", \"Online\")\n",
        "updated_naive_causal_mmm_graph1.node(\"X3\", \"Internal\")\n",
        "updated_naive_causal_mmm_graph1.node(\"E\", \"Exogenous variables\", style=\"dashed\")\n",
        "updated_naive_causal_mmm_graph1.node(\"T\", \"Target\")\n",
        "\n",
        "updated_naive_causal_mmm_graph1.edge(\"E\", \"T\", style=\"dashed\")\n",
        "updated_naive_causal_mmm_graph1.edge(\"X1\",\"X2\")\n",
        "updated_naive_causal_mmm_graph1.edge(\"X1\",\"X3\")\n",
        "updated_naive_causal_mmm_graph1.edge(\"X2\", \"T\")\n",
        "updated_naive_causal_mmm_graph1.edge(\"X3\", \"T\")\n",
        "\n",
        "# Create a figure with five subplots\n",
        "fig, axes = plt.subplots(1, 3,)\n",
        "\n",
        "# Set titles for each subplot\n",
        "titles = [\"Naive DAG\", \"Updated DAG V0\", \"Updated DAG V1\"]\n",
        "for ax, title in zip(axes, titles):\n",
        "    ax.set_title(title, fontsize=6)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Render and plot each graph\n",
        "naive_causal_mmm_graph.render(format='png', filename='images/naive_dag')\n",
        "axes[0].imshow(mpimg.imread('images/naive_dag.png'))\n",
        "\n",
        "updated_naive_causal_mmm_graph.render(format='png', filename='images/updated_dag')\n",
        "axes[1].imshow(mpimg.imread('images/updated_dag.png'))\n",
        "\n",
        "updated_naive_causal_mmm_graph1.render(format='png', filename='images/updated_dag1')\n",
        "axes[2].imshow(mpimg.imread('images/updated_dag1.png'))\n",
        "\n",
        "# Add main title\n",
        "plt.suptitle(\"Comparison of DAG Graphs\", fontsize=24)\n",
        "plt.tight_layout()"
      ],
      "id": "1c3f82d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is great! Our new causal graph is more complex, but it is more accurate to the one defined before. Nevertheless, we need a significant amount of time, and manual work to arrive to this conclusion.\n",
        "\n",
        "How could we automate this process? It's even possible? and how this would solve the intial problem?\n",
        "\n",
        "Yes, it is possible! We can use causal discovery algorithms to automate this process.\n",
        "\n",
        "# Introduction to causal discovery\n",
        "Causal discovery infers directional cause-and-effect relationships from observational data. It uses computational algorithms to construct directed acyclic graphs that represent potential causal mechanisms. These techniques are based on the causal Markov condition and the assumption of statistical faithfulness. They employ statistical tests for conditional independence to differentiate direct influences from indirect associations. This approach integrates statistical inference and graph theory to model complex systems. Overall, it uncovers hidden causal structures that enhance our understanding and estimations of dynamic phenomena.\n",
        "\n",
        "> Causal Markov Assumption: Each variable is independent of its non-effects given its direct causes, meaning that the joint probability distribution can be factorized according to the directed acyclic graph structure. This implies that once you condition on a variable's immediate causes, any further upstream or parallel influences are rendered statistically irrelevant.\n",
        "\n",
        "> Statistical Faithfulness: This assumption posits that all and only the conditional independence relations observed in the data are those implied by the causal graph. In other words, there are no accidental cancellations or coincidental independencies beyond what the causal structure predicts.\n"
      ],
      "id": "dfdc36af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CausalDiscovery:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.labels = [f'{col}' for col in data.columns.to_list()]\n",
        "\n",
        "    def greedy_search(self, **kwargs):\n",
        "        result = ges(X=self.data.to_numpy(), **kwargs)\n",
        "        return result[\"G\"]\n",
        "\n",
        "    def peter_clark(self, **kwargs):\n",
        "        result = pc(self.data.to_numpy(), **kwargs)\n",
        "        return result.G\n",
        "\n",
        "    def to_pydot(self, graph):\n",
        "        return GraphUtils.to_pydot(graph, labels=self.labels)\n",
        "\n",
        "    def to_dict(self, graph):\n",
        "        \"\"\"\n",
        "        Convert a general graph to a dictionary representation where each node is a key\n",
        "        and the value is a list of its descendants.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : causallearn.graph.GeneralGraph.GeneralGraph\n",
        "            The input graph.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            A dictionary where keys are node labels and values are lists of descendant node labels.\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "        nodes = sorted(graph.get_nodes(), key=lambda x: str(x))\n",
        "\n",
        "        # Initialize the dictionary with empty lists for all nodes\n",
        "        for i, node in enumerate(nodes):\n",
        "            result[self.labels[i]] = []\n",
        "\n",
        "        # For each node, find its children (direct descendants)\n",
        "        for i, node in enumerate(nodes):\n",
        "            node_label = self.labels[i]\n",
        "            for j, potential_child in enumerate(nodes):\n",
        "                if i != j and graph.get_edge(node, potential_child) is not None:\n",
        "                    # Check if there's a directed edge from node to potential_child\n",
        "                    edge = graph.get_edge(node, potential_child)\n",
        "                    if (edge.get_endpoint1() == Endpoint.TAIL and\n",
        "                        edge.get_endpoint2() == Endpoint.ARROW):\n",
        "                        result[node_label].append(self.labels[j])\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def to_graphviz(self, graph, handle_circle=\"skip\"):\n",
        "        \"\"\"\n",
        "        Convert a general graph into a Graphviz Digraph using the pydot conversion\n",
        "        for nodes while preserving the original directed edge ordering.\n",
        "\n",
        "        Only if the original graph indicates that an edge is undirected (both endpoints\n",
        "        are TAIL) do we override the arrow style (using dir=\"none\"). Otherwise, we leave\n",
        "        the pydot-provided direction unchanged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : causallearn.graph.GeneralGraph.GeneralGraph\n",
        "            The input graph.\n",
        "        handle_circle : str, optional\n",
        "            How to handle circle endpoints (not used explicitly here but available for future logic).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        graphviz.Digraph\n",
        "            A Graphviz Digraph where undirected edges are rendered without arrowheads.\n",
        "        \"\"\"\n",
        "        # Get the pydot graph (for node positions/labels)\n",
        "        dot = self.to_pydot(graph)\n",
        "        digraph = Digraph()\n",
        "        digraph.attr(size='8,8')\n",
        "\n",
        "        # Build a mapping of the original graph nodes based on sorted order.\n",
        "        original_nodes = sorted(graph.get_nodes(), key=lambda x: str(x))\n",
        "        # Map string indices (\"0\", \"1\", …) to the original nodes.\n",
        "        index_to_node = {str(i): node for i, node in enumerate(original_nodes)}\n",
        "        # Map indices to labels using self.labels.\n",
        "        node_labels = {str(i): self.labels[i] for i in range(len(original_nodes))}\n",
        "\n",
        "        # Add nodes to the Graphviz Digraph.\n",
        "        for idx_str, label in node_labels.items():\n",
        "            digraph.node(label)\n",
        "\n",
        "        # Process each edge from the pydot graph.\n",
        "        processed = set()\n",
        "        pydot_edges = dot.get_edges()\n",
        "\n",
        "        for edge in pydot_edges:\n",
        "            # Get source and destination from pydot.\n",
        "            src_raw = edge.get_source()\n",
        "            dst_raw = edge.get_destination()\n",
        "            src_str = src_raw.strip('\"') if isinstance(src_raw, str) else str(src_raw)\n",
        "            dst_str = dst_raw.strip('\"') if isinstance(dst_raw, str) else str(dst_raw)\n",
        "\n",
        "            # Get original node objects using our mapping.\n",
        "            src_node = index_to_node.get(src_str)\n",
        "            dst_node = index_to_node.get(dst_str)\n",
        "            if src_node is None or dst_node is None:\n",
        "                continue\n",
        "\n",
        "            # Get display labels.\n",
        "            src_label = node_labels.get(src_str, src_str)\n",
        "            dst_label = node_labels.get(dst_str, dst_str)\n",
        "\n",
        "            # Use a tuple (src_str, dst_str) to ensure we don't add duplicates.\n",
        "            edge_key = (src_str, dst_str)\n",
        "            reverse_key = (dst_str, src_str)\n",
        "            if edge_key in processed or reverse_key in processed:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Get endpoint information from the original graph.\n",
        "                e_uv = graph.get_endpoint(src_node, dst_node)\n",
        "                e_vu = graph.get_endpoint(dst_node, src_node)\n",
        "            except KeyError:\n",
        "                # Skip if the original graph doesn't contain this edge.\n",
        "                continue\n",
        "\n",
        "            # If both endpoints are TAIL, we treat the edge as undirected.\n",
        "            if e_uv == Endpoint.TAIL and e_vu == Endpoint.TAIL:\n",
        "                digraph.edge(src_label, dst_label, dir=\"none\")\n",
        "                processed.add(edge_key)\n",
        "                processed.add(reverse_key)\n",
        "            else:\n",
        "                # Otherwise, preserve the original pydot direction.\n",
        "                digraph.edge(src_label, dst_label)\n",
        "                processed.add(edge_key)\n",
        "\n",
        "        return digraph\n",
        "\n",
        "    def to_networkx(self, graph) -> nx.DiGraph:\n",
        "        \"\"\"\n",
        "        Convert a general graph (e.g. from causallearn) into a NetworkX DiGraph.\n",
        "\n",
        "        Nodes are added as provided by graph.get_nodes(), and for each ordered pair (u, v)\n",
        "        where an edge exists (as determined by graph.get_endpoint(u, v)), we add a directed\n",
        "        edge with an attribute 'endpoint' that stores the edge marker.\n",
        "\n",
        "        If the general graph does not provide a direct list of edges (e.g. via a get_edges() method),\n",
        "        we iterate over all pairs of nodes.\n",
        "        \"\"\"\n",
        "        digraph = nx.DiGraph()\n",
        "        nodes = graph.get_nodes()\n",
        "        # Add nodes to the networkx graph.\n",
        "        for node in nodes:\n",
        "            digraph.add_node(node)\n",
        "\n",
        "        # If available, use a dedicated method to get edges.\n",
        "        try:\n",
        "            edges = graph.get_edges()\n",
        "        except AttributeError:\n",
        "            # Fallback: iterate over all ordered pairs (inefficient for large graphs)\n",
        "            edges = []\n",
        "            for u in nodes:\n",
        "                for v in nodes:\n",
        "                    if u == v:\n",
        "                        continue\n",
        "                    try:\n",
        "                        # Attempt to get an endpoint; if present, we consider that an edge exists.\n",
        "                        _ = graph.get_endpoint(u, v)\n",
        "                        edges.append((u, v))\n",
        "                    except KeyError:\n",
        "                        continue\n",
        "\n",
        "        # Add edges with endpoint attributes.\n",
        "        for u, v in edges:\n",
        "            try:\n",
        "                endpoint_uv = graph.get_endpoint(u, v)\n",
        "            except KeyError:\n",
        "                continue\n",
        "            digraph.add_edge(u, v, endpoint=endpoint_uv)\n",
        "\n",
        "        return digraph\n",
        "\n",
        "    def _networkx_to_graphviz(self, nx_graph: nx.DiGraph) -> Digraph:\n",
        "        \"\"\"\n",
        "        Convert a NetworkX DiGraph into a Graphviz Digraph.\n",
        "\n",
        "        This method uses similar logic to 'to_graphviz', checking for reciprocal edges.\n",
        "        If an edge (u,v) and its reverse (v,u) exist and both have the attribute endpoint\n",
        "        equal to Endpoint.TAIL, the edge is rendered as undirected (dir=\"none\").\n",
        "        \"\"\"\n",
        "        digraph = Digraph()\n",
        "        digraph.attr(size='8,8')\n",
        "        processed = set()\n",
        "\n",
        "        # Sort nodes to create a consistent mapping with self.labels.\n",
        "        sorted_nodes = sorted(nx_graph.nodes(), key=lambda x: str(x))\n",
        "        node_labels = {}\n",
        "        for i, node in enumerate(sorted_nodes):\n",
        "            # Use self.labels if available, otherwise default to the node's string representation.\n",
        "            label = self.labels[i] if i < len(self.labels) else str(node)\n",
        "            node_labels[node] = label\n",
        "            digraph.node(label)\n",
        "\n",
        "        for u, v in nx_graph.edges():\n",
        "            if (u, v) in processed or (v, u) in processed:\n",
        "                continue\n",
        "            src_label = node_labels.get(u, str(u))\n",
        "            dst_label = node_labels.get(v, str(v))\n",
        "\n",
        "            # Check if the reverse edge exists to potentially mark as undirected.\n",
        "            if nx_graph.has_edge(v, u):\n",
        "                endpoint_uv = nx_graph.edges[u, v].get('endpoint', None)\n",
        "                endpoint_vu = nx_graph.edges[v, u].get('endpoint', None)\n",
        "                if endpoint_uv == Endpoint.TAIL and endpoint_vu == Endpoint.TAIL:\n",
        "                    digraph.edge(src_label, dst_label, dir=\"none\")\n",
        "                    processed.add((u, v))\n",
        "                    processed.add((v, u))\n",
        "                    continue\n",
        "            # Otherwise, add the edge as directed.\n",
        "            digraph.edge(src_label, dst_label)\n",
        "            processed.add((u, v))\n",
        "\n",
        "        return digraph\n",
        "\n",
        "    def _graphviz_to_networkx(self, gv_graph: Digraph) -> nx.DiGraph:\n",
        "        \"\"\"\n",
        "        Convert a Graphviz Digraph into a NetworkX DiGraph.\n",
        "\n",
        "        This method extracts the DOT source from the provided Graphviz Digraph,\n",
        "        parses it using pydot, and then converts the resulting pydot graph into\n",
        "        a NetworkX directed graph. This ensures that node labels and edge orientations\n",
        "        are maintained consistently.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        gv_graph : graphviz.Digraph\n",
        "            The Graphviz Digraph to be converted.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        nx.DiGraph\n",
        "            A NetworkX DiGraph representation of the input Graphviz graph.\n",
        "        \"\"\"\n",
        "        # Retrieve the DOT source code from the Graphviz Digraph.\n",
        "        dot_str = gv_graph.source\n",
        "\n",
        "        # Parse the DOT data using pydot.\n",
        "        pydot_graphs = pydot.graph_from_dot_data(dot_str)\n",
        "        if not pydot_graphs:\n",
        "            raise ValueError(\"No valid pydot graphs could be parsed from the DOT data.\")\n",
        "        # pydot.graph_from_dot_data returns a list; we take the first one.\n",
        "        pydot_graph = pydot_graphs[0]\n",
        "\n",
        "        # Use NetworkX’s built-in conversion from a pydot graph to a DiGraph.\n",
        "        nx_graph = nx.nx_pydot.from_pydot(pydot_graph)\n",
        "        return nx_graph"
      ],
      "id": "c460c0be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Causal Learn allows use to use different algorithms to infer the markov equivalent class of the causal graph. The previous class is a wrapper that allows us to use the different algorithms implemented in the causal learn library, and plot them more easily.\n",
        "\n",
        "Currently we wrap the following algorithms:\n",
        "\n",
        "- Greedy Search (GES)\n",
        "- Peter-Clark (PC)\n",
        "\n",
        "::: {.callout-info}\n",
        "## Causal Discovery Algorithms\n",
        "The **Peter-Clark algorithm** is a constraint-based method that infers causal structures from observational data using conditional independence tests. It starts with a fully connected undirected graph where every variable is initially connected to every other variable. The algorithm systematically tests conditional independence between pairs of variables, conditioning on increasingly larger subsets of other variables. When a conditional independence is detected, the corresponding edge is removed from the graph.\n",
        "\n",
        "On the other hand, **Greedy Search** is a score-based method that iteratively improves a candidate causal model by locally modifying its structure. It begins with an initial directed acyclic graph and evaluates a scoring metric that balances goodness-of-fit with model complexity. The algorithm explores modifications such as adding, deleting, or reversing edges to find local improvements in the score. At each iteration, it selects the change that produces the highest increase in the score, following a step-by-step improvement strategy. The search continues until no single modification can further enhance the model's score. This method efficiently navigates the combinatorial search space of possible graphs by making locally optimal choices.\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Causal Sufficiency Assumption\n",
        "Any causal discovery algorithm is based on the assumption that all the relevant variables are observed. If some relevant variable is not observed, the algorithm will not be able to infer the correct causal graph. Each variable, even the unobserved ones, should be represented in the dataset, so the algorithm can include them in the causal graph and validation tests.\n",
        ":::\n",
        "\n",
        "The following example shows the causal graph inferred using the Greedy Search algorithm.\n"
      ],
      "id": "f0953ae7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "causal_model = CausalDiscovery(scaled_df[[\"holiday_signal\", \"inflation\", \"x1\", \"x2\", \"x3\", \"y\"]])\n",
        "ges_graph = causal_model.greedy_search()\n",
        "\n",
        "# Create a figure with five subplots\n",
        "fig, axes = plt.subplots(1, 4,)\n",
        "\n",
        "# Set titles for each subplot\n",
        "titles = [\"Naive DAG\", \"Updated DAG\", \"Updated DAG 1\", \"Discovered DAG\"]\n",
        "for ax, title in zip(axes, titles):\n",
        "    ax.set_title(title, fontsize=6)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Render and plot each graph\n",
        "naive_causal_mmm_graph.render(format='png', filename='images/naive_dag')\n",
        "axes[0].imshow(mpimg.imread('images/naive_dag.png'))\n",
        "\n",
        "updated_naive_causal_mmm_graph.render(format='png', filename='images/updated_dag')\n",
        "axes[1].imshow(mpimg.imread('images/updated_dag.png'))\n",
        "\n",
        "updated_naive_causal_mmm_graph1.render(format='png', filename='images/updated_dag1')\n",
        "axes[2].imshow(mpimg.imread('images/updated_dag1.png'))\n",
        "\n",
        "real_dag_graph = causal_model.to_graphviz(ges_graph)\n",
        "real_dag_graph.render(format='png', filename='images/discovered_dag')\n",
        "axes[3].imshow(mpimg.imread('images/discovered_dag.png'))\n",
        "\n",
        "# Add main title\n",
        "plt.suptitle(\"Comparison of DAG Graphs\", fontsize=24)\n",
        "plt.tight_layout()"
      ],
      "id": "97217059",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The causal graph capture by greedy search is very similar to the true causal graph. Some arrows are directed to variables that are not related, but this is expected given the nature of the data, we still have noise in the data, and sporious correlations that can't be fully falsify by the independence tests. Additionally, the finded graph can lie in the markov equivalence class of the true causal graph, meaning that there are multiple DAGs that are compatible with the data.\n",
        "\n",
        "This instead of being a problem, is a great news because we can now start to work with experimentation to test the current structure, and improve it iteratively, without the need to wait for these answers to get the right estimates in a regression model.\n",
        "\n",
        "Let's break down the causal paths from x2 to y in the graph:\n",
        "\n",
        "**Confounding paths:**\n",
        "\n",
        "- Holiday: Affects both x2 and y (holiday → x2 and holiday → y).\n",
        "- Inflation: Affects both x2 and y (inflation → x2 and inflation → y).\n",
        "- x1: Influences x2 (x1 → x2) and also affects y indirectly through x3 (x1 → x3 → y).\n",
        "\n",
        "**Mediating path:**\n",
        "\n",
        "- x3: Lies on the causal pathway from x2 to y (x2 → x3 → y).\n",
        "\n",
        "**What needs to be controlled?**\n",
        "\n",
        "To estimate the total effect of x2 on y without bias, you need to block all backdoor (confounding) paths. This means controlling for the common causes:\n",
        "\n",
        "- Holiday\n",
        "- Inflation\n",
        "- x1\n",
        "\n",
        "**Why not control for x3?**\n",
        "Since x3 is a mediator (i.e., it transmits part of the effect of x2 to y), including it in your regression would block the indirect effect of x2 on y. This “over-control” would result in an estimate that reflects only the direct effect of x2 on y, not the total effect. Additionally, controlling for mediators can sometimes introduce bias if there are other complex relationships in the graph.\n"
      ],
      "id": "05e277e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mmm = MMM(\n",
        "    model_config=model_config,\n",
        "    date_column=\"date\",\n",
        "    channel_columns=[\n",
        "        \"x1\",\n",
        "        # \"x2\",\n",
        "        \"x3\"\n",
        "    ],\n",
        "    control_columns=[\n",
        "        \"holiday_signal\",\n",
        "        \"inflation\",\n",
        "    ],\n",
        "    adstock=GeometricAdstock(l_max=24),\n",
        "    saturation=MichaelisMentenSaturation(),\n",
        ")\n",
        "\n",
        "mmm.fit(X, y, **fit_kwargs)\n",
        "mmm.sample_posterior_predictive(\n",
        "    X=X,\n",
        "    extend_idata=True,\n",
        "    combined=True,\n",
        "    random_seed=rng,\n",
        ")\n",
        "\n",
        "initial_model_recover_effect = (\n",
        "    az.hdi(mmm.fit_result[\"channel_contributions\"], hdi_prob=0.95)\n",
        "    * mmm.target_transformer[\"scaler\"].scale_.item()\n",
        ")\n",
        "initial_model_mean_effect = (\n",
        "    mmm.fit_result.channel_contributions.mean(dim=[\"chain\", \"draw\"])\n",
        "    * mmm.target_transformer[\"scaler\"].scale_.item()\n",
        ")"
      ],
      "id": "8022f9dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's plot the posterior distribution of the effect of x3 on y.\n"
      ],
      "id": "b97ad225"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_posterior(y_real, posterior, figsize=(8, 4), path_color='orange', hist_color='orange', **kwargs):\n",
        "    \"\"\"\n",
        "    Plot the posterior distribution of a stochastic process.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_real : array-like\n",
        "        The real values to compare against the posterior.\n",
        "    posterior : xarray.DataArray\n",
        "        The posterior distribution with shape (draw, chain, date).\n",
        "    figsize : tuple, optional\n",
        "        Size of the figure. Default is (8, 4).\n",
        "    path_color : str, optional\n",
        "        Color of the paths in the time series plot. Default is 'orange'.\n",
        "    hist_color : str, optional\n",
        "        Color of the histogram. Default is 'orange'.\n",
        "    **kwargs : dict\n",
        "        Additional keyword arguments to pass to the plotting functions.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    fig : matplotlib.figure.Figure\n",
        "        The figure object containing the plots.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the expected value (mean) across all draws and chains for each date\n",
        "    expected_value = posterior.mean(dim=(\"draw\", \"chain\"))\n",
        "\n",
        "    # Create a figure and a grid of subplots\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    gs = fig.add_gridspec(1, 2, width_ratios=[3, 1])\n",
        "\n",
        "    # Time series plot\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    for chain in range(posterior.shape[1]):\n",
        "        for draw in range(0, posterior.shape[0], 10):  # Plot every 10th draw for performance\n",
        "            ax1.plot(posterior.date, posterior[draw, chain], color=path_color, alpha=0.1, linewidth=0.5)\n",
        "\n",
        "    ax1.plot(posterior.date, expected_value, color='grey', linestyle='--', linewidth=2)\n",
        "    ax1.plot(posterior.date, y_real, color='black', linestyle='-', linewidth=2, label='Real',)\n",
        "    ax1.set_title(\"Posterior Predictive\")\n",
        "    ax1.set_xlabel('Date')\n",
        "    ax1.set_ylabel('Value')\n",
        "    ax1.grid(True)\n",
        "    ax1.legend()\n",
        "\n",
        "    # KDE plot\n",
        "    ax2 = fig.add_subplot(gs[1])\n",
        "    final_values = posterior[:, :, -1].values.flatten()\n",
        "\n",
        "    # Use seaborn for KDE plot\n",
        "    sns.kdeplot(y=final_values, ax=ax2, fill=True, color=hist_color, alpha=0.6, **kwargs)\n",
        "\n",
        "    # Add histogram on top of KDE\n",
        "    ax2.hist(final_values, orientation='horizontal', color=hist_color, bins=30,\n",
        "             alpha=0.3, density=True)\n",
        "\n",
        "    ax2.axhline(y=expected_value[-1], color='grey', linestyle='--', linewidth=2)\n",
        "    ax2.set_title('Distribution at T')\n",
        "    ax2.set_xlabel('Density')\n",
        "    ax2.set_yticklabels([])  # Hide y tick labels to avoid duplication\n",
        "    ax2.grid(True)\n",
        "    return fig\n",
        "\n",
        "plot_posterior(\n",
        "    df[\"x3_adstock_saturated\"].values,\n",
        "    mmm.idata.posterior.channel_contributions.sel(channel=\"x3\") * df[\"y\"].max(),\n",
        "    path_color='lightblue',\n",
        "    hist_color='lightblue'\n",
        ");"
      ],
      "id": "4e8fca68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The effect was recovered perfectly, using this model, we can safely inform how much we'll get back if we invest in x3. Nevertheless, we need to control by holiday and inflation to get the total effect. What happen if we don't have this control variables?\n",
        "\n",
        "# How to get right estimates if we don't have all the covariates?\n",
        "\n",
        "If we are confident in our data generative process we can be sure that by surgically excluding a node, a Gaussian process can absorb such variability. Let's see how this works in practice.\n"
      ],
      "id": "f04bc4e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mmm = MMM(\n",
        "    model_config=model_config,\n",
        "    date_column=\"date\",\n",
        "    channel_columns=[\n",
        "        \"x1\",\n",
        "        \"x2\",\n",
        "        # \"x3\"\n",
        "    ],\n",
        "    adstock=GeometricAdstock(l_max=24),\n",
        "    saturation=MichaelisMentenSaturation(),\n",
        "    time_varying_intercept=True,\n",
        ")\n",
        "\n",
        "mmm.model_config[\"intercept_tvp_config\"].ls_mu = 15\n",
        "mmm.model_config[\"intercept_tvp_config\"].m = 200\n",
        "\n",
        "mmm.fit(X, y, **fit_kwargs)\n",
        "mmm.sample_posterior_predictive(\n",
        "    X=X,\n",
        "    extend_idata=True,\n",
        "    combined=True,\n",
        "    random_seed=rng,\n",
        ")\n",
        "\n",
        "az.summary(mmm.idata, var_names=[\"saturation_alpha\", \"saturation_lam\", \"adstock_alpha\",])"
      ],
      "id": "73d713c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see by the parameters of the model is able to recover the effect of x2 on y, even though we removed x3 from the model.\n"
      ],
      "id": "367060c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_posterior(\n",
        "    df[\"x2_adstock_saturated\"].values,\n",
        "    mmm.idata.posterior.channel_contributions.sel(channel=\"x2\") * df[\"y\"].max(),\n",
        "    path_color='lightgreen',\n",
        "    hist_color='lightgreen'\n",
        ");"
      ],
      "id": "0c3f44bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the effect of x2 on y is recovered, even though we removed control variables from the model, and use a gaussian process to account for the variability of the data instead.\n",
        "\n",
        "# Conclusions\n",
        "\n",
        "1. Don't seek the one single model: The real world is very dynamic, before its possible that the one single model it doesn't exist.\n",
        "\n",
        "2. \"Find\" the Causal Truth: Dive into the world of causal structures and learn how to map out the hidden pathways that influence your outcomes. Not consider causal structures will drive you to consider more simple causal structures, which can be problematic in a real world enviroment.\n",
        "\n",
        "3. Embrace Model Evolution: Don't get too attached to your first model! As we saw in our DAG progression, models can (and should) change as we learn more. Starting simple is fine, but be ready to level up your model game when the data shows there's more to the story.\n",
        "\n",
        "# Our causal discovery process in a nutshell\n",
        "\n",
        "During the notebook, we have seen how we can use bayesian regression models to identify the causal structure of a dataset, and how we can use this information to make better decisions. We have also seen how we can use this information to make better decisions. In short, we start with a simple naive understanding of the world, which was evolved through the identification of the causal structure of the data, and the use of the causal graph to make better modelling decisions.\n"
      ],
      "id": "0b2599a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a figure with five subplots\n",
        "fig, axes = plt.subplots(1, 5,)\n",
        "\n",
        "# Set titles for each subplot\n",
        "titles = [\"Naive DAG\", \"Updated DAG\", \"Updated DAG 1\", \"Discovered DAG\", \"True DAG\"]\n",
        "for ax, title in zip(axes, titles):\n",
        "    ax.set_title(title, fontsize=6)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Render and plot each graph\n",
        "naive_causal_mmm_graph.render(format='png', filename='images/naive_dag')\n",
        "axes[0].imshow(mpimg.imread('images/naive_dag.png'))\n",
        "\n",
        "updated_naive_causal_mmm_graph.render(format='png', filename='images/updated_dag')\n",
        "axes[1].imshow(mpimg.imread('images/updated_dag.png'))\n",
        "\n",
        "updated_naive_causal_mmm_graph1.render(format='png', filename='images/updated_dag1')\n",
        "axes[2].imshow(mpimg.imread('images/updated_dag1.png'))\n",
        "\n",
        "real_dag_graph = causal_model.to_graphviz(ges_graph)\n",
        "real_dag_graph.render(format='png', filename='images/discovered_dag')\n",
        "axes[3].imshow(mpimg.imread('images/discovered_dag.png'))\n",
        "\n",
        "new_real_dag.render(format='png', filename='images/true_dag')\n",
        "axes[4].imshow(mpimg.imread('images/true_dag.png'))\n",
        "\n",
        "# Add main title\n",
        "plt.suptitle(\"Comparison of DAG Graphs\", fontsize=24)\n",
        "plt.tight_layout()"
      ],
      "id": "73187b56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Last update:\n"
      ],
      "id": "c5fd5087"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext watermark\n",
        "%watermark -n -u -v -iv -w -p pymc_marketing,pytensor"
      ],
      "id": "88bd427d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "cetagostini_web",
      "language": "python",
      "display_name": "cetagostini_web",
      "path": "/Users/carlostrujillo/Library/Jupyter/kernels/cetagostini_web"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}