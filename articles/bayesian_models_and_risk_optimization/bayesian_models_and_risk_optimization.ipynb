{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Bayesian Models and Risk Optimization\"\n",
        "date: \"2025-08-20\"\n",
        "categories: [python, experimentation, media mix modeling, mmm, bayesian, pymc, pydata, germany, berlin]\n",
        "image: \"../images/bayesian_models_and_risk_optimization.png\"\n",
        "jupyter: cetagostini_web\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-tools: true\n",
        "    code-overflow: wrap\n",
        "---\n",
        "\n",
        "# üìò Introduction\n",
        "\n",
        "This article explores how Bayesian Media Mix Modeling (MMM) represents uncertainty and how we can optimize budget decisions under risk. We build a generative view of media response (carryover via adstock, diminishing returns via saturation, trend and seasonality) and use full posterior predictive distributions to compare allocations not only by expected outcomes but also by dispersion and tail risk.\n",
        "\n",
        "This material accompanies my PyData Berlin 2025 talk, where I discuss practical risk-aware optimization for MMM: moving beyond mean-only plans to objectives that explicitly incorporate uncertainty‚Äîand how to communicate these trade-offs to stakeholders.\n",
        "\n",
        "# üì¶ Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from pymc_marketing.mmm.builders.yaml import build_mmm_from_yaml\n",
        "from pymc_marketing.mmm import GeometricAdstock, MichaelisMentenSaturation\n",
        "from pymc_marketing.mmm.budget_optimizer import optimizer_xarray_builder\n",
        "from pymc_marketing.mmm.multidimensional import (\n",
        "    MultiDimensionalBudgetOptimizerWrapper,\n",
        ")\n",
        "from pymc_marketing.prior import Prior\n",
        "from pymc_marketing.mmm import utility as ut\n",
        "\n",
        "from scipy import ndimage\n",
        "\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚öôÔ∏è Notebook setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "az.style.use(\"arviz-darkgrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = [8, 4]\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "plt.rcParams[\"axes.labelsize\"] = 6\n",
        "plt.rcParams[\"xtick.labelsize\"] = 6\n",
        "plt.rcParams[\"ytick.labelsize\"] = 6\n",
        "plt.rcParams.update({\"figure.constrained_layout.use\": True})\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "seed: int = sum(map(ord, \"pydata_berlin_2025\"))\n",
        "rng: np.random.Generator = np.random.default_rng(seed=seed)\n",
        "default_figsize = (8, 4) # repeat to use later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Data generation process\n",
        "\n",
        "This section encodes a generative story for media response with clear uncertainty sources. We simulate outcomes as\n",
        "$$\n",
        "y_t = \\beta_0 + \\sum_{c} f(x_{c,t}; \\theta_c) + \\text{trend}_t + \\text{seasonality}_t + \\varepsilon_t\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $y_t$ is the observed outcome (e.g., app installs or revenue) at time $t$\n",
        "- $\\beta_0$ is the baseline intercept\n",
        "- $f(x_{c,t}; \\theta_c)$ is the media response function for channel $c$ with spend $x_{c,t}$ and parameters $\\theta_c$.\n",
        "- $\\text{trend}_t$ captures long-term growth or decline patterns\n",
        "- $\\text{seasonality}_t$ models periodic effects (weekly, monthly patterns)\n",
        "- $\\varepsilon_t$ represents aleatoric uncertainty‚Äîirreducible noise from unobserved factors, measurement error, and inherent randomness that remains even with perfect knowledge of all parameters\n",
        "\n",
        "We do not consider interactions; the causal DAG looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "import graphviz\n",
        "\n",
        "graph = graphviz.Digraph()\n",
        "graph.node(\"Media Spend\")\n",
        "graph.node(\"Trend\", style=\"dashed\")\n",
        "graph.node(\"Seasonality\", style=\"dashed\")\n",
        "graph.node(\"Target\")\n",
        "\n",
        "graph.edge(\"Media Spend\", \"Target\")\n",
        "graph.edge(\"Trend\", \"Target\")\n",
        "graph.edge(\"Seasonality\", \"Target\")\n",
        "\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÜ Date range\n",
        "We start by defining the date range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# date range\n",
        "min_date = pd.to_datetime(\"2024-09-01\")\n",
        "max_date = pd.to_datetime(\"2025-09-01\")\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    data={\"date_week\": pd.date_range(start=min_date, end=max_date, freq=\"W-MON\")}\n",
        ")\n",
        "\n",
        "n = df.shape[0]\n",
        "print(f\"Number of observations: {n}\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì£ Media data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# media data\n",
        "scaler_x1 = 300\n",
        "scaler_x2 = 280\n",
        "scaler_x3 = 50\n",
        "scaler_x4 = 100\n",
        "y_scaler = 1000\n",
        "\n",
        "# media data\n",
        "x1 = rng.uniform(low=0.0, high=1.0, size=n)\n",
        "df[\"x1\"] = np.where(x1 > 0.8, x1, x1 / 2)\n",
        "\n",
        "x2 = rng.uniform(low=0.0, high=0.6, size=n)\n",
        "df[\"x2\"] = np.where(x2 > 0.5, x2, 0)\n",
        "\n",
        "x3 = rng.uniform(low=0.0, high=0.8, size=n)\n",
        "df[\"x3\"] = np.where(x3 > 0.7, x3, x3 / 6)\n",
        "\n",
        "x4 = rng.uniform(low=0.0, high=0.2, size=n)\n",
        "df[\"x4\"] = np.where(x4 > 0.15, x4, x4 / 2)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(\n",
        "    nrows=4, ncols=1, sharex=True, sharey=True, layout=\"constrained\"\n",
        ")\n",
        "sns.lineplot(x=\"date_week\", y=\"x1\", data=df, color=\"C0\", ax=ax[0])\n",
        "sns.lineplot(x=\"date_week\", y=\"x2\", data=df, color=\"C1\", ax=ax[1])\n",
        "sns.lineplot(x=\"date_week\", y=\"x3\", data=df, color=\"C2\", ax=ax[2])\n",
        "sns.lineplot(x=\"date_week\", y=\"x4\", data=df, color=\"C3\", ax=ax[3])\n",
        "ax[3].set(xlabel=\"date\")\n",
        "fig.suptitle(\"Media Costs Data\", fontsize=16);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Trend and seasonality components\n",
        "\n",
        "We define trend and seasonality. Seasonality follows a 4-week cycle modeled with a Fourier basis; trend is linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# Create Fourier components for monthly seasonality\n",
        "monthly_period = 4  # 4-week cycle\n",
        "t = np.arange(n)\n",
        "\n",
        "# Create sin-cos signals for fourier components\n",
        "monthly_sin = np.sin(2 * np.pi * t / monthly_period)\n",
        "monthly_cos = np.cos(2 * np.pi * t / monthly_period)\n",
        "\n",
        "# Combine sin-cos to create the desired pattern\n",
        "# Use coefficients to shape the pattern\n",
        "monthly_pattern = 0.6 * monthly_sin + 0.4 * monthly_cos\n",
        "\n",
        "# Apply smoothing using ndimage to reduce sharp transitions\n",
        "monthly_pattern = ndimage.gaussian_filter1d(monthly_pattern, sigma=0.2)\n",
        "\n",
        "# Normalize to [-1, 1] range\n",
        "monthly_pattern = (monthly_pattern / np.max(np.abs(monthly_pattern))) * .18\n",
        "\n",
        "df[\"monthly_effect\"] = monthly_pattern\n",
        "df[\"trend\"] = (np.linspace(start=0.0, stop=10, num=n) + 10) ** (1 / 8) - 1\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "\n",
        "# Plot monthly pattern\n",
        "ax1.plot(df[\"date_week\"], monthly_pattern, label=\"Monthly Pattern (Smoothed)\", linewidth=2, color='blue')\n",
        "ax1.set_ylabel(\"Pattern Value\")\n",
        "ax1.set_title(\"Monthly Fourier Pattern (4-week cycle, Smoothed)\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot trend\n",
        "ax2.plot(df[\"date_week\"], df[\"trend\"], label=\"Trend\", linewidth=2, color='red')\n",
        "ax2.set_xlabel(\"Date\")\n",
        "ax2.set_ylabel(\"Trend Value\")\n",
        "ax2.set_title(\"Linear Trend Component\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÅ Adstock and saturation transformations\n",
        "\n",
        "First, we apply the adstock transformation to the media data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# apply geometric adstock transformation\n",
        "alpha: float = 0.55\n",
        "\n",
        "df[\"x1_adstock\"] = (\n",
        "    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x1\"].to_numpy(), alpha=alpha)\n",
        "    .eval()\n",
        ")\n",
        "\n",
        "df[\"x2_adstock\"] = (\n",
        "    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x2\"].to_numpy(), alpha=alpha)\n",
        "    .eval()\n",
        ")\n",
        "\n",
        "df[\"x3_adstock\"] = (\n",
        "    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x3\"].to_numpy(), alpha=alpha)\n",
        "    .eval()\n",
        ")\n",
        "\n",
        "df[\"x4_adstock\"] = (\n",
        "    GeometricAdstock(l_max=8, normalize=True).function(x=df[\"x4\"].to_numpy(), alpha=alpha)\n",
        "    .eval()\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we apply the saturation transformation to the adstock transformed media data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "alpha_sat_x1: float = 0.3\n",
        "lam_sat_x1: float = 1.1\n",
        "\n",
        "alpha_sat_x2: float = 0.1\n",
        "lam_sat_x2: float = 1.5\n",
        "\n",
        "alpha_sat_x3: float = 0.2\n",
        "lam_sat_x3: float = 0.3\n",
        "\n",
        "alpha_sat_x4: float = 0.8\n",
        "lam_sat_x4: float = 0.8\n",
        "\n",
        "df[\"x1_adstock_saturated\"] = (\n",
        "    MichaelisMentenSaturation().function(\n",
        "        x=df[\"x1_adstock\"].to_numpy(),\n",
        "        alpha=alpha_sat_x1,\n",
        "        lam=lam_sat_x1,\n",
        "    ).eval()\n",
        ")\n",
        "\n",
        "df[\"x2_adstock_saturated\"] = (\n",
        "    MichaelisMentenSaturation().function(\n",
        "        x=df[\"x2_adstock\"].to_numpy(),\n",
        "        alpha=alpha_sat_x2,\n",
        "        lam=lam_sat_x2,\n",
        "    ).eval()\n",
        ")\n",
        "\n",
        "df[\"x3_adstock_saturated\"] = (\n",
        "    MichaelisMentenSaturation().function(\n",
        "        x=df[\"x3_adstock\"].to_numpy(),  \n",
        "        alpha=alpha_sat_x3,\n",
        "        lam=lam_sat_x3,\n",
        "    ).eval()\n",
        ")\n",
        "\n",
        "df[\"x4_adstock_saturated\"] = (\n",
        "    MichaelisMentenSaturation().function(\n",
        "        x=df[\"x4_adstock\"].to_numpy(),\n",
        "        alpha=alpha_sat_x4,\n",
        "        lam=lam_sat_x4,\n",
        "    ).eval()\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize how the media data look after adstock and saturation, and how they translate into units of Y (app installs or revenue)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "fig, ax = plt.subplots(\n",
        "    nrows=3, ncols=4, sharex=True, sharey=False, layout=\"constrained\"\n",
        ")\n",
        "sns.lineplot(x=\"date_week\", y=\"x1\", data=df, color=\"C0\", ax=ax[0, 0])\n",
        "sns.lineplot(x=\"date_week\", y=\"x2\", data=df, color=\"C1\", ax=ax[0, 1])\n",
        "sns.lineplot(x=\"date_week\", y=\"x1_adstock\", data=df, color=\"C0\", ax=ax[1, 0])\n",
        "sns.lineplot(x=\"date_week\", y=\"x2_adstock\", data=df, color=\"C1\", ax=ax[1, 1])\n",
        "sns.lineplot(x=\"date_week\", y=\"x1_adstock_saturated\", data=df, color=\"C0\", ax=ax[2, 0])\n",
        "sns.lineplot(x=\"date_week\", y=\"x2_adstock_saturated\", data=df, color=\"C1\", ax=ax[2, 1])\n",
        "sns.lineplot(x=\"date_week\", y=\"x3\", data=df, color=\"C2\", ax=ax[0, 2])\n",
        "sns.lineplot(x=\"date_week\", y=\"x3_adstock\", data=df, color=\"C2\", ax=ax[1, 2])\n",
        "sns.lineplot(x=\"date_week\", y=\"x3_adstock_saturated\", data=df, color=\"C2\", ax=ax[2, 2])\n",
        "sns.lineplot(x=\"date_week\", y=\"x4\", data=df, color=\"C3\", ax=ax[0, 3])\n",
        "sns.lineplot(x=\"date_week\", y=\"x4_adstock\", data=df, color=\"C3\", ax=ax[1, 3])\n",
        "sns.lineplot(x=\"date_week\", y=\"x4_adstock_saturated\", data=df, color=\"C3\", ax=ax[2, 3])\n",
        "fig.suptitle(\"Media Costs Data - Transformed\", fontsize=16);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now add the intercept and noise, and sum the transformed media, trend, and seasonality components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "df[\"intercept\"] = 0.15\n",
        "df[\"epsilon\"] = rng.normal(loc=0.0, scale=0.075, size=n)\n",
        "\n",
        "df[\"app_installs\"] = df[[\"intercept\", \"x1_adstock_saturated\", \"x2_adstock_saturated\", \"x3_adstock_saturated\", \"x4_adstock_saturated\", \"trend\", \"monthly_effect\", \"epsilon\"]].sum(axis=1)\n",
        "df[\"app_installs\"] *= y_scaler\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is how the target looks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "df.set_index(\"date_week\").app_installs.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also add the original media to the DataFrame so we can visualize it before any transformations and use it as model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "df[[\"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"x4_original_scale\"]] = df[[\"x1\", \"x2\", \"x3\", \"x4\"]]\n",
        "df[\"x1_original_scale\"] *= scaler_x1\n",
        "df[\"x2_original_scale\"] *= scaler_x2\n",
        "df[\"x3_original_scale\"] *= scaler_x3\n",
        "df[\"x4_original_scale\"] *= scaler_x4\n",
        "\n",
        "df[[\"date_week\", \"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"app_installs\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è Building the model\n",
        "\n",
        "The YAML configuration encodes a fully Bayesian MMM with priors over the core response mechanics and temporal structure. For model specifics and worked examples, see the PyMC‚ÄëMarketing [Example Gallery](https://www.pymc-marketing.io/en/stable/gallery/gallery.html) and [API](https://www.pymc-marketing.io/en/stable/api/index.html).\n",
        "\n",
        "Here we split the data into train and test sets. Not to evaluate the fit; instead, we use the test set to compare the results of the optimization, checking if it will be \"better\" than the budget recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "df_train = df.query(\"date_week <= '2025-08-30'\").copy()\n",
        "x_train = df_train[[\"date_week\", \"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"x4_original_scale\"]]\n",
        "y_train = df_train[\"app_installs\"]\n",
        "\n",
        "df_test = df.query(\"date_week > '2025-08-30'\").copy()\n",
        "x_test = df_test[[\"date_week\", \"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"x4_original_scale\"]]\n",
        "y_test = df_test[\"app_installs\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the model was defined previously in the YAML, building it is straightforward and takes only a few lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "mmm = build_mmm_from_yaml(\n",
        "    X=x_train,\n",
        "    y=y_train,\n",
        "    config_path=\"pymc_model.yml\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we fit the model and check convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "mmm.fit(\n",
        "    X=x_train,\n",
        "    y=y_train,\n",
        "    random_seed=rng,\n",
        ")\n",
        "\n",
        "mmm.sample_posterior_predictive(\n",
        "    X=x_train,\n",
        "    extend_idata=True,\n",
        "    combined=True,\n",
        "    random_seed=rng,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, no divergences üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "mmm.idata.sample_stats.diverging.sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we inspect the parameters relevant for optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "media_vars = [\n",
        "    \"saturation_alpha\",\n",
        "    \"saturation_lam\",\n",
        "    \"adstock_alpha\",\n",
        "]\n",
        "\n",
        "_ = az.plot_trace(\n",
        "    data=mmm.fit_result,\n",
        "    var_names=media_vars,\n",
        "    compact=True,\n",
        "    backend_kwargs={\"figsize\": default_figsize, \"layout\": \"constrained\"},\n",
        ")\n",
        "plt.gcf().suptitle(\"Model Trace\", fontsize=16, fontweight=\"bold\", y=1.03);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, some parameters are well identified while others remain uncertain.\n",
        "\n",
        "Sampling saturation curves from the posterior helps us visualize parameter uncertainty as bands around each channel‚Äôs response. Wide bands indicate poorly identified marginal returns; allocating in those regions increases outcome variance because small parameter shifts cause large changes in response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "curve = mmm.saturation.sample_curve(\n",
        "    mmm.idata.posterior[[\"saturation_alpha\", \"saturation_lam\"]], max_value=3\n",
        ")\n",
        "\n",
        "fig, axes = mmm.plot.saturation_curves(\n",
        "    curve,\n",
        "    original_scale=True,\n",
        "    n_samples=10,\n",
        "    hdi_probs=0.85,\n",
        "    random_seed=rng,\n",
        "    subplot_kwargs={\"figsize\": default_figsize, \"ncols\": 4, \"sharey\": True},\n",
        "    rc_params={\n",
        "        \"xtick.labelsize\": 10,\n",
        "        \"ytick.labelsize\": 10,\n",
        "        \"axes.labelsize\": 10,\n",
        "        \"axes.titlesize\": 10,\n",
        "    },\n",
        "\n",
        ")\n",
        "\n",
        "for ax in axes.ravel():\n",
        "    ax.title.set_fontsize(10)\n",
        "\n",
        "if fig._suptitle is not None:\n",
        "    fig._suptitle.set_fontsize(12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The picture is clear: channels like X1 and X3 exhibit more variation across spend levels, allowing the model to learn their parameters more precisely. Channels like X2 and X4 have relatively constant spending with less variation, so their parameters are estimated with greater uncertainty.\n",
        "\n",
        "# üé≤ Understanding uncertainty\n",
        "\n",
        "In a Bayesian MMM we explicitly model two forms of uncertainty that compound in forecasts and in budget decisions:\n",
        "\n",
        "- Aleatoric uncertainty: randomness in outcomes conditional on fixed parameters. In the simulation, this is `epsilon`. Formally, if parameters are $\\theta$, aleatoric uncertainty is the spread of $p(y\\mid x,\\theta)$ once we model the likelihood as $\\mathcal{N}(0, \\sigma^2)$. In our model, the parameter $\\sigma$ explicitly captures aleatoric uncertainty: it quantifies the amount of outcome variability that remains even if all structural parameters $\\theta$ were known exactly. It represents inherent unpredictability due to unobserved micro-variation, demand shocks, or logging noise. Even with infinite data, aleatoric uncertainty remains.\n",
        "\n",
        "- Epistemic uncertainty: uncertainty about the parameters and latent functions themselves due to limited or weakly informative data. This is the spread of the posterior $p(\\theta\\mid \\text{data})$. It shrinks with more data, better priors, or richer experimental variation. In our model it includes carryover memory (adstock $\\alpha$), saturation curvature and half-saturation (Michaelis‚ÄìMenten $\\alpha,\\lambda$), trend slopes, and seasonal Fourier weights.\n",
        "\n",
        "Why this separation matters for planning:\n",
        "\n",
        "1) Outcome distribution under a plan. For a given allocation plan $b$ over channels and time, the posterior predictive is\n",
        "$$\n",
        "p\\big(Y(b)\\mid \\text{data}\\big) = \\int p\\big(Y(b)\\mid \\theta\\big)\\, p(\\theta\\mid \\text{data})\\, d\\theta,\n",
        "$$\n",
        "which mixes aleatoric variability (the inner term) and epistemic variability (integration over $\\theta$). Our Monte Carlo estimator samples $\\theta^{(s)}$ from the posterior, simulates carryover and saturation under $b$, and draws predictive outcomes.\n",
        "\n",
        "2) Example: known vs unknown curvature. Suppose a channel‚Äôs saturation is well learned around historical spends but not beyond. Two plans with the same total spend differ in risk:\n",
        "\n",
        "   - Plan A concentrates around the historical mode (epistemic low), yielding a narrow predictive distribution.\n",
        "   - Plan B pushes beyond observed spends (epistemic high), producing a wider distribution and heavier downside tails if the curve flattens earlier than hoped.\n",
        "\n",
        "During optimization, we focus on the second component ‚Äîepistemic uncertainty‚Äî and can choose how much confidence we require around it. Once we choose an allocation, then we incorporate aleatoric uncertainty to quantify the total response distribution, if we want to.\n",
        "\n",
        "# üß≠ Optimization\n",
        "\n",
        "Because the posterior predictive distribution\n",
        "$$\n",
        "p\\big(Y(b)\\mid \\text{data}\\big) = \\int p\\big(Y(b)\\mid \\theta\\big)\\, p(\\theta\\mid \\text{data})\\, d\\theta\n",
        "$$\n",
        "already incorporates both aleatoric and epistemic uncertainty, the optimization problem reduces to choosing an allocation $b$ that optimizes a scalar summary of this distribution.\n",
        "\n",
        "Formally, let $b \\in \\mathbb{R}^C$ denote a feasible allocation (e.g., channel budgets) with constraints\n",
        "$$\n",
        "\\sum_{c=1}^C b_c = B, \\qquad \\underline{b}_c \\leq b_c \\leq \\overline{b}_c.\n",
        "$$\n",
        "\n",
        "For each candidate allocation $b$, we obtain Monte Carlo draws\n",
        "$$\n",
        "\\{Y^{(s)}(b)\\}_{s=1}^S \\sim p(Y \\mid \\mathrm{do}(X=b), \\mathcal{D}),\n",
        "$$\n",
        "\n",
        "from the posterior predictive distribution. A statistic\n",
        "$$\n",
        "\\phi\\!\\left(\\{Y^{(s)}(b)\\}\\right)\n",
        "$$\n",
        "\n",
        "is then computed (for example, the mean, a quantile, a risk-adjusted score, or the mean tightness score).  \n",
        "\n",
        "By consequence, the optimization problem solved by SLSQP is simply\n",
        "$$\n",
        "\\min_{b \\in \\mathcal{B}} J(b), \\qquad J(b) = f\\!\\left(\\phi(\\{Y^{(s)}(b)\\})\\right),\n",
        "$$\n",
        "\n",
        "where $f$ is defined so the solver minimizes the chosen statistic from the posterior predictive distribution.  \n",
        "\n",
        "This formulation is flexible: by changing $\\phi$, we can target risk-neutral or risk-sensitive criteria, while always grounding the decision in the posterior predictive distribution.\n",
        "\n",
        "::: {.callout-note}\n",
        "## üí° Assumption notes\n",
        "Why do we say do=$(X=b)$?\n",
        "\n",
        "- **Structural invariance**: The response functions (trend, seasonality, adstock, saturation, link) are invariant under interventions $b$ over the optimization horizon.\n",
        "- **No unmeasured confounding**: Conditional on included covariates and time controls, there are no unmeasured (especially time-varying) confounders affecting both spend and outcome; the backdoor criterion holds.\n",
        "\n",
        "We use $p\\big(Y \\mid \\mathrm{do}(X=b), \\mathcal{D}\\big)$ as shorthand for the posterior predictive under these assumptions. When allocations move far outside support, results become extrapolative and should be treated as sensitivity analysis rather than identified effects.\n",
        ":::\n",
        "\n",
        "## üõ†Ô∏è Define the optimizer\n",
        "\n",
        "Initializing the optimizer is straightforward: pass the model and the date range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "optimizable_model = MultiDimensionalBudgetOptimizerWrapper(\n",
        "    model=mmm, \n",
        "    start_date=df_test.date_week.min().strftime(\"%Y-%m-%d\"), \n",
        "    end_date=df_test.date_week.max().strftime(\"%Y-%m-%d\")\n",
        ")\n",
        "print(f\"Start date: {optimizable_model.start_date}\")\n",
        "print(f\"End date: {optimizable_model.end_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the test set to define the budget and optimization period so we can compare the resulting allocation to our current plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false \n",
        "channels = [\"x1_original_scale\", \"x2_original_scale\", \"x3_original_scale\", \"x4_original_scale\"]\n",
        "num_periods = optimizable_model.num_periods\n",
        "time_unit_budget = df_test[channels].sum(axis=1).mean()\n",
        "print(f\"Total budget to allocate: {num_periods * time_unit_budget:,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the budget and channels, we can estimate the response for our initial plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "initial_budget = df_test[channels].sum(axis=0).to_xarray().rename({\"index\":\"channel\"})\n",
        "initial_posterior_response = optimizable_model.sample_response_distribution(\n",
        "    allocation_strategy=initial_budget,\n",
        "    include_carryover=True,\n",
        "    include_last_observations=False,\n",
        "    additional_var_names=[\"y_original_scale\"]\n",
        ")\n",
        "\n",
        "fig, ax = optimizable_model.plot.budget_allocation(\n",
        "    samples=initial_posterior_response,\n",
        "    figsize=default_figsize,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bar chart shows allocation and response per channel. To see totals, we can sum and create a simple scatter plot with a label for the ROAS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# Create scatterplot with spend and mean response\n",
        "spend = initial_posterior_response.allocation.sum().values\n",
        "mean_response = initial_posterior_response.total_media_contribution_original_scale.mean(dim='sample').values\n",
        "\n",
        "initial_roas = mean_response / spend\n",
        "\n",
        "plt.scatter(spend, mean_response, alpha=0.7, s=100, label=f\"ROAS: {initial_roas:.2f}\")\n",
        "plt.xlabel('Spend')\n",
        "plt.ylabel('Mean Response')\n",
        "plt.title('Spend vs Mean Response')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(fontsize='small', loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We got a ROAS of $3.9$ for the initial plan, which is below our target ROAS (let's say $8$). Now we run a vanilla optimization to maximize mean response: can we reallocate to achieve a higher response given the same budget?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "allocation_strategy, optimization_result = optimizable_model.optimize_budget(\n",
        "    budget=time_unit_budget,\n",
        ")\n",
        "\n",
        "naive_posterior_response = optimizable_model.sample_response_distribution(\n",
        "    allocation_strategy=allocation_strategy,\n",
        "    include_carryover=True,\n",
        "    include_last_observations=False,\n",
        "    additional_var_names=[\"y_original_scale\"]\n",
        ")\n",
        "\n",
        "print(\"Budget allocation by channel:\")\n",
        "for channel in channels:\n",
        "    print(\n",
        "        f\"  {channel}: {naive_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n",
        "    )\n",
        "print(\n",
        "    f\"Total Allocated Budget: {np.sum(naive_posterior_response.allocation.to_numpy()):,.0f}\"\n",
        ")\n",
        "\n",
        "fig, ax = optimizable_model.plot.budget_allocation(\n",
        "    samples=naive_posterior_response,\n",
        "    figsize=default_figsize,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yes, we can allocate to achieve a higher response. Let's compare the optimized response against the baseline plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# Create scatterplot with spend and mean response\n",
        "mean_response_v2 = naive_posterior_response.total_media_contribution_original_scale.mean(dim='sample').values\n",
        "roas_v2 = mean_response_v2 / spend\n",
        "\n",
        "# Calculate the delta in response\n",
        "response_delta = mean_response_v2.sum() - mean_response.sum()\n",
        "\n",
        "plt.scatter(spend, mean_response_v2, alpha=0.7, s=100, color=\"blue\", label=f\"Optimized allocation (+{response_delta:.1f} response, ROAS: {roas_v2:.2f})\")\n",
        "plt.scatter(spend, mean_response, alpha=0.7, s=100, color=\"red\", label=\"Guessed allocation\")\n",
        "\n",
        "plt.xlabel('Spend')\n",
        "plt.ylabel('Mean Response')\n",
        "plt.title('Spend vs Mean Response')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(fontsize='small', loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optimized allocation is ~$500$ units higher than the guessed allocation, and the new estimated ROAS is $13$ which it's over our expectations. As consequence, we assume we'll get an estimate Y revenue in the next N periods and planning against this incoming cashflow we'll get back.\n",
        "\n",
        "The plotwist? We got a lower response, which mean a lower ROAS and we got in serious financial problems because we don't have enough cash to payback providers or services.\n",
        "\n",
        "Why this happen? Maximizing the mean response is risk-neutral. It often reallocates budget toward regions with potential high returns even if they are weakly identified, increasing dispersion of outcomes. This is rational when stakeholders are indifferent to risk. However, that's not the case for every company. Sometimes our stakeholders need to know how certain we are about expected outcomes.\n",
        "\n",
        "By inspecting posterior predictive samples under each allocation, we can quantify uncertainty and answer this question, based on the model current understanding.\n",
        "\n",
        "Let's plot the response distributions for both baseline and optimized allocations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Get the values\n",
        "optimized_values = naive_posterior_response.total_media_contribution_original_scale.values\n",
        "guessed_values = initial_posterior_response.total_media_contribution_original_scale.values\n",
        "\n",
        "# Plot distributions\n",
        "az.plot_dist(\n",
        "    optimized_values,\n",
        "    color=\"blue\",\n",
        "    label=\"Optimized allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "az.plot_dist(\n",
        "    guessed_values,\n",
        "    color=\"red\",\n",
        "    label=\"Guessed allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "# Calculate means\n",
        "optimized_mean = optimized_values.mean()\n",
        "guessed_mean = guessed_values.mean()\n",
        "\n",
        "# Add vertical lines for means\n",
        "ax.axvline(optimized_mean, color=\"blue\", linestyle=\"--\", alpha=0.8)\n",
        "ax.axvline(guessed_mean, color=\"red\", linestyle=\"--\", alpha=0.8)\n",
        "\n",
        "# Add text boxes with mean values\n",
        "ax.text(optimized_mean + 10, ax.get_ylim()[1] * 0.8, \n",
        "        f'Optimized Mean:\\n{optimized_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n",
        "        ha='left', va='center')\n",
        "\n",
        "ax.text(guessed_mean - 10, ax.get_ylim()[1] * 0.6, \n",
        "        f'Guessed Mean:\\n{guessed_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.7),\n",
        "        ha='right', va='center')\n",
        "\n",
        "plt.title(\"Response Distribution\")\n",
        "plt.xlabel(\"Total Media Contribution\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the means differ (we optimized to increase it), and so does the certainty around the mean. Like an excersise, lets observe how probable is to get a response higher and lower than the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "az.plot_posterior(\n",
        "    optimized_values,\n",
        "    figsize=default_figsize,\n",
        "    ref_val=optimized_mean,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This makes everything clear now, the chances of getting some higher or equal than the mean where 43% but the chances of getting some lower than the mean where 56%. It's no surprise that we got a lower response and a lower ROAS.\n",
        "\n",
        "::: {.callout-tip icon=false}\n",
        "## üí° First Takeaway\n",
        "Comparing full distributions makes risk transparent: width quantifies forecast reliability, skewness reveals asymmetry of upside vs downside, and overlaps show practical indistinguishability. \n",
        ":::\n",
        "\n",
        "\n",
        "Where this risk is coming from? The new allocation is riskier, but why? Let's look at each spend level relative to its saturation curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "curve = mmm.saturation.sample_curve(\n",
        "    mmm.idata.posterior[[\"saturation_alpha\", \"saturation_lam\"]], max_value=10\n",
        ")\n",
        "\n",
        "fig, axes = mmm.plot.saturation_curves(\n",
        "    curve,\n",
        "    original_scale=True,\n",
        "    n_samples=10,\n",
        "    hdi_probs=0.85,\n",
        "    random_seed=rng,\n",
        "    subplot_kwargs={\"figsize\": default_figsize, \"ncols\": 4, \"sharey\": True},\n",
        "    rc_params={\n",
        "        \"xtick.labelsize\": 10,\n",
        "        \"ytick.labelsize\": 10,\n",
        "        \"axes.labelsize\": 10,\n",
        "        \"axes.titlesize\": 10,\n",
        "    },\n",
        "\n",
        ")\n",
        "\n",
        "# Add vertical lines for optimal allocation on each subplot\n",
        "allocation_values = naive_posterior_response.allocation.values\n",
        "channel_names = naive_posterior_response.allocation.channel.values\n",
        "\n",
        "for i, (ax, allocation_value) in enumerate(zip(axes.ravel(), allocation_values)):\n",
        "    ax.axvline(allocation_value, color=\"red\", linestyle=\"--\", alpha=0.8, linewidth=2, label=\"Optimal allocation\")\n",
        "    ax.title.set_fontsize(10)\n",
        "\n",
        "if fig._suptitle is not None:\n",
        "    fig._suptitle.set_fontsize(12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vertical lines are located at the spend allocation given for each channel. For channels as x4, the model has few observations at those spend levels, so posterior bands are wide and the induced response distribution is diffuse. Risk-aware optimization tends to pull spend toward well-identified regions (often near inflection), trading a small mean decrease for a large reduction in variance.\n",
        "\n",
        "Could we understand this in advance? and if so, would we prefer a different type of allocation? Which get us closer to our objective in a safer way? -A narrower distribution with a slightly lower mean can be preferable when shortfall risk is costly-.\n",
        "\n",
        "The short answer is **definetly**. Let's create now a new optimization process that will be risk-aware. \n",
        "\n",
        "# ‚öñÔ∏è Risk metrics consistent with PyMC-Marketing\n",
        "\n",
        "Below are the definitions of two risk-aware metrics as implemented in the PyMC-Marketing library.\n",
        "\n",
        "---\n",
        "\n",
        "## üå™Ô∏è Tail Distance\n",
        "\n",
        "For a confidence level $\\gamma \\in (0,1)$,\n",
        "\n",
        "$$\n",
        "\\operatorname{TailDistance}_\\gamma(Y) \\;=\\; \n",
        "\\big|Q_{1-\\gamma}(Y) - \\mu\\big| \\;+\\; \\big|\\mu - Q_{\\gamma}(Y)\\big|,\n",
        "$$\n",
        "\n",
        "where \n",
        "- $\\mu = \\mathbb{E}[Y]$ is the mean (estimated by the sample mean),  \n",
        "- $Q_q(Y)$ is the $q$-th quantile of the distribution of $Y$.\n",
        "\n",
        "This metric measures the **spread of the distribution's tails around the mean**.  \n",
        "Larger values indicate wider tails and thus higher uncertainty.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Mean Tightness Score (MTS)\n",
        "\n",
        "The Mean Tightness Score combines the mean with a penalty for tail spread:\n",
        "\n",
        "$$\n",
        "\\operatorname{MTS}(Y; \\alpha, \\gamma) \\;=\\; \n",
        "\\mu \\;-\\; \\alpha \\cdot \\operatorname{TailDistance}_\\gamma(Y),\n",
        "$$\n",
        "\n",
        "where \n",
        "- $\\mu = \\mathbb{E}[Y]$ is the mean,  \n",
        "- $\\gamma$ is the confidence level used for the tail distance,  \n",
        "- $\\alpha > 0$ is a penalty weight that controls the trade-off between higher mean and tighter distribution.\n",
        "\n",
        "Interpretation:  \n",
        "- **Higher is better:** Allocations with higher means and smaller dispersion score better.  This provides a balance between risk-neutral (maximize mean) and risk-averse (penalize uncertainty) optimization.\n",
        "\n",
        "Let's optimize with the Mean Tightness Score and see the resulting allocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "mts_budget_allocation, mts_optimizer_result, callback_results = (\n",
        "    optimizable_model.optimize_budget(\n",
        "        budget=time_unit_budget,\n",
        "        utility_function=ut.mean_tightness_score(alpha=0.15),\n",
        "        callback=True,\n",
        "        minimize_kwargs={\"options\": {\"maxiter\": 2_000, \"ftol\": 1e-16}},\n",
        "    )\n",
        ")\n",
        "\n",
        "mts_posterior_response = optimizable_model.sample_response_distribution(\n",
        "    allocation_strategy=mts_budget_allocation,\n",
        "    include_carryover=True,\n",
        "    include_last_observations=False,\n",
        "    additional_var_names=[\"y_original_scale\"]\n",
        ")\n",
        "\n",
        "# Print budget allocation by channel\n",
        "print(\"Budget allocation by channel:\")\n",
        "for channel in channels:\n",
        "    print(\n",
        "        f\"  {channel}: {mts_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n",
        "    )\n",
        "print(\n",
        "    f\"Total Allocated Budget: {np.sum(mts_posterior_response.allocation.to_numpy()):,.0f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, it looks like the allocation shifts toward better-identified regions. Let's plot the saturation curves to see where the allocation lands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "curve = mmm.saturation.sample_curve(\n",
        "    mmm.idata.posterior[[\"saturation_alpha\", \"saturation_lam\"]], max_value=4\n",
        ")\n",
        "\n",
        "fig, axes = mmm.plot.saturation_curves(\n",
        "    curve,\n",
        "    original_scale=True,\n",
        "    n_samples=10,\n",
        "    hdi_probs=0.85,\n",
        "    random_seed=rng,\n",
        "    subplot_kwargs={\"figsize\": default_figsize, \"ncols\": 4, \"sharey\": True},\n",
        "    rc_params={\n",
        "        \"xtick.labelsize\": 10,\n",
        "        \"ytick.labelsize\": 10,\n",
        "        \"axes.labelsize\": 10,\n",
        "        \"axes.titlesize\": 10,\n",
        "    },\n",
        "\n",
        ")\n",
        "\n",
        "# Add vertical lines for optimal allocation on each subplot\n",
        "allocation_values = mts_posterior_response.allocation.values\n",
        "\n",
        "for i, (ax, allocation_value) in enumerate(zip(axes.ravel(), allocation_values)):\n",
        "    ax.axvline(allocation_value, color=\"red\", linestyle=\"--\", alpha=0.8, linewidth=2, label=\"Optimal allocation\")\n",
        "    ax.title.set_fontsize(10)\n",
        "\n",
        "if fig._suptitle is not None:\n",
        "    fig._suptitle.set_fontsize(12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, recommendations for every channel lie in well-known regions.\n",
        "\n",
        "The consequence: the posterior distribution narrows because budget concentrates in well-learned, less-nonlinear regions.\n",
        "\n",
        "Let's plot posterior distributions for this lower-risk allocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Get the values\n",
        "optimized_risk_values = mts_posterior_response.total_media_contribution_original_scale.values\n",
        "\n",
        "# Plot distributions\n",
        "az.plot_dist(\n",
        "    optimized_values,\n",
        "    color=\"blue\",\n",
        "    label=\"Optimized allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "az.plot_dist(\n",
        "    guessed_values,\n",
        "    color=\"red\",\n",
        "    label=\"Guessed allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "az.plot_dist(\n",
        "    optimized_risk_values,\n",
        "    color=\"green\",\n",
        "    label=\"Risk-adjusted allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "# Calculate means\n",
        "risk_adjusted_mean = optimized_risk_values.mean()\n",
        "\n",
        "# Add vertical lines for means\n",
        "ax.axvline(optimized_mean, color=\"blue\", linestyle=\"--\", alpha=0.8)\n",
        "ax.axvline(guessed_mean, color=\"red\", linestyle=\"--\", alpha=0.8)\n",
        "ax.axvline(risk_adjusted_mean, color=\"green\", linestyle=\"--\", alpha=0.8)\n",
        "\n",
        "# Add text boxes with mean values\n",
        "ax.text(optimized_mean + 10, ax.get_ylim()[1] * 0.8, \n",
        "        f'Optimized Mean:\\n{optimized_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n",
        "        ha='left', va='center')\n",
        "\n",
        "ax.text(guessed_mean - 10, ax.get_ylim()[1] * 0.6, \n",
        "        f'Guessed Mean:\\n{guessed_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.7),\n",
        "        ha='right', va='center')\n",
        "\n",
        "ax.text(risk_adjusted_mean - 10, ax.get_ylim()[1] * 0.4, \n",
        "        f'Risk-adjusted Mean:\\n{risk_adjusted_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7),\n",
        "        ha='right', va='center')\n",
        "\n",
        "plt.title(\"Response Distribution\")\n",
        "plt.xlabel(\"Total Media Contribution\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This makes it clear: we gained much more certainty‚Äîa risk-averse (lower-variance) response distribution. You may be thinking that playing in known regions tends to reduce the mean. Do you want to know why?\n",
        "\n",
        "Posterior response curves tend to be more certain at the origin because two sources of uncertainty are minimized there: structurally, we know that zero spend produces zero incremental effect, and empirically, the lowest spend region is often well supported in historical data. As spend increases, especially beyond historically observed levels, epistemic uncertainty about the saturation and curvature parameters dominates, widening the credible intervals.\n",
        "\n",
        "Does that mean we are doomed to lower values if we want certainty? Not at all, we can change the utility to prefer riskier options üî•.\n",
        "\n",
        "Let's run a more risk-seeking allocation üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "inverse_mts_budget_allocation, inverse_mts_optimizer_result, callback_results = (\n",
        "    optimizable_model.optimize_budget(\n",
        "        budget=time_unit_budget,\n",
        "        utility_function=ut.mean_tightness_score(alpha=0.95),\n",
        "        callback=True,\n",
        "        minimize_kwargs={\"options\": {\"maxiter\": 2_000, \"ftol\": 1e-16}},\n",
        "    )\n",
        ")\n",
        "\n",
        "inverse_mts_posterior_response = optimizable_model.sample_response_distribution(\n",
        "    allocation_strategy=inverse_mts_budget_allocation,\n",
        "    include_carryover=True,\n",
        "    include_last_observations=False,\n",
        "    additional_var_names=[\"y_original_scale\"]\n",
        ")\n",
        "\n",
        "# Print budget allocation by channel\n",
        "print(\"Budget allocation by channel:\")\n",
        "for channel in channels:\n",
        "    print(\n",
        "        f\"  {channel}: {inverse_mts_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n",
        "    )\n",
        "print(\n",
        "    f\"Total Allocated Budget: {np.sum(inverse_mts_posterior_response.allocation.to_numpy()):,.0f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Flipping the tightness preference (alpha parameter) induces risk-seeking behavior, moving allocations toward higher-variance, high-upside regions.\n",
        "\n",
        "Now we choose an allocation that is less certain but with higher potential upside than the baseline. Let's plot the response distributions. \n",
        "\n",
        "::: {.callout-tip icon=false}\n",
        "## üí° Second Takeaway\n",
        "Discover your risk preferences and adjust your objective function to reflect them. You don't need to commit to a single function or approach, you can build a custom one which tailor your needs.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Get the values\n",
        "optimized_inverse_risk_values = inverse_mts_posterior_response.total_media_contribution_original_scale.values\n",
        "\n",
        "# Plot distributions\n",
        "az.plot_dist(\n",
        "    optimized_values,\n",
        "    color=\"blue\",\n",
        "    label=\"Optimized allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "az.plot_dist(\n",
        "    guessed_values,\n",
        "    color=\"red\",\n",
        "    label=\"Guessed allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "az.plot_dist(\n",
        "    optimized_risk_values,\n",
        "    color=\"green\",\n",
        "    label=\"Risk-adjusted allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "az.plot_dist(\n",
        "    optimized_inverse_risk_values,\n",
        "    color=\"orange\",\n",
        "    label=\"Inverse Risk-adjusted allocation\",\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "# Calculate means\n",
        "inverse_risk_adjusted_mean = optimized_inverse_risk_values.mean()\n",
        "\n",
        "# Add vertical lines for means\n",
        "ax.axvline(optimized_mean, color=\"blue\", linestyle=\"--\", alpha=0.8)\n",
        "ax.axvline(guessed_mean, color=\"red\", linestyle=\"--\", alpha=0.8)\n",
        "ax.axvline(risk_adjusted_mean, color=\"green\", linestyle=\"--\", alpha=0.8)\n",
        "ax.axvline(inverse_risk_adjusted_mean, color=\"orange\", linestyle=\"--\", alpha=0.8)\n",
        "\n",
        "# Add text boxes with mean values\n",
        "ax.text(optimized_mean + 10, ax.get_ylim()[1] * 0.8, \n",
        "        f'Optimized Mean:\\n{optimized_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7),\n",
        "        ha='left', va='center')\n",
        "\n",
        "ax.text(guessed_mean - 10, ax.get_ylim()[1] * 0.6, \n",
        "        f'Guessed Mean:\\n{guessed_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.7),\n",
        "        ha='right', va='center')\n",
        "\n",
        "ax.text(risk_adjusted_mean - 10, ax.get_ylim()[1] * 0.4, \n",
        "        f'Risk-adjusted Mean:\\n{risk_adjusted_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7),\n",
        "        ha='right', va='center')\n",
        "\n",
        "ax.text(inverse_risk_adjusted_mean - 10, ax.get_ylim()[1] * 0.2, \n",
        "        f'Inverse Risk-adjusted Mean:\\n{inverse_risk_adjusted_mean:.1f}', \n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"orange\", alpha=0.7),\n",
        "        ha='right', va='center')\n",
        "\n",
        "plt.title(\"Response Distribution\")\n",
        "plt.xlabel(\"Total Media Contribution\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, the new allocation is riskier, and the mean is higher (still less uncertant than the risk-neutral allocation). We can go beyond visuals and quantify this.\n",
        "\n",
        "Because all are posterior distributions, we can check the density that has each response distribution at their respective mean. We'll be using kernel density estimation (KDE). This value, denoted $\\hat{f}(\\mu)$, represents the estimated height of the probability density function at the mean. Importantly, this is not itself a probability but a density, with units of ‚Äú1 over the units of the variable.‚Äù Higher values of $\\hat{f}(\\mu)$ indicate that the distribution is sharply peaked around the mean, reflecting greater certainty that posterior draws will lie close to the central value. Conversely, lower values correspond to flatter, more diffuse posteriors, indicating higher uncertainty.\n",
        "\n",
        "Let's check the density at the mean for each allocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "def kde_density_at_point(x, x0):\n",
        "    \"\"\"Gaussian KDE density estimate at x0 using Scott's rule bandwidth.\"\"\"\n",
        "    kde = gaussian_kde(x)  # Scott's rule by default\n",
        "    return float(kde.evaluate([x0])[0])\n",
        "\n",
        "optimized_density = kde_density_at_point(optimized_values, optimized_mean)\n",
        "guessed_density = kde_density_at_point(guessed_values, guessed_mean)\n",
        "risk_adjusted_density = kde_density_at_point(optimized_risk_values, risk_adjusted_mean)\n",
        "inverse_risk_adjusted_density = kde_density_at_point(optimized_inverse_risk_values, inverse_risk_adjusted_mean)\n",
        "\n",
        "print(f\"Optimized allocation response density at mean: {optimized_density:.3f}\")\n",
        "print(f\"Guessed allocation response density at mean: {guessed_density:.3f}\")\n",
        "print(f\"Risk-adjusted allocation response density at mean: {risk_adjusted_density:.3f}\")\n",
        "print(f\"Inverse Risk-adjusted allocation response density at mean: {inverse_risk_adjusted_density:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The density estimations tell the same story as the plots. How can we use this to take actions? For example, suppose we want to hit a target ROAS of $9.5$, then we can check the density of the ROAS distribution at $9.5$ for each response distribution given their respective allocation strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "optimized_roas = optimized_values / (time_unit_budget*num_periods)\n",
        "guessed_roas = guessed_values / (time_unit_budget*num_periods)\n",
        "risk_adjusted_roas = optimized_risk_values / (time_unit_budget*num_periods)\n",
        "inverse_risk_adjusted_roas = optimized_inverse_risk_values / (time_unit_budget*num_periods)\n",
        "\n",
        "# Calculate kde density at 9.5\n",
        "_target_roas = 9.5\n",
        "optimized_kde_density = kde_density_at_point(optimized_roas, _target_roas)\n",
        "guessed_kde_density = kde_density_at_point(guessed_roas, _target_roas)\n",
        "risk_adjusted_kde_density = kde_density_at_point(risk_adjusted_roas, _target_roas)\n",
        "inverse_risk_adjusted_kde_density = kde_density_at_point(inverse_risk_adjusted_roas, _target_roas)\n",
        "\n",
        "#plot the ROAS distributions\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot distributions\n",
        "az.plot_dist(optimized_roas, color=\"blue\", label=f\"Optimized allocation: {optimized_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(guessed_roas, color=\"red\", label=f\"Guessed allocation: {guessed_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(risk_adjusted_roas, color=\"green\", label=f\"Risk-adjusted allocation: {risk_adjusted_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(inverse_risk_adjusted_roas, color=\"orange\", label=f\"Inverse Risk-adjusted allocation: {inverse_risk_adjusted_kde_density:.3f}\", ax=ax)\n",
        "\n",
        "# Add vertical lines for means\n",
        "ax.axvline(_target_roas, color=\"black\", linestyle=\"--\", alpha=0.8, label=\"Target ROAS\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.title(\"ROAS Distribution\")\n",
        "plt.xlabel(\"ROAS\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to hit a target ROAS of $9.5$, the risk-neutral optimized allocation is the most certain, followed by the inverse risk-adjusted allocation.\n",
        "\n",
        "If instead our target ROAS is $7$, the inverse risk-adjusted allocation concentrates more density around that value, and the risk-neutral optimized allocation has less density around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# Calculate kde density at 7\n",
        "_roas_target = 7\n",
        "optimized_kde_density = kde_density_at_point(optimized_roas, _roas_target)\n",
        "guessed_kde_density = kde_density_at_point(guessed_roas, _roas_target)\n",
        "risk_adjusted_kde_density = kde_density_at_point(risk_adjusted_roas, _roas_target)\n",
        "inverse_risk_adjusted_kde_density = kde_density_at_point(inverse_risk_adjusted_roas, _roas_target)\n",
        "\n",
        "#plot the ROAS distributions\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot distributions\n",
        "az.plot_dist(optimized_roas, color=\"blue\", label=f\"Optimized allocation: {optimized_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(guessed_roas, color=\"red\", label=f\"Guessed allocation: {guessed_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(risk_adjusted_roas, color=\"green\", label=f\"Risk-adjusted allocation: {risk_adjusted_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(inverse_risk_adjusted_roas, color=\"orange\", label=f\"Inverse Risk-adjusted allocation: {inverse_risk_adjusted_kde_density:.3f}\", ax=ax)\n",
        "\n",
        "# Add vertical lines for means\n",
        "ax.axvline(_roas_target, color=\"black\", linestyle=\"--\", alpha=0.8, label=\"Target ROAS\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.title(\"ROAS Distribution\")\n",
        "plt.xlabel(\"ROAS\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under this paradigm, you can define a target estimate and select an allocation that maximizes the probability of hitting it. One way is to create a function that reduces variance with respect to the target, favoring narrower distributions with density near the target.\n",
        "\n",
        "Let's make this custom utility function, and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "import pytensor.tensor as pt\n",
        "import pymc_marketing.mmm.utility as ut\n",
        "\n",
        "def target_hit_probability(target, num_periods):\n",
        "    \"\"\"\n",
        "    Target hit probability utility function.\n",
        "    \n",
        "    Minimizing the mean squared error between predicted ROAS and the target\n",
        "    Adding a variance penalty to encourage tighter distributions.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    target : float\n",
        "        The target ROAS value to optimize towards\n",
        "    num_periods : int\n",
        "        Number of time periods for the budget allocation\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    callable\n",
        "        A utility function that can be used with the budget optimizer.\n",
        "        Returns negative objective (since optimizer minimizes).\n",
        "    \"\"\"\n",
        "    def _function(samples, budgets):\n",
        "        roas_samples = samples / (pt.sum(budgets) * num_periods)\n",
        "        \n",
        "        # Use mean squared error from target, which provides smoother gradients\n",
        "        mse_from_target = pt.mean((roas_samples - target) ** 2)\n",
        "        \n",
        "        # Add penalty for variance to encourage tighter distributions around target\n",
        "        variance_penalty = pt.var(roas_samples)\n",
        "        \n",
        "        # Combine MSE and variance penalty with weighting\n",
        "        # Higher variance penalty encourages narrower distributions\n",
        "        total_objective = mse_from_target + 0.1 * variance_penalty\n",
        "        \n",
        "        return -total_objective\n",
        "    return _function\n",
        "\n",
        "_roas_target = 8\n",
        "custom_utility_budget_allocation, custom_utility_optimizer_result, callback_results = (\n",
        "    optimizable_model.optimize_budget(\n",
        "        budget=time_unit_budget,\n",
        "        utility_function=target_hit_probability(_roas_target, num_periods),\n",
        "        callback=True,\n",
        "        minimize_kwargs={\"options\": {\"maxiter\": 2_000}},\n",
        "    )\n",
        ")\n",
        "\n",
        "custom_utility_posterior_response = optimizable_model.sample_response_distribution(\n",
        "    allocation_strategy=custom_utility_budget_allocation,\n",
        "    include_carryover=True,\n",
        "    include_last_observations=False,\n",
        "    additional_var_names=[\"y_original_scale\"]\n",
        ")\n",
        "\n",
        "# Print budget allocation by channel\n",
        "print(\"Budget allocation by channel:\")\n",
        "for channel in channels:\n",
        "    print(\n",
        "        f\"  {channel}: {custom_utility_posterior_response.allocation.sel(channel=channel).astype(int).sum():,}\"\n",
        "    )\n",
        "print(\n",
        "    f\"Total Allocated Budget: {np.sum(custom_utility_posterior_response.allocation.to_numpy()):,.0f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The allocation is similar to those observed before. Let's plot the ROAS distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "custom_utility_values = custom_utility_posterior_response.total_media_contribution_original_scale.values\n",
        "custom_utility_roas = custom_utility_values / (time_unit_budget*num_periods)\n",
        "optimized_kde_density = kde_density_at_point(optimized_roas, _roas_target)\n",
        "guessed_kde_density = kde_density_at_point(guessed_roas, _roas_target)\n",
        "risk_adjusted_kde_density = kde_density_at_point(risk_adjusted_roas, _roas_target)\n",
        "inverse_risk_adjusted_kde_density = kde_density_at_point(inverse_risk_adjusted_roas, _roas_target)\n",
        "custom_utility_kde_density = kde_density_at_point(custom_utility_roas, _roas_target)\n",
        "\n",
        "#plot the ROAS distributions\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot distributions\n",
        "az.plot_dist(optimized_roas, color=\"blue\", label=f\"Optimized allocation: {optimized_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(guessed_roas, color=\"red\", label=f\"Guessed allocation: {guessed_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(risk_adjusted_roas, color=\"green\", label=f\"Risk-adjusted allocation: {risk_adjusted_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(inverse_risk_adjusted_roas, color=\"orange\", label=f\"Inverse Risk-adjusted allocation: {inverse_risk_adjusted_kde_density:.3f}\", ax=ax)\n",
        "az.plot_dist(custom_utility_roas, color=\"purple\", label=f\"Custom utility allocation: {custom_utility_kde_density:.3f}\", ax=ax)\n",
        "\n",
        "# Add vertical lines for means\n",
        "ax.axvline(_roas_target, color=\"black\", linestyle=\"--\", alpha=0.8, label=\"Target ROAS\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.title(\"ROAS Distribution\")\n",
        "plt.xlabel(\"ROAS\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great üôåüèª The initial optimized allocation which was risk neutral, had initially the higher density around the target ROAS, but the density around it was not high enough, the new allocation bring a more certain answer around the target, because the objective function was built for it.\n",
        "\n",
        "::: {.callout-tip icon=false}\n",
        "## üí° Key Insight\n",
        "Here we have a custom utility function that allows us to optimize for a target ROAS. Nevertheless, you can build other utilities and use them as objectives. You can also introduce risk-aware constraints‚Äîwithout on top of your business constraints.\n",
        ":::\n",
        "\n",
        "Let's observe our final posterior around the target ROAS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "az.plot_posterior(\n",
        "    custom_utility_roas, \n",
        "    ref_val=_roas_target\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could report: the probability of achieving ROAS ‚â• 10 with this allocation is 31%, and ROAS < 10 is 69%. If we need more certainty, we can make the optimization more risk-averse.\n",
        "\n",
        "Now, if you want to think really bayesian, then you can define a region of practical equivalence, and check the probability of the ROAS being in that region. For example, you can ask yourself: Would I do something different if ROAS is 7, 8 or 9? If the answer it's no, then you find your ROPE. \n",
        "\n",
        "Let's say we want to know the probability of the ROAS being between $7$ and $10$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| warning: false\n",
        "# Calculate probability of ROAS being between 7 and 9\n",
        "prob_7_to_9 = np.mean((custom_utility_roas >= 7) & (custom_utility_roas <= 9))\n",
        "\n",
        "# Plot posterior with reference values and region\n",
        "fig, ax = plt.subplots()\n",
        "az.plot_posterior(\n",
        "    custom_utility_roas, \n",
        "    ref_val=_roas_target,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Add vertical lines for the region of interest\n",
        "ax.axvline(7, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"ROAS = 7\")\n",
        "ax.axvline(9, color=\"black\", linestyle=\"--\", alpha=0.7, label=\"ROAS = 9\")\n",
        "\n",
        "# Add text box with probability\n",
        "ax.text(0.02, 0.98, \n",
        "        f'P(7 ‚â§ ROAS ‚â§ 9) = {prob_7_to_9:.2%}', \n",
        "        transform=ax.transAxes,\n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
        "        ha='left', va='top', fontsize=12)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip icon=false}\n",
        "## üí° Key Insight\n",
        "You can **define a region of practical equivalence (ROPE)**, in order to be more precise with your decision making. This is quite natural way to think about a problem, and lightens the burden of the decision maker to commit to a single number. At the end of the day, we don't need to be 99.999% precise around every single answer, or number, we can be 95% or 90% precise and sometimes that's enough. *Identify if thats your case, and think accordingly*.\n",
        ":::\n",
        "\n",
        "# ‚úÖ Takeaways on uncertainty\n",
        "- **Treat uncertainty as first-class**: Optimize over the full posterior predictive, not point estimates. Compare plans by their distributions, not just expected means.\n",
        "- **Separate uncertainty types**: Aleatoric (irreducible noise) vs epistemic (learnable model/parameter uncertainty). Planning choices mostly shift epistemic risk; always communicate both.\n",
        "- **Find your risk appetite**: Ask stakeholders how certain we must be about reported results. What changes if they are a bit higher or lower?\n",
        "- **Choose a utility aligned to risk appetite**: Working with the mean is risk-neutral; quantiles, TailDistance, CVaR, or MTS are more risk-aware.\n",
        "- **Communicate distributions, not single numbers**: Show HDIs/quantiles and probabilities (e.g., P(ROAS ‚â• target)). It's better to understand the full spectrum of possibilities than to follow single numbers and be short-sighted.\n",
        "- **Define a region of practical equivalence (ROPE)**: Once you are comfortable with the uncertainty, play with ROPEs and find out how much you could lighten the estimated answer, and if you will do anything different if this one change under a range of values.\n",
        "\n",
        "# üöß Limitations\n",
        "- This approach represents the model‚Äôs confidence, but models can be very certain about a wrong answer. Always add business knowledge and guardrails to keep recommendations realistic.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "Hope this article was helpful to understand how to use Bayesian media mix models to optimize your media spend. Understand uncertanty it's a powerful tool to make better decisions, but it's not a silver bullet.\n",
        "\n",
        "The current example is a simple one, real life applications are more complex and require more sophisticated models, risk functions, and complex optimization problems, where constraints and business knowledge are key.\n",
        "\n",
        "At [PyMC Labs](https://www.pymc-labs.com/) we're building tools to make this process easier. If you're interested in learning more, explore the PyMC‚ÄëMarketing [Example Gallery](https://www.pymc-marketing.io/en/stable/gallery/gallery.html) and [API](https://www.pymc-marketing.io/en/stable/api/index.html), our [documentation](https://docs.pymc-marketing.com/) and [blog](https://www.pymc-labs.com/blog-posts).\n",
        "\n",
        "You can get a 30 minutes free consultation with our team to discuss your specific needs and how we can help you.\n",
        "\n",
        "- 1:1 Session with me: [Book a call](https://calendar.app.google/vX9DziLkdMSAAszU8)\n",
        "- Discovery session with PyMC Labs team: [Book a call](https://www.pymc-labs.com/?utm_source=carlos_trujillo&utm_medium=pydata_berlin&utm_campaign=bayesian_mmm_article)\n",
        "\n",
        "Thanks if you read this far!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cetagostini_web",
      "language": "python",
      "name": "cetagostini_web",
      "path": "/Users/carlostrujillo/Library/Jupyter/kernels/cetagostini_web"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
