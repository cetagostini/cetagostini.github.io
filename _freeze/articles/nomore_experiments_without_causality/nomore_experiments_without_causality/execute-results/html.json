{
  "hash": "2dfb23322438bd761f9343ba8bc4f08e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Media Mix Model calibration is useless without causal knowledge\"\ndate: \"2025-04-01\"\ncategories: [python, experimentation, media mix modeling, mmm, bayesian, pymc, pydata, germany, darmstadt]\nimage: \"../images/nomore_experiments_without_causality.png\"\njupyter: cetagostini_web\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    code-overflow: wrap\n---\n\n\n\n\n# Introduction\n\nImagine you just shipped a shiny new Bayesian Media-Mix Model (MMM) that *perfectly* back-fits years of marketing data. A/B-lift experiments then tell you channel-A is worth **‚Ç¨2.7 M**, but your model insists it is worth **‚Ç¨7 M**. \"Easy fix,\" you think: *calibrate* the MMM with the lift tests‚Äîadd an extra likelihood term, rerun, publish.\n\nYet the calibrated model still over/under-values the channel based on the experimental evidence. Looks like it can't reconcile the experimental evidence with the data, and adding new calibration for other channels actually makes it worse.\n\nThat is the calibration trap: without causal structure the posterior can't happily reconcile observations and **clean** experiments at the same time.\n\nIn this article we will build a PyMC MMM, add lift-test calibration, and then show‚Äîstep-by-step‚Äîwhy calibration alone cannot save a misspecified causal story.\n\n---\n\n# Why marketers love calibration\n\n* **Ground-truth anchor.** Lift tests are randomised, so their incremental effects are (almost) unbiased.  \n* **Sample-size boost.** MMMs see every day and every channel; experiments see only a slice. Combining them promises lower variance.  \n* **Storytelling power.** \"Our model *matches* the experiments\" is an executive-friendly sound-bite.\n\nCalibration therefore feels like catching two Bayesian birds with one conjugate stone.\n\n---\n\n# What *is* calibration‚Äîmathematically?\n\nFor each experiment $i$ the model predicts a lift\n\n$$\n\\widehat{\\Delta y_i}(\\theta)\\;=\\;\ns\\bigl(x_i+\\Delta x_i;\\,\\theta_{c(i)}\\bigr)\n\\;-\\;\ns\\bigl(x_i;\\,\\theta_{c(i)}\\bigr),\n$$  \n\nwhere  \n\n* $x_i$ ‚Äì baseline spend before the experiment,  \n* $\\Delta x_i$ ‚Äì change in spend during the experiment,  \n* $s(\\cdot;\\theta_{c(i)})$ ‚Äì saturation curve for the channel that experiment $i$ targets,  \n* $\\theta$ ‚Äì all saturation-curve parameters,  \n* $\\widehat{\\Delta y_i}(\\theta)$ ‚Äì model-predicted incremental outcome.\n\nWe then attach the observed lift $\\Delta y_i$ and its error $\\sigma_i$ through an additional likelihood\n\n$$\np\\!\\bigl(\\Delta y_i \\mid \\theta\\bigr)\\;=\\;\n\\operatorname{Gamma}\\!\\bigl(\n\\mu=\\lvert\\widehat{\\Delta y_i}(\\theta)\\rvert,\\;\n\\sigma=\\sigma_i\n\\bigr),\n$$  \n\nwhere  \n\n* $\\Delta y_i$ ‚Äì experimentally measured incremental outcome,  \n* $\\sigma_i$ ‚Äì reported standard error of $\\Delta y_i$,  \n* $\\mu$ ‚Äì mean parameter set to the *absolute* predicted lift so the Gamma remains non-negative.\n\nStacking all $n_{\\text{lift}}$ experiments gives the calibrated posterior\n\n$$\np\\!\\bigl(\\theta \\mid \\mathbf y,\\mathcal L\\bigr)\n\\;\\propto\\;\np\\!\\bigl(\\mathbf y \\mid \\theta\\bigr)\\;\n\\prod_{i=1}^{n_{\\text{lift}}}\np\\!\\bigl(\\Delta y_i \\mid \\theta\\bigr)\\;\np(\\theta),\n$$  \n\nwhere  \n\n* $\\mathbf y$ ‚Äì full time-series of observed outcomes (sales, sign-ups ‚Ä¶),  \n* $\\mathcal L$ ‚Äì the collection of lift-test observations $(\\Delta y_i,\\sigma_i)$,  \n* $p(\\theta)$ ‚Äì priors for all parameters.\n\nPyMC turns this into a three-liner:  \n\n```python\nadd_lift_measurements_to_likelihood_from_saturation(\n    model=mmm,\n    df_lift=df_lifts,     # experiment data-frame\n    dist=pm.Gamma,\n)\n```\n\nIn simple terms, calibration appends one extra likelihood per experiment: for lift `i` we run the channel's saturation curve at the pre-spend and post-spend levels, subtract the two, and call that result the model-expected incremental response for experiment `i` (a deterministic function of the saturation parameter vector $\\theta$). We then treat the observed lift $\\Delta y_i$ as a Gamma-distributed draw whose mean is the absolute value of that model-expected increment and whose dispersion is the experiment's reported standard error $\\sigma_i$. \n\nThese independent $\\Gamma(\\mu = |\\text{model-expected increment}|, \\sigma = \\sigma_i)$ factors multiply into the original time-series likelihood, yielding a posterior where $\\theta$ is pulled toward values that keep every model-expected increment within the experimental noise band. In effect, each lift test imposes a Bayesian anchor that penalises any parameter setting whose predicted causal effect disagrees with ground-truth, while still allowing the full sales history to inform the remaining uncertainty.\n\nLet's see how this works in practice, by creating a synthetic dataset and fitting a simple MMM.\n\n# Getting started\n\nWe'll use Pytensor to run our data-generation-process (DGP). Let's set the seed for reproducibility, and define the number of observations, and finally add some default configurations for the notebook.\n\n::: {#319ef799 .cell execution_count=1}\n``` {.python .cell-code}\nimport warnings\nimport pymc as pm\nimport arviz as az\nimport pytensor.tensor as pt\nfrom pytensor.graph import rewrite_graph\nimport preliz as pz\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\n\nfrom pymc_marketing.mmm import GeometricAdstock, MichaelisMentenSaturation, MMM\nfrom pymc_marketing.prior import Prior\n\nSEED = 42\nn_observations = 1050\n\nwarnings.filterwarnings(\"ignore\")\n\n# Set the style\naz.style.use(\"arviz-darkgrid\")\nplt.rcParams[\"figure.figsize\"] = [8, 4]\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"axes.labelsize\"] = 6\nplt.rcParams[\"xtick.labelsize\"] = 6\nplt.rcParams[\"ytick.labelsize\"] = 6\n\n%config InlineBackend.figure_format = \"retina\"\n```\n:::\n\n\nNow, we can define the date range.\n\n::: {#8a30361b .cell execution_count=2}\n``` {.python .cell-code}\nmin_date = pd.to_datetime(\"2022-01-01\")\nmax_date = min_date + pd.Timedelta(days=n_observations)\n\ndate_range = pd.date_range(start=min_date, end=max_date, freq=\"D\")\n\ndf = pd.DataFrame(data={\"date_week\": date_range}).assign(\n    year=lambda x: x[\"date_week\"].dt.year,\n    month=lambda x: x[\"date_week\"].dt.month,\n    dayofyear=lambda x: x[\"date_week\"].dt.dayofyear,\n)\n```\n:::\n\n\nWe can start by creating the spend vectors for each channel. These are the will define later the amount of impressions or exposition we get from each channel, which by the end will transform into sales.\n\n::: {#3252dd1a .cell execution_count=3}\n``` {.python .cell-code}\nspend_x1 = pt.vector(\"spend_x1\")\nspend_x2 = pt.vector(\"spend_x2\")\nspend_x3 = pt.vector(\"spend_x3\")\nspend_x4 = pt.vector(\"spend_x4\")\n\n# Create sample inputs for demonstration using preliz distributions:\npz_spend_x1 = np.convolve(\n    pz.Gamma(mu=.8, sigma=.3).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x1[:14] = pz_spend_x1.mean()\npz_spend_x1[-14:] = pz_spend_x1.mean()\n\npz_spend_x2 = np.convolve(\n    pz.Gamma(mu=.6, sigma=.4).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x2[:14] = pz_spend_x2.mean()\npz_spend_x2[-14:] = pz_spend_x2.mean()\n\npz_spend_x3 = np.convolve(\n    pz.Gamma(mu=.2, sigma=.2).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x3[:14] = pz_spend_x3.mean()\npz_spend_x3[-14:] = pz_spend_x3.mean()\n\npz_spend_x4 = np.convolve(\n    pz.Gamma(mu=.1, sigma=.03).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_spend_x4[:14] = pz_spend_x4.mean()\npz_spend_x4[-14:] = pz_spend_x4.mean()\n\nfig, ax = plt.subplots()\nax.plot(date_range[1:], pz_spend_x1, label='Channel 1')\nax.plot(date_range[1:], pz_spend_x2, label='Channel 2')\nax.plot(date_range[1:], pz_spend_x3, label='Channel 3')\nax.plot(date_range[1:], pz_spend_x4, label='Channel 4')\nax.set_xlabel('Time')\nax.set_ylabel('Spend')\nax.legend()\nplt.show()\n```\n:::\n\n\nUsing the same logic we can create other components such as trend, noise, seasonality, and certain events.\n\n::: {#5c97c1ea .cell execution_count=4}\n``` {.python .cell-code}\n## Trend\ntrend = pt.vector(\"trend\")\n# Create a sample input for the trend\nnp_trend = (np.linspace(start=0.0, stop=.50, num=n_observations) + .10) ** (.1 / .4)\n\n## NOISE \nglobal_noise = pt.vector(\"global_noise\")\n# Create a sample input for the noise\npz_global_noise = pz.Normal(mu=0, sigma=.005).rvs(size=n_observations, random_state=SEED)\n\n# EVENTS EFFECT\npt_event_signal = pt.vector(\"event_signal\")\npt_event_contributions = pt.vector(\"event_contributions\")\n\nevent_dates = [\"24-12\", \"09-07\"]  # List of events as month-day strings\nstd_devs = [25, 15]  # List of standard deviations for each event\nevents_coefficients = [.094, .018]\n\nsignals_independent = []\n\n# Initialize the event effect array\nevent_signal = np.zeros(len(date_range))\nevent_contributions = np.zeros(len(date_range))\n\n# Generate event signals\nfor event, std_dev, event_coef in zip(\n    event_dates, std_devs, events_coefficients, strict=False\n):\n    # Find all occurrences of the event in the date range\n    event_occurrences = date_range[date_range.strftime(\"%d-%m\") == event]\n\n    for occurrence in event_occurrences:\n        # Calculate the time difference in days\n        time_diff = (date_range - occurrence).days\n\n        # Generate the Gaussian basis for the event\n        _event_signal = np.exp(-0.5 * (time_diff / std_dev) ** 2)\n\n        # Add the event signal to the event effect\n        signals_independent.append(_event_signal)\n        event_signal += _event_signal\n\n        event_contributions += _event_signal * event_coef\n\nnp_event_signal = event_signal\nnp_event_contributions = event_contributions\n\nplt.plot(pz_global_noise, label='Global Noise')\nplt.plot(np_trend, label='Trend')\nplt.plot(np_event_signal, label='Event Contributions')\nplt.title('Components of the Time Series Model')\nplt.xlabel('Time (days)')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n:::\n\n\nIn order to make it more interesting, lets add a price variable. Usually, price creates more impact as it's slower. The product price contribution function we'll use is a diminishing returns function:\n\n$$f(X, \\alpha, \\lambda) = \\frac{\\alpha}{1 + (X / \\lambda)}$$\n\nwhere $\\alpha$ represents the maximum contribution and $\\lambda$ is a scaling parameter that controls how quickly the contribution diminishes as price increases.\n\n::: {#b4f25996 .cell execution_count=5}\n``` {.python .cell-code}\ndef product_price_contribution(X, alpha, lam):\n    return alpha / (1 + (X / lam))\n    \n# Create a product price vector.\nproduct_price = pt.vector(\"product_price\")\nproduct_price_alpha = pt.scalar(\"product_price_alpha\")\nproduct_price_lam = pt.scalar(\"product_price_lam\")\n\n# Create a sample input for the product price\npz_product_price = np.convolve(\n    pz.Gamma(mu=.05, sigma=.02).rvs(size=n_observations, random_state=SEED), \n    np.ones(14) / 14, mode=\"same\"\n)\npz_product_price[:14] = pz_product_price.mean()\npz_product_price[-14:] = pz_product_price.mean()\n\nproduct_price_alpha_value = .08\nproduct_price_lam_value = .03\n\n# Direct contribution to the target.\npt_product_price_contribution = product_price_contribution(\n    product_price, \n    product_price_alpha, \n    product_price_lam\n)\n\n# plot the product price contribution\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\n# Plot the raw price data\nax1.plot(pz_product_price, color=\"green\")\nax1.set_title('Product Price')\nax1.set_xlabel('Time (days)')\nax1.set_ylabel('Price')\nax1.grid(True, alpha=0.3)\n\n# Plot the price contribution\nprice_contribution = pt_product_price_contribution.eval({\n    \"product_price\": pz_product_price,\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value\n})\nax2.plot(price_contribution, color=\"black\")\nax2.set_title('Price Contribution')\nax2.set_xlabel('Time (days)')\nax2.set_ylabel('Contribution')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nWith all the principal components in place, all parent nodes we can start to write down our causal DAG to define the relationships we want to explain.\n\n::: {#e404ef44 .cell execution_count=6}\n``` {.python .cell-code}\n# Plot causal graph of the vars x1, x2, x3, x4 using graphviz\ncdag_impressions = graphviz.Digraph(comment='Causal DAG for Impressions')\n\ncdag_impressions.node('spend_x1', 'Spend X1')\ncdag_impressions.node('spend_x2', 'Spend X2')\ncdag_impressions.node('spend_x3', 'Spend X3')\ncdag_impressions.node('spend_x4', 'Spend X4')\ncdag_impressions.node('events', 'Events')\n\ncdag_impressions.edge('spend_x1', 'impressions_x1')\ncdag_impressions.edge('spend_x2', 'impressions_x2')\ncdag_impressions.edge('spend_x3', 'impressions_x3')\ncdag_impressions.edge('spend_x4', 'impressions_x4')\n\ncdag_impressions.edge('impressions_x1', 'impressions_x3')\ncdag_impressions.edge('impressions_x2', 'impressions_x3')\ncdag_impressions.edge('impressions_x2', 'impressions_x4')\n\ncdag_impressions.edge('events', 'impressions_x2')\ncdag_impressions.edge('events', 'impressions_x3')\n\ncdag_impressions\n```\n:::\n\n\nOnce our causal graph is defined, we can start to write down in pytensor the structure and relationships.\n\n::: {#8fbefa1e .cell execution_count=7}\n``` {.python .cell-code}\n# Create a impressions vector, result of x1, x2, x3, x4. by some beta with daily values.\n# Define all parameters as PyTensor variables\nbeta_x1 = pt.vector(\"beta_x1\")\nimpressions_x1 = spend_x1 * beta_x1\n\nbeta_x2 = pt.vector(\"beta_x2\")\nalpha_event_x2 = pt.scalar(\"alpha_event_x2\")\nimpressions_x2 = spend_x2 * beta_x2 + pt_event_signal * alpha_event_x2\n\nbeta_x3 = pt.vector(\"beta_x3\")\nalpha_event_x3 = pt.scalar(\"alpha_event_x3\")\nalpha_x1_x3 = pt.scalar(\"alpha_x1_x3\")\nalpha_x2_x3 = pt.scalar(\"alpha_x2_x3\")\nimpressions_x3 = spend_x3 * beta_x3 + pt_event_signal * alpha_event_x3 + (\n    impressions_x2 * alpha_x2_x3\n    + impressions_x1 * alpha_x1_x3\n)\n\nbeta_x4 = pt.vector(\"beta_x4\")\nalpha_x2_x4 = pt.scalar(\"alpha_x2_x4\")\nimpressions_x4 = spend_x4 * beta_x4 + impressions_x2 * alpha_x2_x4\n\n# Create sample values for the parameters (to be used in eval)\npz_beta_x1 = pz.Beta(alpha=0.05, beta=.1).rvs(size=n_observations, random_state=SEED)\npz_beta_x2 = pz.Beta(alpha=.015, beta=.05).rvs(size=n_observations, random_state=SEED)\npz_alpha_event_x2 = 0.015\npz_beta_x3 = pz.Beta(alpha=.1, beta=.1).rvs(size=n_observations, random_state=SEED)\npz_alpha_event_x3 = 0.001\npz_alpha_x1_x3 = 0.005\npz_alpha_x2_x3 = 0.12\npz_beta_x4 = pz.Beta(alpha=.125, beta=.05).rvs(size=n_observations, random_state=SEED)\npz_alpha_x2_x4 = 0.01\n\n# plot all impressions\n# Define dependencies for each variable\nx1_deps = {\n    \"beta_x1\": pz_beta_x1,\n    \"spend_x1\": pz_spend_x1,\n}\n\nx2_deps = {\n    \"beta_x2\": pz_beta_x2,\n    \"spend_x2\": pz_spend_x2,\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"event_signal\": event_signal[:-1],  # Slice to match 1050 length\n}\n\n# For x3, we need all dependencies from x1 and x2 plus its own\nx3_deps = {\n    \"beta_x3\": pz_beta_x3,\n    \"spend_x3\": pz_spend_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    **x1_deps,\n    **x2_deps,\n}\n\n# For x4, we need dependencies from x2 plus its own\nx4_deps = {\n    \"beta_x4\": pz_beta_x4,\n    \"spend_x4\": pz_spend_x4,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    **x2_deps,\n}\n\n# Plot each impression series\nfig, axs = plt.subplots(2, 2, sharex='row', sharey='row')\n\n# Channel 1\naxs[0, 0].plot(impressions_x1.eval(x1_deps), color='blue')\naxs[0, 0].set_title('Channel 1')\naxs[0, 0].set_ylabel('Impressions')\n\n# Channel 2\naxs[0, 1].plot(impressions_x2.eval(x2_deps), color='orange')\naxs[0, 1].set_title('Channel 2')\n\n# Channel 3\naxs[1, 0].plot(impressions_x3.eval(x3_deps), color='green')\naxs[1, 0].set_title('Channel 3')\naxs[1, 0].set_xlabel('Time')\naxs[1, 0].set_ylabel('Impressions')\n\n# Channel 4\naxs[1, 1].plot(impressions_x4.eval(x4_deps), color='red')\naxs[1, 1].set_title('Channel 4')\naxs[1, 1].set_xlabel('Time')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {.callout-note}\n## Visualizing the computational graph\n\nIn order to check we write down the process properly, we can ask PyTensor to print our structural causal model. This is not necessary for the analysis, but can be helpful for debugging and understanding the model structure.\n\n::: {#878dfe44 .cell execution_count=8}\n``` {.python .cell-code}\nimport pytensor.printing as printing\n# Plot the graph of our model using pytensor\nprinting.pydotprint(rewrite_graph(impressions_x4), outfile=\"images/impressions.png\", var_with_name_simple=True)\n# Display the generated graph\nfrom IPython.display import Image\nImage(filename=\"images/impressions.png\")\n```\n:::\n\n\nIf, you don't like to see the graphical version, you can ask for the string representation.\n\n::: {#6f0667ec .cell execution_count=9}\n``` {.python .cell-code}\n# dprint the target_var\nrewrite_graph(impressions_x4).dprint(depth=5);\n```\n:::\n\n\n:::\n\nNow, let's define our forward pass - how media exposure actually impacts our target variable. In marketing, we typically see two key effects: saturation (diminishing returns) and lagging (delayed impact). We'll model these using the Michaelis-Menten function for saturation and Geometric Adstock for the lagging effects.\n\n::: {#ec5cfd39 .cell execution_count=10}\n``` {.python .cell-code}\n# Creating forward pass for impressions\ndef forward_pass(x, adstock_alpha, saturation_lam, saturation_alpha):\n    # return type pytensor.tensor.variable.TensorVariable\n    return MichaelisMentenSaturation.function(\n        MichaelisMentenSaturation, \n        x=GeometricAdstock(\n            l_max=24, normalize=False\n        ).function(\n            x=x, alpha=adstock_alpha,\n        ), lam=saturation_lam, alpha=saturation_alpha,\n    )\n\n# Applying forward pass to impressions\n# Create scalars variables for the parameters x2, x3, x4\npt_saturation_lam_x2 = pt.scalar(\"saturation_lam_x2\")\npt_saturation_alpha_x2 = pt.scalar(\"saturation_alpha_x2\")\n\npt_saturation_lam_x3 = pt.scalar(\"saturation_lam_x3\")\npt_saturation_alpha_x3 = pt.scalar(\"saturation_alpha_x3\")\n\npt_saturation_lam_x4 = pt.scalar(\"saturation_lam_x4\")\npt_saturation_alpha_x4 = pt.scalar(\"saturation_alpha_x4\")\n\npt_global_adstock_effect = pt.scalar(\"global_adstock_alpha\")\n\n# Apply forward pass to impressions\nimpressions_x2_forward = forward_pass(\n    impressions_x2, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x2, \n    pt_saturation_alpha_x2\n)\n\nimpressions_x3_forward = forward_pass(\n    impressions_x3, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x3, \n    pt_saturation_alpha_x3\n)\n\nimpressions_x4_forward = forward_pass(\n    impressions_x4, \n    pt_global_adstock_effect, \n    pt_saturation_lam_x4, \n    pt_saturation_alpha_x4\n)\n```\n:::\n\n\nWith all of the following in place, we can define the causal DAG for the target variable and the structural equation as the sum of all previous variables.\n\n::: {#93d3c549 .cell execution_count=11}\n``` {.python .cell-code}\n# Plot graphviz causal dag for the target_var\n# Create a Graphviz object\ndot = graphviz.Digraph(comment='Causal DAG for Target Variable')\n\n# Add nodes for each variable\ndot.node('spend_x1', 'Spend X1')\ndot.node('spend_x2', 'Spend X2')\ndot.node('spend_x3', 'Spend X3')\ndot.node('spend_x4', 'Spend X4')\ndot.node('trend', 'Trend')\ndot.node('global_noise', 'Global Noise')\ndot.node('event_contributions', 'Events')\ndot.node('product_price_contribution', 'Product Price Contribution')\n\ndot.edge('spend_x1', 'impressions_x1')\ndot.edge('spend_x2', 'impressions_x2')\ndot.edge('spend_x3', 'impressions_x3')\ndot.edge('spend_x4', 'impressions_x4')\n\ndot.edge('impressions_x1', 'impressions_x3')\ndot.edge('impressions_x2', 'impressions_x3')\ndot.edge('impressions_x2', 'impressions_x4')\ndot.edge('event_contributions', 'impressions_x2')\ndot.edge('event_contributions', 'impressions_x3')\n\ndot.edge('trend', 'target_var')\ndot.edge('global_noise', 'target_var')\ndot.edge('event_contributions', 'target_var')\ndot.edge('product_price_contribution', 'target_var')\n\ndot.edge('impressions_x2', 'target_var')\ndot.edge('impressions_x3', 'target_var')\ndot.edge('impressions_x4', 'target_var')\n\n# Render the graph\ndot\n```\n:::\n\n\n$$\n\\begin{align}\n\\text{Target} &\\sim \\sum_{i \\in \\{2,3,4\\}} f_i(\\text{impressions}_i) + \\\\\n&\\text{event\\_contributions} + \\\\\n&\\text{product\\_price\\_contribution} + \\\\\n&\\text{trend} + \\\\\n&\\text{noise}\n\\end{align}\n$$\n\nWhere $f_i$ represents the forward pass function (adstock and saturation) applied to each channel's impressions.\n\n::: {#dd589fb2 .cell execution_count=12}\n``` {.python .cell-code}\ntarget_var = rewrite_graph(\n    impressions_x4_forward + \n    impressions_x3_forward +\n    impressions_x2_forward +\n    pt_event_contributions +\n    pt_product_price_contribution + \n    trend + \n    global_noise\n)\n\n# Eval target_var and plot\nnp_target_var = target_var.eval({\n    \"spend_x4\": pz_spend_x4,\n    \"spend_x3\": pz_spend_x3,\n    \"spend_x2\": pz_spend_x2,\n    \"spend_x1\": pz_spend_x1,\n    \"event_signal\": event_signal[:-1],\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    \"beta_x2\": pz_beta_x2,\n    \"beta_x3\": pz_beta_x3,\n    \"beta_x4\": pz_beta_x4,\n    \"beta_x1\": pz_beta_x1,\n    \"saturation_lam_x2\": .5,\n    \"saturation_alpha_x2\": .2,\n    \"saturation_lam_x3\": .7,\n    \"saturation_alpha_x3\": .7,\n    \"saturation_lam_x4\": .2,\n    \"saturation_alpha_x4\": .1,\n    \"global_adstock_alpha\": .2,\n    \"product_price\": pz_product_price,\n    \"event_contributions\": np_event_contributions[:-1],\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value,\n    \"trend\": np_trend,\n    \"global_noise\": pz_global_noise,\n})\n\nplt.plot(np_target_var, linewidth=2)\nplt.title('Target Variable Over Time', fontsize=14)\nplt.xlabel('Time Period', fontsize=12)\nplt.ylabel('Target Value', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nNow, we can imagine our dataframe in this case will be something like the following:\n\n::: {#bb8d9e6e .cell execution_count=13}\n``` {.python .cell-code}\n# make dataset with impressions x1, x2, x3, x4 and target_var\nscaler_factor_for_all = 150\ndates = pd.date_range(start='2020-01-01', periods=n_observations, freq='D')\ndata = pd.DataFrame({\n    \"date\": dates,\n    \"target_var\": np.round(np_target_var * scaler_factor_for_all, 4),\n    \"impressions_x1\": np.round(impressions_x1.eval(x1_deps) * scaler_factor_for_all, 4),\n    \"impressions_x2\": np.round(impressions_x2.eval(x2_deps) * scaler_factor_for_all, 4),\n    \"impressions_x3\": np.round(impressions_x3.eval(x3_deps) * scaler_factor_for_all, 4),\n    \"impressions_x4\": np.round(impressions_x4.eval(x4_deps) * scaler_factor_for_all, 4),\n    \"event_2020_09\": np.round(signals_independent[0][:-1], 4),\n    \"event_2020_12\": np.round(signals_independent[1][:-1], 4),\n    \"event_2021_09\": np.round(signals_independent[2][:-1], 4),\n    \"event_2021_12\": np.round(signals_independent[3][:-1], 4),\n    \"event_2022_09\": np.round(signals_independent[4][:-1], 4),\n})\ndata[\"trend\"] = data.index\ndata.head()\n```\n:::\n\n\nIf we don't think in a causal way, we will probably just say, \"lets add all to the blender\".\n\n::: {#0cd9a487 .cell execution_count=14}\n``` {.python .cell-code}\n# Building priors for adstock and saturation\nadstock_priors = {\n    \"alpha\": Prior(\"Beta\", alpha=1, beta=1, dims=\"channel\"),\n}\n\nadstock = GeometricAdstock(l_max=28, priors=adstock_priors)\n\nsaturation_priors = {\n    \"lam\": Prior(\n        \"Gamma\",\n        mu=2,\n        sigma=1,\n        dims=\"channel\",\n    ),\n    \"alpha\": Prior(\n        \"Gamma\",\n        mu=.5,\n        sigma=.5,\n        dims=\"channel\",\n    ),\n}\n\nsaturation = MichaelisMentenSaturation(priors=saturation_priors)\n\n# Split data into train and test sets\ntrain_idx = 879\n\nX_train = data.iloc[:train_idx].drop(columns=[\"target_var\"])\nX_test = data.iloc[train_idx:].drop(columns=[\"target_var\"])\ny_train = data.iloc[:train_idx][\"target_var\"]\ny_test = data.iloc[train_idx:][\"target_var\"]\n\ncontrol_columns = [\n    \"event_2020_09\", \"event_2020_12\", \n    \"event_2021_09\", \"event_2021_12\", \n    \"event_2022_09\",\n    \"trend\"\n]\nchannel_columns = [\n    col for col in X_train.columns if col not in control_columns and col != \"date\"\n]\n\n# Model config\nmodel_config = {\n    \"likelihood\": Prior(\n        \"TruncatedNormal\",\n        lower=0,\n        sigma=Prior(\"HalfNormal\", sigma=1),\n        dims=\"date\",\n    ),\n}\n\n# sampling options for PyMC\nsample_kwargs = {\n    \"tune\": 1000,\n    \"draws\": 500,\n    \"chains\": 4,\n    \"random_seed\": 42,\n    \"target_accept\": 0.94,\n    \"nuts_sampler\": \"nutpie\",\n}\n\nnon_causal_mmm = MMM(\n    date_column=\"date\",\n    channel_columns=channel_columns,\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\nnon_causal_mmm.build_model(X_train, y_train)\n```\n:::\n\n\n:::callout-note\n## Building the model\n\nAll PyMC models are structural causal models, which means they represent the causal generative process of the data. We can visualize this process through a Directed Acyclic Graph (DAG) that shows how variables influence each other in the model.\n\n::: {#2bcf42ff .cell execution_count=15}\n``` {.python .cell-code}\nnon_causal_mmm.model.to_graphviz()\n```\n:::\n\n\n:::\n\nOnce the model is build, we can train it.\n\n::: {#d0f8fb96 .cell execution_count=16}\n``` {.python .cell-code}\nnon_causal_mmm.fit(X_train, y_train,)\nnon_causal_mmm.sample_posterior_predictive(X_train, extend_idata=True, combined=True)\n```\n:::\n\n\nWe are happy with our model, we don't get any divergencies, and the sampling looks good.\n\n::: {#3a70b1bc .cell execution_count=17}\n``` {.python .cell-code}\n# Number of diverging samples\nprint(\n    f\"Total divergencies: {non_causal_mmm.idata['sample_stats']['diverging'].sum().item()}\"\n)\n\naz.summary(\n    data=non_causal_mmm.fit_result,\n    var_names=[\n        \"intercept\",\n        \"y_sigma\",\n        \"saturation_alpha\",\n        \"saturation_lam\",\n        \"adstock_alpha\",\n    ],\n)\n```\n:::\n\n\nIf our model has a correct understanding of causality, we can use it to perform a do-calculus to estimate the effect of our channel, using out of sample (sampling from the posterior). Mathematically, we want to compute the causal effect as the difference between two interventions: $$P(Y|do(X=x)) - P(Y|do(X=0))$$\n\nThis should allows us to isolate the causal impact of our marketing channels on the outcome variable.\n\n::: {#50970096 .cell execution_count=18}\n``` {.python .cell-code}\nX_test_x2_zero = X_test.copy()\nX_test_x2_zero[\"impressions_x2\"].iloc[:100] = 0\n\ny_do_x2_zero = non_causal_mmm.sample_posterior_predictive(\n    X_test_x2_zero, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\ny_do_x2 = non_causal_mmm.sample_posterior_predictive(\n    X_test, extend_idata=False, include_last_observations=True, random_seed=42\n)\n```\n:::\n\n\nNow that we have both posteriors, we can compute the difference between the period with the index 880-890 and plot the causal effect and the cumulative causal effect.\n\n::: {#06feccf7 .cell execution_count=19}\n``` {.python .cell-code}\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect = (y_do_x2_zero - y_do_x2).y\n# Get dates from the coordinates for x-axis\ndates = x2_causal_effect.coords['date'].values[:100]  # Take only first 100 days\n\n# Plot the causal effect\nplt.subplot(1, 2, 1)\n# Calculate mean and quantiles\nmean_effect = x2_causal_effect.mean(dim=\"sample\")[:100]\nplt.plot(dates, mean_effect)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\n# For cumulative effect, compute quantiles directly from cumulative sums\ncum_effect = x2_causal_effect.cumsum(dim=\"date\")\ncum_mean = cum_effect.mean(dim=\"sample\")[:100]\nplt.plot(dates, cum_mean)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\n```\n:::\n\n\nIn reality, in order to validate the following estimated effect, we'll need to run an actual experiment. Because we did the data generation process we can run this actual experiment to compare.\n\n::: {#bbd7ff48 .cell execution_count=20}\n``` {.python .cell-code}\n# Create an intervened spend_x2 with zeros between index 880 and 980\nintervened_spend_x2 = pz_spend_x2.copy()\nintervened_spend_x2[880:980] = 0\n\n# Evaluate target variable with the intervention\nnp_target_var_x2_zero = target_var.eval({\n    \"spend_x4\": pz_spend_x4,\n    \"spend_x3\": pz_spend_x3,\n    \"spend_x2\": intervened_spend_x2,\n    \"spend_x1\": pz_spend_x1,\n    \"event_signal\": event_signal[:-1],\n    \"alpha_event_x2\": pz_alpha_event_x2,\n    \"alpha_event_x3\": pz_alpha_event_x3,\n    \"alpha_x1_x3\": pz_alpha_x1_x3,\n    \"alpha_x2_x3\": pz_alpha_x2_x3,\n    \"alpha_x2_x4\": pz_alpha_x2_x4,\n    \"beta_x2\": pz_beta_x2,\n    \"beta_x3\": pz_beta_x3,\n    \"beta_x4\": pz_beta_x4,\n    \"beta_x1\": pz_beta_x1,\n    \"saturation_lam_x2\": .5,\n    \"saturation_alpha_x2\": .2,\n    \"saturation_lam_x3\": .7,\n    \"saturation_alpha_x3\": .7,\n    \"saturation_lam_x4\": .2,\n    \"saturation_alpha_x4\": .1,\n    \"global_adstock_alpha\": .2,\n    \"product_price\": pz_product_price,\n    \"event_contributions\": np_event_contributions[:-1],\n    \"product_price_alpha\": product_price_alpha_value,\n    \"product_price_lam\": product_price_lam_value,\n    \"trend\": np_trend,\n    \"global_noise\": pz_global_noise,\n})\n\n# x2 total effect y | do(x2=>1) - y | do(x2=0)\nx2_intervention_real_effect = np_target_var_x2_zero - np_target_var\nx2_intervention_real_cumulative_effect = np.cumsum(x2_intervention_real_effect)\n\n# Plot both the intervention effect and cumulative effect\nplt.subplot(1, 2, 1)\n# Plot the daily effect\ndaily_effect = x2_intervention_real_effect[880:980] * scaler_factor_for_all\nplt.plot(dates, daily_effect)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\ncumulative_effect = x2_intervention_real_cumulative_effect[880:980] * scaler_factor_for_all\nplt.plot(dates, cumulative_effect)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nHow does compare to the recovered effect? Let's observe! üëÄ\n\n::: {#2f82f4dc .cell execution_count=21}\n``` {.python .cell-code}\n# Create a figure to compare real effects with estimated effects\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nThe initial model have been under estimating the effect of $X2$. We can see the model was thinking we'll loosing almost none users when in reality wi'll loose around 600 in total. Maybe we did something wrong? Are we maybe the wrong causal question?\n\nThat doesn't matter, we have calibration! ü§™\n\nLets compute the observable delta in Y and observable delta in X and use it for calibration.\n\n::: {#235cb990 .cell execution_count=22}\n``` {.python .cell-code}\nintervened_channel = \"impressions_x2\"\ntotal_observed_effect = cumulative_effect[-1] # delta Y\ntotal_previous_imp_before_intervention = X_train[intervened_channel].iloc[-100:].sum()\ntotal_change_imp_during_intervention = -X_train[intervened_channel].iloc[-100:].sum()\nsigma = 0.3 # confidence in the experiment.\n\ndf_lift_test = pd.DataFrame(\n    [{\n        \"channel\": intervened_channel,\n        \"x\": total_previous_imp_before_intervention,\n        \"delta_x\": total_change_imp_during_intervention,\n        \"delta_y\": total_observed_effect,\n        \"sigma\": sigma,\n    }]\n)\n\nintervened_data = data.copy()\nintervened_data.loc[880:980, \"impressions_x2\"] = 0\n\nnon_causal_mmm2 = MMM(\n    date_column=\"date\",\n    channel_columns=channel_columns,\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\nnon_causal_mmm2.build_model(\n    intervened_data.drop(columns=[\"target_var\"]), \n    intervened_data[\"target_var\"]\n)\n\nnon_causal_mmm2.add_lift_test_measurements(df_lift_test)\nnon_causal_mmm2.model.to_graphviz()\n```\n:::\n\n\nAs we can see a new observational point have been added to our data. This new point must be satisfied as the rest of our data, pooling parameter into a new direction.\n\n::: {.callout-note}\nIn a Bayesian model, each observation‚Äîwhether it is a daily data point $y_t$ or a lift measurement $\\Delta y$‚Äîcontributes a term to the likelihood. The posterior arises from the product of all these likelihood terms and the prior(s). In other words, theres no actual difference between priors and data, they both carry the same weight and multiply in the numerator of Bayes theorem. There's no discrete \"decision\" about which part of the data (or which prior) to weight more; it all goes into the same log‚Äêposterior function. The sampling or optimization algorithm (MCMC, variational inference, etc.) explores the parameter space in proportion to the posterior probability (which is prior √ó likelihood). Whichever parameters jointly give higher posterior density get visited more often by the sampler.\n:::\n\n::: {#ab818231 .cell execution_count=23}\n``` {.python .cell-code}\nnon_causal_mmm2.fit(\n    intervened_data.drop(columns=[\"target_var\"]), \n    intervened_data[\"target_var\"],\n)\nnon_causal_mmm2.sample_posterior_predictive(\n    intervened_data.drop(columns=[\"target_var\"]), \n    extend_idata=True, \n    combined=True\n)\n```\n:::\n\n\nNow that our model is ready, we can check the new estimated effect.\n\n::: {#c8b85e68 .cell execution_count=24}\n``` {.python .cell-code}\ny_do_x2_zero_second_model = non_causal_mmm2.idata.posterior_predictive.copy()\ny_do_x2_second_model = non_causal_mmm2.sample_posterior_predictive(\n    data.drop(columns=[\"target_var\"]), \n    extend_idata=False, \n    include_last_observations=False, \n    combined=False,\n    random_seed=42\n)\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect_second_model = (y_do_x2_zero_second_model.y - y_do_x2_second_model.y).isel(date=slice(880, 980))\n\n# Plot the causal effect\nplt.subplot(1, 2, 1)\n# Calculate mean and quantiles\nmean_effect_second_model = x2_causal_effect_second_model.mean(dim=[\"chain\",\"draw\"])\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, mean_effect_second_model)\nplt.title(\"Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\n\n# Plot the cumulative causal effect\nplt.subplot(1, 2, 2)\n# For cumulative effect, compute quantiles directly from cumulative sums\ncum_effect_second_model = x2_causal_effect_second_model.cumsum(dim=\"date\")\ncum_mean_second_model = cum_effect_second_model.mean(dim=[\"chain\",\"draw\"])\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, cum_mean_second_model)\nplt.title(\"Cumulative Causal Effect of Channel X2\", fontsize=6)\nplt.xlabel(\"Date\", fontsize=6)\nplt.ylabel(\"Cumulative Effect\", fontsize=6)\nplt.tick_params(axis='both', which='major', labelsize=4)\nplt.legend(fontsize=6)\nplt.tight_layout()\n```\n:::\n\n\nAs you can see the effect looks fully different. The size is 1000X higher than before. Let's compare!\n\n::: {#298f314b .cell execution_count=25}\n``` {.python .cell-code}\n# Create a figure to compare real effects with estimated effects\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, mean_effect_second_model, label='Estimated Effect (2)', color='orange', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.plot(x2_causal_effect_second_model.coords[\"date\"].values, cum_mean_second_model, \n         label='Estimated Cumulative Effect (2)', color='orange', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nAs expected the new observation makes the model add more credit to X2 but this came with the price of an overestimation of the true impact. Meanwhile, it was true that X2 impact was bigger than the original one, the second model absorbe all the variability possibly explain by other variables such as X1, X3 and bring a 1000X more extra impact, with a very tight posterior.\n\n::: {#fdcb7177 .cell execution_count=26}\n``` {.python .cell-code}\n# plot the recovered mean daily contribution as distribution.\nchannels_contribution_original_scale_model1 = non_causal_mmm.compute_channel_contribution_original_scale()\n\nchannels_contribution_original_scale_model2 = non_causal_mmm2.compute_channel_contribution_original_scale()\n\n_dist1 = channels_contribution_original_scale_model1.isel(date=slice(0, 800)).mean(\n    dim=[\"date\"]\n).sel(channel=\"impressions_x2\").values.flatten()\n\n_dist2 = channels_contribution_original_scale_model2.isel(date=slice(0, 800)).mean(\n    dim=[\"date\"]\n).sel(channel=\"impressions_x2\").values.flatten()\n\n\n# First subplot for Model 1\nplt.subplot(1, 2, 1)\nsns.kdeplot(_dist1, shade=True, label=\"Model 1\", bw_adjust=4.5)\nplt.title(\"Distribution of Channel X2 Contribution - Model 1\", fontsize=12)\nplt.xlabel(\"Contribution Value\", fontsize=10)\nplt.ylabel(\"Density\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=9)\n\n# Second subplot for Model 2\nplt.subplot(1, 2, 2)\nsns.kdeplot(_dist2, shade=True, label=\"Model 2\", bw_adjust=4.5, color=\"orange\")\nplt.title(\"Distribution of Channel X2 Contribution - Model 2\", fontsize=12)\nplt.xlabel(\"Contribution Value\", fontsize=10)\nplt.ylabel(\"Density\", fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.legend(fontsize=9)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {.callout-warning}\n## The Danger of Tight Posteriors\n\nIt's important to note that a tight posterior distribution (like we see in Model 2) should never be understood as the model being more correct or certain about the true causal effect. This is a common misconception in Bayesian analysis.\n\nA tight posterior simply means the model is very confident in its estimates given the data and prior assumptions it has, but says nothing about whether those assumptions are correct. In this case, the addition of the lift test measurement has created a model that is very confident in an incorrect answer.\n\nThis illustrates an important principle in causal inference and Bayesian modeling: **precision is not the same as accuracy**. A model can be precisely wrong - having a narrow posterior around an incorrect value. This often happens when:\n\n1. The model structure doesn't match the true causal process\n2. Important confounders are omitted\n3. The priors or likelihood are misspecified\n:::\n\n\nWhy all the following happened? lets take a look to the graph.\n\n::: {#9ab7f4df .cell execution_count=27}\n``` {.python .cell-code}\ndot\n```\n:::\n\n\nThis DAG shows:\n\n1. **Direct Spend-to-Impression Relationships**: Each spend variable (X1-X4) directly influences its corresponding impression variable.\n\n2. **Cross-Channel Effects**: \n   - Impressions from X1 influence impressions from X3\n   - Impressions from X2 influence both X3 and X4 impressions\n   - Events influence impressions for X2 and X3\n\nIf we were to build a naive regression model including all variables (X1, X2, X3, X4), we would encounter significant estimation problems, particularly for X2. According to Pearl's causal theory.\n\n### 1. Collider Bias\n\nIn our graph, X2 influences X3 and X4, which both influence the target variable. This creates a collider structure where conditioning on x1 variable because induces a spurious correlation between X2, X3. This violates the independence assumptions of standard regression.\n\n### 2. Mediator Effects\n\nX2 has both direct effects on the target variable and indirect effects through X3 and X4. A naive regression would conflate these paths, leading to inconsistent estimates of X2's true total causal effect.\n\n### 3. Confounding from Events\n\nEvents influence both X2 impressions and the target variable directly. Without properly accounting for this common cause, the estimate for X2 will capture some of the effect that actually comes from events.\n\nAll the above means, in order to estimate the effect of X2 we need to address the primal causal questions.\n\n### 4. Minimal Adjustment Set for X2\n\nTo estimate the total causal effect of X2 on the target variable, we need to identify the minimal adjustment set that blocks all non-causal paths while preserving the causal paths. According to Pearl's backdoor criterion, we must control for any confounders (common causes) while avoiding adjusting for colliders or mediators. In our DAG, the minimal adjustment set for estimating X2's total effect would include Events (as it's a confounder affecting both X2 and the target) and Spend X1 (as it influences the target through X3, creating a backdoor path). We should not adjust for impressions_x3 or impressions_x4, as these are mediators through which X2 partially exerts its effect on the target variable. Nevertheless, events are a cofounder of X2, meaning, we need to control for them if we want to get the estimates right on spot.\n\nThe proper identification of this minimal adjustment set is crucial for unbiased estimation. If we control for too few variables, confounding bias remains. If we control for mediators, we block part of the causal effect we're trying to measure. This highlights why structural causal models are superior to naive regression approaches - they allow us to explicitly model the causal pathways and make appropriate adjustments based on causal reasoning rather than statistical correlation. By conditioning only on the minimal adjustment set, we can obtain a consistent estimate of X2's total causal effect, including both its direct impact and indirect effects through other channels.\n\nSo, let's see what happen if we apply causal theory üòÉ\n\n::: {#cb643f34 .cell execution_count=28}\n``` {.python .cell-code}\n# Lets rebuild our media mix model\ncausal_mmm = MMM(\n    date_column=\"date\",\n    channel_columns=[\"impressions_x2\"],\n    control_columns=control_columns,\n    adstock=adstock,\n    saturation=saturation,\n    model_config=model_config,\n    sampler_config=sample_kwargs\n)\ncausal_mmm.fit(X_train, y_train,)\ncausal_mmm.sample_posterior_predictive(X_train, extend_idata=True, combined=True)\n```\n:::\n\n\nNow, lets repeat again the estimation of the effect when X2 is zero.\n\n::: {#bc882a79 .cell execution_count=29}\n``` {.python .cell-code}\nX_test_x2_zero = X_test.copy()\nX_test_x2_zero[\"impressions_x2\"].iloc[:100] = 0\n\ny_do_x2_zero_causal = causal_mmm.sample_posterior_predictive(\n    X_test_x2_zero, extend_idata=False, include_last_observations=True, random_seed=42\n)\n\ny_do_x2_causal = causal_mmm.sample_posterior_predictive(\n    X_test, extend_idata=False, include_last_observations=True, random_seed=42\n)\n# Calculate the causal effect as the difference between interventions\nx2_causal_effect_causal = (y_do_x2_zero_causal - y_do_x2_causal).y\n# Get dates from the coordinates for x-axis\ndates = x2_causal_effect_causal.coords['date'].values[:100]  # Take only first 100 days\n\n# Calculate mean and quantiles\nmean_effect = x2_causal_effect_causal.mean(dim=\"sample\")[:100]\ncum_effect = x2_causal_effect_causal.cumsum(dim=\"date\")\ncum_mean = cum_effect.mean(dim=\"sample\")[:100]\n\n# Plot 1: Compare daily effects\nplt.subplot(2, 1, 1)\nplt.plot(dates, daily_effect, label='Real Effect', color='blue')\nplt.plot(dates, mean_effect, label='Estimated Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Daily Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Compare cumulative effects\nplt.subplot(2, 1, 2)\nplt.plot(dates, cumulative_effect, label='Real Cumulative Effect', color='blue')\nplt.plot(dates, cum_mean, \n         label='Estimated Cumulative Effect', color='red', linestyle='--')\nplt.title(\"Comparison of Real vs Estimated Cumulative Causal Effects of Channel X2\", fontsize=10)\nplt.xlabel(\"Date\", fontsize=8)\nplt.ylabel(\"Cumulative Effect\", fontsize=8)\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.legend(fontsize=8)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\nGreat, as expected the true causal effect for X2 was recovered, and its possible to prove with an experiment. This just prove that maths are not magic, and that if we want to create models that explain the dynamics of the world, we need to use causal reasoning to it üî•üôåüèª\n\n# Conclusion\n\nThe evidence is clear: calibration cannot rescue a misspecified causal model. We've seen that:\n\n* **Causal misspecification persists despite calibration.** Our Model 2 became confidently wrong after calibration‚Äîtight posteriors around incorrect values.\n* **Colliders and mediators matter.** Standard MMMs ignore that marketing channels influence each other, creating spurious correlations that no amount of experimental data can fix.\n* **Adjustment sets are crucial.** Simply including every variable yields biased estimates; we must control only for confounders while preserving causal pathways.\n\nWhen we finally built a causally-aware MMM‚Äîcontrolling for events as confounders but avoiding adjustment for mediators‚Äîour estimates matched the ground truth. The same experimental evidence that couldn't rescue our misspecified model perfectly aligned with our correctly specified one.\n\nThe message: invest in causal discovery before calibration. Draw your DAGs. Identify your minimal adjustment sets. No amount of experimental evidence will save a model asking the wrong causal question.\n\nAs Pearl might say: statistics tells us *what* the data says; causality tells us *what* to do with it.\n\nCalibration without causation is just computation without comprehension!\n\n::: {#c6621059 .cell execution_count=30}\n``` {.python .cell-code}\n%load_ext watermark\n%watermark -n -u -v -iv -w -p pymc_marketing,pytensor\n```\n:::\n\n\n",
    "supporting": [
      "nomore_experiments_without_causality_files"
    ],
    "filters": [],
    "includes": {}
  }
}